---
title: "More Categorical Predictors"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \usepackage{siunitx}
   - \usepackage{dcolumn}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    fig_width: 6
    fig_height: 6
urlcolor: "umn2"
bibliography: epsy8251.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE)
opts_knit$set(width=85)
options(scipen=5)
```


<!-- LaTeX definitions -->

\mdfdefinestyle{mystyle}{userdefinedwidth=5in, align=center, backgroundcolor=yellow, roundcorner=10pt, skipabove=2em}

\mdfdefinestyle{mystyle2}{userdefinedwidth=5.5in, align=center, skipabove=10pt, topline=false, bottomline=false, 
linecolor=myorange, linewidth=5pt}

\mdfdefinestyle{work}{userdefinedwidth=5in, linecolor=blue, align=center, roundcorner=10pt, skipabove=2em}




# Preparation

In this set of notes, you will continue learning about the inclusion of categorical predictors in regression models. We will use data collected by [fivethirtyeight](https://fivethirtyeight.com/) to examine differences in the median incomes across college majors. We will use this to explore several speculative hypotheses about the choice of major, including:

- Women are more likely to choose a college major that earns less money.
- STEM majors make more money than non-STEM majors.


<!-- In particular, we will focus on whether women are attracted to STEM majors that have lower median incomes. In fivethirtyeight's analysis, they suggested that women are more attracted to the "S"-majors than the "TEM"-majors, and it is the "S"-majors that have lower median incomes. We will examine this via fitting a series of regression models.  -->

The dataset, *stem.csv*, includes data on 172 college majors collected from the *American Community Survey 2010--2012 Public Use Microdata Series* on graduates who were under the age of 29. Variabes in the dataset are: 

- `major`: Name of STEM major
- `income`: Median income (in thousands of dollars) for a full-time, year-round worker
- `num_grad`: Number of total graduates
- `women`: Percentage of total graduates who are women
- `unemployment`: Percentage of unemployed graduates
- `major_cat`: Category of major from @Carnevale:2011
- `stem_major`: Is the major a STEM (Science-Technology-Engineering-Mathematics) major? (0 = No; 1 = Yes)
- `s_tem`: Trichotomization of major categories (Science; Tech-Engineer-Math; Non-STEM)

\vspace{1.5em}

```{r preparation, warning=FALSE, message=FALSE}
# Load libraries
library(broom)
library(corrr)
library(dotwhisker)
library(dplyr)
library(ggplot2)
library(readr)
library(sm)
library(tidyr)

# Read in data
stem = read_csv(file = "~/Documents/github/epsy-8251/data/stem.csv")
head(stem)


```

\newpage

# Examine and Describe the Marginal Distribution of the Median Incomes

To begin the analysis, we will explore the outcome variable, median income.

```{r fig.width=6, fig.height=6, out.width='3in', fig.pos='H', fig.cap="Density plot of the median incomes for $n=76$ STEM majors."}
sm.density(stem$income, xlab = "Median income")
```


\vspace{1.5em}

```{r}
# Compute summary statistics
stem %>% 
  summarize(
    M  = mean(income),
    SD = sd(income),
    Min = min(income),
    Max = max(income)
  )
```

The median incomes for the 172 college majors are right-skewed and range from \$22,000 (Library Science) to \$110,000 (Petroleum Engineers). The mean income is \$40,000. The standard deviation of roughly \$11,500 suggests that most graduates earn between \$17,000 and \$63,000. 

To put this in perspective, around the same time period, the Bureau of Labor Statistics estimated that the median income for a person with only a high school education was \$25,500.

\newpage

# H1: Women are more likely to choose a college major that earns less money.

To explore this hypothesis, we can examine a scatterplot of the relationship between the percentage of graduates in each major that are women, and the median income for those majors.

\vspace{1.5em}

```{r fig.width=6, fig.height=6, out.width='3in', fig.pos='H', fig.cap='Scatterplot showing the relationship between the percentage of recent graduates in each STEM major that are women, and the median income for those majors. The OLS regression lines is also displayed on the plot.'}
# Plot the incomes by proportion of women
ggplot(data = stem, aes(x = women, y = income)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
	xlab("Percentage of recent graduates who are women") +
	ylab("Median income (in thousands of dollars)")
```

\vspace{1.5em}

We also compute the correlation matrix for these two variables.

```{r}
# Compute correlation coefficient
stem %>%
  select(income, women) %>%
  correlate() %>%
  fashion(decimals = 3)
```

The correlation coefficient and scatterplot suggest a negative relationship between these variables. This implies that majors that have a higher percentage of female graduates tend to be the same majors that have lower median incomes ($r=-.619$). This relationship seems linear and moderately strong. There is one major (Petroleum Engineering) that has an unusually high median income (\$110,000).

## Fitting a Regression Model

We can also fit a model that regresses median income on percentage of females.

```{r}
lm.1 = lm(income ~ 1 + women, data = stem)

# Model-level info
glance(lm.1)
```

Differences in the pecentage of females in the major explains 38.2\% of the variation in median incomes. This explain variation is statistically diffeent than 0, $F(2,170)=105.43$, $p<.001$.

```{r}
# Coefficient-level info
tidy(lm.1)
```


The fitted regression model is

$$
\hat{\mathrm{Median~Income}} = 56.09 - 0.31(\mathrm{Percentage~of~Women})
$$

- The fitted intercept ($\hat\beta_0=56.09$) indicates that the median income for college majors that are 100\% male is roughly \$56,000, on average.
- The fitted slope ($\hat\beta_1=-0.31$) indicates that, on average, the difference in median incomes for college majors that have one-percent more female graduates is \$310.

The $p$-value associated with the slope suggests that this difference in income is statistically significant ($p<.001$).

\newpage

# H2: STEM majors make more money than non-STEM majors.

To explore the second speculative hypothesis, we will examine a scatterplot and correlation matrix of the median incomes versus whether or not the major is a STEM major. For better plotting, we will coerce `stem` into a factor in the `ggplot()` function.

```{r fig.width=6, fig.height=6, out.width='3in', fig.pos='H', fig.cap='Scatterplot showing median income versus whether or not the major is a STEM major.'}
# Plot the median incomes by STEM major
ggplot(data = stem, aes(x = factor(stem_major), y = income, fill = factor(stem_major))) +
  geom_point(shape = 21, color = "black", size = 4) +
	theme_bw() +
	scale_x_discrete(name = "", labels = c("Non-STEM major", "STEM major")) +
	ylab("Median income (in thousands of dollars)") +
  scale_fill_viridis_d() +
  guides(fill = FALSE)
```

\vspace{1.5em}

```{r}
stem %>%
  group_by(stem_major) %>%
  summarize(
    M = mean(income), 
    SD = sd(income), 
    N = n()
    )
```

We also compute the correlation matrix for these two variables.

```{r}
# Compute correlation coefficient
stem %>%
  select(income, stem_major) %>%
  correlate() %>%
  fashion(decimals = 3)
```

The data suggests that there are potential income differences between STEM and non-STEM majors. On average, STEM majors earn about \$11,000 more annually than non-STEM majors. However, there is a great deal of variation in median incomes for both groups.

## Ridge Plots: An Alternative Plot for Comparing Distributions

Ridge plots are partially overlapping density plots that create the impression of a mountain range. They can be useful for comparing distributions. For more information, see the [package vignette](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).



```{r message=FALSE, fig.width=6, fig.height=6, out.width='3in', fig.pos='H', fig.cap="Ridge plot showing the distribution of median income for STEM and non-STEM majors."}
# Load the package
library(ggridges)

# Ridge plot
# - coerce stem_major into a factor
# - The variable mapped to x= has to be continuous, use coord_flip() to
#   emulate the positioning on the scatterplot
ggplot(data = stem, aes(x = income, y = factor(stem_major))) + 
  geom_density_ridges() +
  theme_bw() +
  xlab("Median income") +
  ylab("Dummy coded STEM major") +
  coord_flip()
```


\vspace{1.5em}

This plot suggests the same pattern in medain incomes as the scatterplot: STEM majors earn a higher salary on average than their non-STEM peers.

### Fit the Regression Model

To examine whether the observed difference in income is due ti chance, we fit a model regressing median income on the dummy variable `stem_major`.

```{r}
lm.2 = lm(income ~ 1 + stem_major, data = stem)

# Model-level info
glance(lm.2)
```

At the model-level, differences in major categorization seem to explain 22.1\% of the variation in median incomes; a statistically significant amount of the variation ($F(2,170)=48.30$, $p < .001$).

```{r}
# Coefficient-level info
tidy(lm.2)
```

The fitted regression equation is

$$
\hat{\mathrm{Median~Income}} = 35.294 + 10.825(\mathrm{STEM~Major})
$$

- The fitted intercept ($\hat\beta_0=35.294$) indicates that the median income for non-STEM majors is roughly \$35,000, on average.
- The fitted slope ($\hat\beta_1=10.825$) indicates that, on average, STEM majors have a median income that is \$10,800 higher than their non-STEM peers, on average.

The $p$-value associated with the slope suggests that this income difference is statistically significant ($p<.001$); likely not due to chance.

## H3: "TEM" majors are more coveted than "S" majors.

More recently, it has been suggested that not all STEM majors are created equal. In the fivethirtyeight.com article [The Economic Guide To Picking A College Major](https://fivethirtyeight.com/features/the-economic-guide-to-picking-a-college-major/), Ben Casselman wrote:

> Politicians love to tout the importance of science, technology, engineering and math majors. But when it comes to earnings, the "S" majors don't really belong with the "TEM" ones.

To explore this hypothesis, we can plot the median incomes versus the `s_tem` categorization.

```{r message=FALSE, fig.width=6, fig.height=6, out.width='3in', fig.pos='H', fig.cap='Ridge plot showing the distribution of median income for "S", "TEM", and non-STEM majors.'}
# Ridge plot
ggplot(data = stem, aes(x = income, y = s_tem)) + 
  geom_density_ridges() +
  theme_bw() +
  xlab("Median income") +
  ylab("Major categorization") +
  coord_flip()
```

The plots for the Science ("S")-majors and non-STEM majors look relatively similar. The distribution of incomes associated with the Technolgy, Engineering, and Mathematics ("TEM")-majors looks to have a higher median income than the other two groups.

```{r}
stem %>%
  group_by(s_tem) %>%
  summarize(
    M = mean(income), 
    SD = sd(income), 
    N = n()
    )
```


The mean income for "S"-majors is lower than for "TEM"-majors, and comparable to that for non-STEM majors. The "TEM" majors earn roughly \$15,000 more than their peers.


### Fitting a Regression Model

To examine whether the differences we observed are due to chance, we need to fit a model regressing median income on the `s_tem` categorization. Before fitting this model, we need to create a dummy variable for EACH category of the `s_tem` variable. For our analysis, we will need to create three dummy variables: `science`, `tech_eng_math`, and `non_stem`. To do this we will use the `if_else()` function. 

The `if_else()` function evaluates a conditional statement (which produces elements that are either `TRUE` or `FALSE`) and outputs one thing IF the element is `TRUE` and outputs something ELSE if the element is `FALSE`. The function's useage looks like this:

$$
\mathtt{if\_else(} \mathrm{conditional~statement,~output~if~TRUE,~output~if~FALSE} \mathtt{)}
$$
\newpage

For example, to evaluate whether a major is a Science major, we can use the conditional statement:

\vspace{1.5em}

```{r eval=FALSE}
s_tem == "Science"
```

When we are creating the dummy variable `science`, we will give this variable a value of `1` if the STEM category is Science (a `TRUE` element in our logical vector) and a `0` if the STEM category is not Science (a `FALSE` element in our logical vector).

The full `if_else()` syntax to create a `science` dummy-coded variable is this:

\vspace{1.5em}

```{r}
# Create science dummy variable
stem %>%
  mutate(
    science = if_else(s_tem == "Science", 1, 0)
    )
```

\vspace{1.5em}

All majors with a `s_tem` value that was `Science`, will have the dummy code 1 in the new `science` variable. The dummy code for all other major categories will be 0. Here we will create all three dummy variables. All three can be put in the same `mutate()` layer. We re-assign this into an object called `stem`

\vspace{1.5em}

```{r}
# Create all three dummy variables
stem = stem %>%
  mutate( 
    science       = if_else(s_tem == "Science",               1, 0),
    tech_eng_math = if_else(s_tem == "Tech-Engineering-Math", 1, 0),
    non_stem      = if_else(s_tem == "Non-STEM",              1, 0)
    )

# Examine data
head(stem)
```

\vspace{1.5em}

If you do not know the actual names of the categories (or you want to check capitalization, etc.) use the `unique()` function to obtain the unique category names.

\vspace{1.5em}

```{r}
# Get the categories
unique(stem$s_tem)
```

Once the dummy variables have been created, fit the regression using all but one of the dummy variables you created. The dummy variable you leave out will correspond to the reference category. For example, in the model fitted below, we include the predictors `science`, and `tech_eng_math` as prdictors in the model; we did not include the `non_stem` predictor. As such, non-STEM majors is our reference group.

\vspace{1.5em}

```{r}
# non-STEM is reference group
lm.3 = lm(income ~ 1 + science + tech_eng_math, data = stem)

# Model-level info
glance(lm.3)
```

At the model-level, differences in major categorization seem to explain a statistically significant amount of the variation in median incomes ($F(3,169)=60.75$, $p < .001$). In this model, differences in major categorization explain 41.8\% of the variation in median incomes.

Note the explained variation in this model is much higher than the explained variation when we only used two categorizations of major ($R^2=0.22$). Increasing variation in the predictor can often lead to stronger explanatory models. This means that if you have a continuous predictor you may not want to cut it up into categories. 

```{r}
# Coefficient-level info
tidy(lm.3)
```

The fitted regression equation is

$$
\hat{\mathrm{Median~Income}} = 35.294 + 2.781(\mathrm{Science~Major}) + 18.063(\mathrm{TEM~Major})
$$

The intercept is the average $Y$ value for the reference group. Each partial slope is the difference in average $Y$ values between the reference group and the group represented by the dummy variable. In our example, 

- The average income for non-STEM majors is \$35,294. 
- Students who earned a Science major earn \$2,781 more annually, on average, than their non-STEM peers.
- Students who earned a technology, engineering, or mathematics major earn \$18,063 more annually, on average, than their non-STEM peers.

It is important to note that the partial slope associated with the difference between non-STEM and science majors ($p=.107$) is not statistically significant. This implies that there is likely no difference in average income between science majors and non-STEM majors. The partial slope for the difference between technology, engineering, and mathematics majors and non-STEM majors ($p<.001$), however, indicates that there is a statistically signiicant difference in the average income between these types of majors.

## Omnibus Test vs. Coefficient Tests with Multiple Dummy Variables

When we use multiple dummy variables to represent a single categorical predictor, each $\beta$-term represents the mean difference between two groups. For example, in our fitted equation we see the results of testing the following two hypotheses:

$$
\begin{split}
\beta_1 &= \mu_{\mathrm{Science}}-\mu_{\mathrm{non\mbox{-}STEM}} \\
\beta_2 &= \mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{non\mbox{-}STEM}} \\
\end{split}
$$

Recall that one manner in which we could write the null hypothesis associated with the model-level test is that all the partial slopes are zero:

$$
H_0: \beta_1 = \beta_2 = \ldots = \beta_k = 0
$$
When we express the null hypothesis at the model-level when we use multiple dummy variables to represent a single categorical predictor, the test includes the mean differences between ALL sets of two groups, not just the differences included in the fitted equation. In our example, it represents

$$
\begin{split}
\beta_1 &= \mu_{\mathrm{Science}}-\mu_{\mathrm{non\mbox{-}STEM}} \\
\beta_2 &= \mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{non\mbox{-}STEM}} \\
\beta_3 &= \mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{Science}} \\
\end{split}
$$

We can express this as

$$
H_0: \beta_1 = \beta_2 = \beta_3 = 0
$$

or as

$$
\begin{split}
H_0: &\bigg(\mu_{\mathrm{Science}}-\mu_{\mathrm{non\mbox{-}STEM}}\bigg) = \bigg(\mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{non\mbox{-}STEM}}\bigg) = \\ &\bigg(\mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{Science}}\bigg) = 0
\end{split}
$$

The test at the model-level is considering all three pairwise differences simultaneously. If the model-level test is significant, any one (or more than one) of the differences may not be zero. Because of this, it is important to examine ALL potential coefficient-level differences, not just those outputted from the initial fitted model.


### Link to the ANOVA Test

Note that if all the means are equal, then each difference in the previous hypothesis would be 0. So we could also write the model-level null hypothesis as,

$$
H_0: \mu_{\mathrm{non\mbox{-}STEM}} = \mu_{\mathrm{Science}} = \mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}
$$

This is the omnibus null hypothesis associated with the one-factor analysis of variance (ANOVA). Fitting a regression model with dummy-variables is the same analysis as carrying out an ANOVA. The difference is that the output from the multiple regression gives $\beta$-terms associated with mean differences (to the reference group), and ANOVA is concerned more directly with the group means. But the model-level regression results are identical to those from the ANOVA. Asking whether the model explains variation in the outcome ($H_0:\rho^2=0$) is the same as asking whether there are mean differences ($H_0: \mu_{\mathrm{non\mbox{-}STEM}} = \mu_{\mathrm{Science}} = \mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}$); these are just different ways of writing the model-level null hypothesis!

# Further Understanding Income Differences

If you are only interested in if there are differences, you can focus on the model-level (omnibus) results. If, however, you want to go further and examine the pairwise differences between major categories, we need to look at the coefficient-level results. Based on the fitted equation from above, so far we have considered only two of the three possible pairwise differences.

```{r echo=FALSE}
tab_01 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  mean_diff = c("\\$2,781", "\\$18,064", "?"),
  p = c("0.107", "<0.001", "?")
)

kable(tab_01, col.names = c("Comparison", "Mean Difference", "*p*"), caption = 'Pairwise Comparisons between Three College Major Categorizations', align = c("llr"))
```



In order to examine the remaining pairwise difference, we need to fit an additional regression model that allows us to evaluate this comparison. Below, we fit a second model (using science majors as the reference group) to predict variation in income.

```{r}
# Science majors is reference group
lm.science = lm(income ~ 1 + non_stem + tech_eng_math, data = stem)

# Model-level info
glance(lm.science)
```

Note that the model-level output for this fitted model is exactly the same as that for the model in which non-STEM majors was the reference group. This is because we are fitting the exact same omnibus model (to examine whether the three categorizations explain variation in income).

```{r}
# Coefficient-level info
tidy(lm.science)
```

The fitted regression equation, which is different than the previous fitted equation, is

$$
\hat{\mathrm{Median~Income}} = 38.070 - 2.781(\mathrm{non\mbox{-}STEM~Major}) + 15.283(\mathrm{TEM~Major})
$$

- The average income for Science majors is \$38,070. 
- Students who earned a non-STEM major earn \$2,781 less annually, on average, than Science majors ($p = .107$).
- Students who earned a technology, engineering, or mathematics major earn \$15,283 more annually, on average, than Science majors ($p<.001$).

For completeness, we also show the syntax to fit the model where Technology, Engineering and Mathematics majors is the reference group. Note that fitting this model does not give us any new information about the overall explained variation or the pairwise comparisons. (It is redundant information to that we have already obtained from fitting the other two models.)

```{r}
# Technology, engineering, and mathematics majors is reference group
lm.tech = lm(income ~ 1 + non_stem + science, data = stem)

glance(lm.tech) # Model-level info
tidy(lm.tech)   # Coefficient-level info
```


At the model-level, all three models give the same information.

```{r, message=FALSE, results='asis', echo=FALSE}
lm_03 = texreg::extract(lm.3, include.rsquared = TRUE, 
    include.adjrs = FALSE, include.nobs = FALSE, 
    include.fstatistic = TRUE, include.df = TRUE, include.rmse = TRUE)
lm_04 = texreg::extract(lm.science, include.rsquared = TRUE, 
    include.adjrs = FALSE, include.nobs = FALSE, 
    include.fstatistic = TRUE, include.df = TRUE, include.rmse = TRUE)
lm_05 = texreg::extract(lm.tech, include.rsquared = TRUE, 
    include.adjrs = FALSE, include.nobs = FALSE, 
    include.fstatistic = TRUE, include.df = TRUE, include.rmse = TRUE)

texreg::texreg(
  list(lm_03, lm_04, lm_05), 
  custom.model.names = c("non-STEM", "Science", "Tech/Eng./Math"),
  float.pos = "H",
  caption = "Regression Results from Fitting Three Different Models to Predict Median Income from a Set of Dummy-Coded Major Categories",
  caption.above = TRUE,
  dcolumn = TRUE,
  use.packages = FALSE
  )
```

Using the coefficient-level output from the fitted equations, we can fill in the remaining cells of the table. Based on the results, it looks as though technology, engineering, and mathematics majors have statistically significantly higher incomes, on average, than science majors and non-STEM majors. There appears to be no significant income differences between science majoars and their non-STEM peers.

```{r echo=FALSE}
tab_02 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  mean_diff = c("\\$2,781", "\\$18,064", "\\$15,283"),
  p = c("0.107", "<0.001", "<0.001")
)

kable(tab_02, col.names = c("Comparison", "Mean Difference", "*p*"), caption = 'Pairwise Comparisons between Three College Major Categorizations', align = c("llr"))
```


## Multiple Comparisons

When we evaluated the $p$-values for each of these differences, we used an alpha value of 0.05 as the criterion for statistical significance. This is consistent with how we have evaluated other predictors in regression models. This is okay when the regression effect constitutes a single term or mean difference in the null hypothesis. For a predictor with more than two levels, however, the null hypothesis constitutes more than one mean difference.

For the effect of major categoryization, we really have three mean differences. To be "fair" with other predictors we might include in the model that would constitute a single term/difference, we should really split the 0.05 across the three differences. The easeist manner to make this "fair" is to divide the 0.05 evenly across the three differences.

$$
\alpha_{\mathrm{Comparison}}=\frac{0.05}{3} = 0.017
$$

Then, rather than rejecting the null hypothesis when the $p$-value is below 0.05, we will only reject if the comparison has a $p$-value below 0.017. Looking back at the table we created earlier, none of our decisions about whether differences were due to chance changed. The income differences between Tech/Eng./Mathematics and Science majors ($p=.00000000002373643<.017$) and non-STEM majors ($p=.00000000000000000002477524<.017$) are both still significantly different. The income differences between Science nd non-STEM majors ($p=.1074411>.017$) is still not statistically reliable.


### Adjusting the $p$-Value (not alpha)

In practice, people are psychologically accustomed to comparing the $p$-value to 0.05, so changing the alpha-value to 0.017 can be a problem. Another way to achieve the same adjustment, but still allow people to compare to 0.05, is to change the $p$-value rather than change the alpha value. To do this, we multiply each $p$-value by the number of mean differences that constitute the regression effect, rather than dividing the alpha value by this value. In our example, we would multiply each $p$-value by 3.

\vspace{1.5em}

```{r}
p_values = c(
  0.00000000002373643,          # Tech/Eng./Mathematics vs. Science
  0.00000000000000000002477524, # Tech/Eng./Mathematics vs. non-STEM
  0.1074411                     # Science vs. non-STEM
)

# Adjust p-values
p_values * 3
```

This method of evenly splitting the alpha value or adjusting the $p$-value evenly is called the *Bonferroni adjustment*. We can also use the `p.adjust()` function to compute the Bonferroni adjusted $p$-values. To use this, create a vector of the unadjusted $p$-values and then include this vector in the `p.adjust()` function along with the argument `method = "bonferroni"`.

```{r}
# Bonferroni adjustment to the p-values
p.adjust(p_values, method = "bonferroni")
```

When reporting the adjusted $p$-values, be careful. Remember that $p$-values are always between 0 and 1. Anything above 1 needs to be reported as 1! (The `p.adjust()` function automatically takes care of this.) 

```{r echo=FALSE}
tab_03 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  #mean_diff = c("\\$2,781", "\\$18,064", "\\$15,283"),
  p = c("0.107", "<0.001", "<0.001"),
  p_adj = c("0.322", "<0.001", "<0.001")
  )


kable(tab_03, col.names = c("Comparison", "Unadjusted *p*", "Bonferroni-Adjusted *p*"), caption = 'Unadjusted and Bonferroni Adjusted p-Values for Income Comparisons Between Major Categorizations', align = c("lrr"))
```

### Other $p$-Value Adjustment Methods

There is nothing that requires you to evenly adjust the $p$-value across the three comparisons. For example, some adjustment methods use different multipliers depending on the size of the initial unadjusted $p$-value. One of those methods is the *Benjamini--Hochberg adjustment*. This adjustment procedure ranks the unadjusted $p$-values from smallest to largest and then adjusts by the following computation\footnote{The actual adjusted $p$-value given is the minimum of this value and the adjusted $p$-value for the next higher raw $p$-value.}:

$$
p_{\mathrm{adjusted}} = \frac{k \times p_{\mathrm{unadjusted}}}{\mathrm{Rank}}
$$

In this adjustment, the numerator is equivalent to making the Bonferroni adjustment. The size of the Bonferroni adjustment is then scaled back depending on the initial rank of the unadjusted $p$-value. The smallest initial $p$-value gets the complete Bonferroni adjustment, while the largest Bonferroni adjustment is scaled back the most. We can use `method="BH"` in the `p.adjust()` function to obtain the Benjamini--Hochberg adjusted $p$-values directly.


```{r}
# Benjamini-Hochberg adjustment to the p-values
p.adjust(p_values, method = "BH")
```

```{r echo=FALSE}
tab_04 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  #mean_diff = c("\\$2,781", "\\$18,064", "\\$15,283"),
  p = c("0.107", "<0.001", "<0.001"),
  p_adj = c("0.107", "<0.001", "<0.001")
  )


kable(tab_04, col.names = c("Comparison", "Unadjusted *p*", "Bonferroni-Adjusted *p*"), caption = 'Unadjusted and Benjamini--Hochberg Adjusted p-Values for Income Comparisons Between Major Categorizations', align = c("lrr"))
```


Using the Benjamini--Hochberg adjusted $p$-values, we still find statistically significant income differences between (1) non-STEM and science majors, and (2) non-STEM and Tech/Eng./Math majors.

### Which Adjustment Method?

There are many, many different adjustment methods you can choose. The `p.adjust()` function, for example, includes six adjustment options (the Holm method, the Hochberg method, the Hommel method, the Bonferroni method, the Bnjamani--Hochberg method, and the Benjamini--Yekutieli method). In addition, the **multcomp** package includes several other adjustment methods.

You should decide which adjustment method you will use before you do the analysis. In the social sciences, the Bonferroni method has been historically the most popular method (probably because it was easy to implement before computing). That being said, I would encourage you to use the Benjamini--Hochberg adjustment method. It is from a family of adjustment methods that a growing pool of research evidence points toward as the "best" solution to the problem of multiple comparisons [@Williams:1999].  Because of its usefulness, the Institute of Education Sciences has recommended this procedure for use in its [What Works Clearinghouse Handbook of Standards](http://ies.ed.gov/ncee/wwc/Handbooks).

# Does Major Categorization Mediate the Relationship Between Proportion of Women and Income?

According to [Wikipedia](https://en.wikipedia.org/wiki/Mediation_(statistics)),

> [A] mediation model is one that seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable...Rather than a direct causal relationship between the independent variable and the dependent variable, a mediation model proposes that the independent variable influences the (non-observable) mediator variable, which in turn influences the dependent variable.

In our example, we hypothesize that is is not the influx of women into a major that causes lower median incomes, but rather that women are attracted to the "S"- and non-STEM majors and it is the type of major that is causing the lower incomes. In this sense, we would argue that type of major MEDIATES the relationship between the proportion of women graduating with that major and the income level.

To test this hypothesis, we will fit a multiple regression model that includes both the proportion of women graduating and the set of dummy-coded major categorization predictors; a multiple regression model. Then, we can see if the partial/controlled relationship associated with the `women` predictor has changed from the simple regression model.

```{r}
lm.mediator = lm(income ~ 1 + women + science + tech_eng_math, data = stem)
glance(lm.mediator)
```

At the model-level, we find that differences in the proportion of women who graduate from the major and major categorization explain 50.2\% of the variation in median incomes. This is a statistically significant amount of variation, $F(4,168)=56.41$, $p < .0001$.


```{r}
tidy(lm.mediator)
```

At the coefficient-level, we are most interested in the $\beta$-coefficient associated with the `women` predictor. This coefficient, $\hat\beta=-0.19$, indicates that after controlling for type of major, majors that graduate more women have lower median incomes, on average. Each one-percent difference in the percentage of female graduates is associated with a \$185 decrease in median income, on average.

Although this effect is still statistically significant, $t(168)=-5.31$, $p<.001$, the size of the effect has dimished from what it was in the simple regression model ($\hat\beta=-0.30$). After controlling for type of major, the effect has diminished. This seems to support our mediation hypothesis that the type of major mediates the relationship between percentage of female graduates and median income.

Mediation is really a hypothesis about the underlying causal mechanism. Understanding the nature of cause is a substantive, not statistical, issue. The way we analyze mediation is by comparing uncontrolled effects (simple regression) to controlled effects (multiple regression). As @Kenny:2016 writes, "if the presumed causal model is not correct, the results from the mediational analysis are likely of little value".

# ANCOVA: Controlled Group Differences

Sometimes the goal of the analysis is the group differences. For example, in Hypothesis 3, our focal research was on the income differences between major categorizations. In this case, we would likely be interested in evaluating whether the differences we saw in the uncontrolled model persist after controlling for differences in one or more covariates. In psychology, an analysis that focuses on controlled group differences is referred to as an *Analysis of Covariance* or ANCOVA.

We will examine whether income difference across major type persist after controlling for the percentage of women in the major. In this analysis, the focus is on the group differences, NOT on the `women` effect. We can again fill out a table of mean differences, unadjusted and adjusted $p$-values. But for the controlled group differences, we will use the results from our multiple regression analysis. (The syntax for fitting the models is shown below, but the summary results are not printed.)


```{r}
# Fit ANCOVA models
lm.7 = lm(income ~ 1 + women + science + tech_eng_math, data = stem)
lm.8 = lm(income ~ 1 + women + non_stem + tech_eng_math, data = stem)

# Adjust p-values
p_values = c(
  0.02544385, #non-STEM vs. science
  0.00000003928573, #non-STEM vs. Tech/Eng/Math
  0.0003641043 #science vs. Tech/Eng/Math
  )
p.adjust(p_values, method = "BH")
```


```{r echo=FALSE}
tab_05 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  mean_diff = c("\\$3,613", "\\$11,938", "\\$8,325"),
  p     = c(0.02544, 0.000000039, 0.00036),
  p_adj = c(0.02544, 0.000000118, 0.00055)
  )


kable(tab_05, col.names = c("Comparison", "Adj. Mean Difference", "Unadjusted *p*", "Adjusted *p*"), caption = 'Adjusted Mean Differences, Unadjusted and Benjamini--Hochberg Adjusted p-Values for Income Comparisons Between Major Categories Controlling for Differences in the Percentage of Female Graduates', align = c("lrrr"))
```



Using the Benjamini--Hochberg adjusted $p$-values, after controlling for differences in the percentage of female graduate, we find statistically significant income differences between (1) non-STEM  and Science majors, (2) non-STEM and Tech/Eng./Math majors, and (3) Science and Tech/Eng./Math majors. After controlling for the percentage of women in the major, the income differences between non-STEM  and Science majors has become statistically significant!

In the language of ANCOVA, the controlled mean differences are referred to as *Adjusted Mean Differences*. So, for example, the adjusted mean difference in incomes between non-STEM and Science majors is \$3,613 (controlling for differences in the percentage of female graduates). When the mean difference is from a model that has no covariates, it is referred to as an *Unadjusted Mean Difference*. It can be useful to present both the unadjusted and adjusted mean differences in a table.

```{r echo=FALSE}
tab_06 = data.frame(
  comparison = c("Science $-$ non-STEM", "Tech/Eng/Math $-$ non-STEM", "Tech/Eng/Math $-$ Science"),
  mean_diff = c("\\$2,781", "\\$18,064", "\\$15,283"),
  adj_mean_diff = c("\\$3,613", "\\$11,938", "\\$8,325")
)

kable(tab_06, 
      col.names = c("Comparison", "Unadjusted", "Adjusted"), 
      caption = 'Unadjusted and Adjusted Mean Differences for Income Comparisons Between STEM Categories. Adjusted Mean Differences are Controlling for Differences in the Percentage of Female Graduates', 
      align = c("lrr"))
```


The results show how the income difference between major categories changes when we control for differences in other covariates, in this case, the percentage of female graduates.


# Technical Reasons to Adjust for Multiple Comparisons

In the earlier sections, we presented the reason for adjusting the $p$-values for the major category comparisons as one of "fairness" with the other predictors in the model. This is true, but there are also technical reasons to make these adjustments. The main technical reason is related to the *Type I error rate*. Remember that a Type I error occurs when you falsely reject a true null hypothesis. In other words, we would say there is an income difference between major categories when there really isn't a difference.

When we use an alpha value of 0.05, we are saying we are willing to make a Type I error in 5\% of the samples that could be randomly selected (we have no idea whether our sample is one of the 5\% where we will make an error, or one of the 95\% where we won't). For effects that only have one row in the model, there is only one test in which we can make a Type I error ($H_0: \beta_j=0$), so we are okay evaluating each at the alpha of 0.05.

When we have more than two levels of a categorical predictor, there are multiple differences that constitute the effect of that predictor. To test whether there is an effect of that predictor, we evaluate multiple hypotheis tests. For our data, to test whether there is an effect of major category on income, we evaluate three hypothesis tests:


$$
\begin{split}
H_0: &\mu_{\mathrm{Science}}-\mu_{\mathrm{non\mbox{-}STEM}} = 0 \\ 
H_0: &\mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{non\mbox{-}STEM}} = 0 \\ 
H_0: &\mu_{\mathrm{Tech\mbox{-}Eng\mbox{-}Math}}-\mu_{\mathrm{Science}} = 0
\end{split}
$$

Because of this, there are many ways to make a Type I error. For example, we could make a Type I error in any one of the three tests, or in two of the three tests, or in all three of the three tests. Therefore, the probability of making at least one Type I error is no longer 0.05, it is

$$
1 - (1 - \alpha)^k
$$
where $\alpha$ is the alpha level for each test, and $k$ is the number of tests (comparisons) for the effect.

\newpage

In our example this is

$$
P(\mathrm{type~I~error}) = 1 - (1 -0.05)^{3} = 0.142
$$

The probability that we will make at least one Type I error in the six tests is .142 NOT .05!!! This probability is called the family-wise Type I error rate. In the social sciences, the family-wise error rate needs to be 0.05. What should $\alpha$ be if we want the family-wise error rate to be 0.05? Essentially we would need to solve this equation:

$$
0.05 = 1 - (1 - \alpha)^{3}
$$

Carlo Emilio Bonferroni solved this algebra problem for any value of $k$ and found that the value for alpha that $\dfrac{\mathrm{family{\mbox{-}}wise~error~rate}}{k}$ gives an upper-bound for the solution. Olive Jean Dunn then used Bonferroni's solution in practice. This is why dividing by the number of comparisons is referred to as the Bonferroni or the Dunn--Bonferroni method.

## False Discovery Rate

The Benjamini--Hochberg procedure is an ensemble method based on *false discovery rate* (FDR). FDR is a relatively new approach to the multiple comparisons problem. Instead of making adjustments to control the probability of making at least one Type I error, FDR controls the *expected proportion of discoveries* (rejected null hypotheses) when the null hypothesis is true; in other words, it controls the expected proportion of Type I error. You can find out more from [Wikipedia](https://en.wikipedia.org/wiki/False_discovery_rate).

The FDR concept was formally described in a 1995 paper by Yoav Benjamini and Yosi Hochberg, and resulted in their proposal of the Benjamini--Hochbergm method [@Benjamini:1995]. They argued that using FDR produces a less conservative and arguably more appropriate approach for identifying statistically significant comparisons.

In practice, using FDR rather than family-wise adjustment of error makes these methods less prone to over-adjustment of the $p$-values. However, the increased statistical power that comes with using the FDR methods is not without cost. They also have increased probabilities of Type I errors relative to the family-wise adjustment methods.


# References



