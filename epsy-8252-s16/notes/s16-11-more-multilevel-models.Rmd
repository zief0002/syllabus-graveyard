---
title: "Multilevel Models II"
author: "Andrew Zieffler"
date: "March 29, 2016"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r message=FALSE}
library(AICcmodavg)
library(dplyr)
library(foreign)
library(ggplot2)
library(lme4)
library(sm)
```


## Load and Prepare Data


```{r warning=FALSE}
nbaL1 = read.spss(file = "~/Documents/EPsy-8252/data/nbaLevel1.sav", to.data.frame = TRUE)
nbaL2 = read.spss(file = "~/Documents/EPsy-8252/data/nbaLevel2.sav", to.data.frame = TRUE)
nba = left_join(nbaL1, nbaL2, by = "Team_ID")
head(nba)
```

# Fit Models

Here we will fit three models we considered in the last set of notes (using REML). These are chosen to illustrate several concepts, not because they were necessarily adopted.


```{r}
lmer.0 = lmer(Life_Satisfaction ~ 1  + (1 | Team_ID), data = nba)
lmer.1 = lmer(Life_Satisfaction ~ 1  + Shots_on_five + (1 | Team_ID), data = nba)
lmer.2 = lmer(Life_Satisfaction ~ 1  + Shots_on_five + (1 + Shots_on_five | Team_ID), data = nba)
```

## Variance--Covariance Matrix of the Random Effects

It is common to provide covariances (rather than correlations) when reporting results from multilevel models. To obtain the variance--covariance matrix of the random effects, we use the `varCorr()` function. Without accessing the `Team_ID` component (the cluster variable), this function would just return the RE part of the `summary()` output. 

```{r}
# Get variance-covariance matrix of the random effects
VarCorr(lmer.2)$Team_ID
```

If you want only the variance--covariance matrix, then we use indexing to extract only those elements that are related to this matrix, in this case the first two rows and the first two columns.

```{r}
VarCorr(lmer.2)$Team_ID[1:2, 1:2]
```



- $\sigma^2_0 = 0.093$
- $\sigma^2_1 = 0.099$
- $\sigma_{01} = 0.096$

All three estimates should be reported in a summary or write-up of results.

In Model 0 and Model 1, the only RE is the intercept, so the variance--covariance matrix of the RE would be a 1x1 matrix.

```{r}
VarCorr(lmer.0)$Team_ID
VarCorr(lmer.1)$Team_ID
```


# Assumptions

In fixed-effects regression models we looked at two plots to evaluate whether assumptions were satisfied: (1) density plot of the residuals, and (2) scatterplot of the residuals vs. the fitted values. We are going to look at these same plots with multilevel models, but now there is a set of fitted values and residuals for each equation in the multilevel model. Consider a model which includes random effects for both intercepts and slopes, and a level-2 predictor for both:

$$
\begin{split}
\mathrm{Life~Satisfaction}_i &= \beta^{*}_0 + \beta^{*}_1(\mathrm{SO5}) + \epsilon_i \\
\beta^{*}_0 &= \beta_{00} + \beta_{01}(\mathrm{Coach}) + \eta_{0j}\\
\beta^{*}_1 &= \beta_{10} + \beta_{11}(\mathrm{Coach}) + \eta_{1j}
\end{split}
$$

```{r tidy=FALSE}
lmer.3 = lmer(Life_Satisfaction ~ 1  + Shots_on_five + Coach_Experience + 
                Coach_Experience:Shots_on_five + (1 + Shots_on_five | Team_ID), 
              data = nba)
```



## Level-1 Assumptions

For the level-1 model we will examine (1) a density plot of the level-1 residuals, and (2) a scatterplot of the level-1 residuals vs. the fitted life satisfaction values. We will look for the same things in this plot as we did in the fixed-effects regression. The `fortify()` function works on a model fitted using `lmer()` as well. Here you get the typical columns. The `.scresid` column are *scaled residuals*.


```{r tidy=FALSE}
out = fortify(lmer.3)
head(out)

sm.density(out$.scresid, model = "normal")

ggplot(data = out, aes(x = .fitted, y = .scresid)) +
  geom_point(size = 3) +
  theme_bw() +
  geom_hline(yintercept = 0)
```

## Level-2 Residuals: Intercept

For the level-2 intercept model we will examine (1) a density plot of the level-2 residuals from the intercept model, and (2) a scatterplot of the level-2 residuals from the intercept model vs. the fitted intercept ($\hat{\beta}^{*}_0$) values. We will again look for the same things in these plots as we did before. 

The level-2 residuals and fitted values do not exist in a data frame, so we will compute them. The residuals are just the REs for intercept, so they are easy to access. The fitted values at level-2 we can compute by using the level-2 fitted model: $\hat{\beta}^{*}_0 = \hat{\beta}_{00} + \hat{\beta}_{01}(\mathrm{Coach})$. This is at the team level ($n=30$), so we will use the level-2 data set to compute these. 

```{r}
# Compute the residuals
res0 = ranef(lmer.3)$Team_ID[ , 1]

# Get fixed effects
fixef(lmer.3)

# Compute the fitted values
fit0 = 4.575960 + nbaL2$Coach_Experience*1.229287

# Put in data frame
int = data.frame(fit0, res0)
head(int)

# Density plot
sm.density(int$res0, model = "normal")

# Scatterplot of residuals vs fitted values
ggplot(data = int, aes(x = fit0, y = res0)) +
  geom_point(size = 3) +
  theme_bw() +
  geom_hline(yintercept = 0)
```



## Level-2 Residuals: Slope

We can repeat this process to examine the residuals from the slope model.

```{r}
# Compute the residuals and fitted values
slp = data.frame(
  res1 = ranef(lmer.3)$Team_ID[ , 2],
  fit1 = 2.676017  + nbaL2$Coach_Experience*0.204538
)
head(slp)

# Density plot
sm.density(slp$res1, model = "normal")

# Scatterplot of residuals vs fitted values
ggplot(data = slp, aes(x = fit1, y = res1)) +
  geom_point(size = 3) +
  theme_bw() +
  geom_hline(yintercept = 0)
```

# Pseudo $R^2$ Values

In fixed effects model, the $R^2$ value summarized the variation in $Y$ accounted for by the model. There were two ways that we could arrive at this value. The first is to compute the reduction in the residual variation from the intercept only model. The second is to compute the squared correlation between the $Y$ and $\hat{Y}$ values. In fixed effects regression these gave the same result.

We will start by computing the squared correlation between the $Y$ and $\hat{Y}$ values. Again, we can use the fortified data.

```{r}
cor(out$Life_Satisfaction, out$.fitted) ^ 2
```

This we typically interpret as the percentage of variation in $Y$ accounted for by the model.

There are three computations we will need to make to compute the psudo-$R^2$ values based on the reduction in variance, that for: level-1, level-2 intercept, and level-2 slope residuals. Note in the fixed effects model we always compared the model under study to the intercept only model. For the level-1 and level-2 intercept residuals, we will compare the model under study to the unconditional means model. Consider the RE portion of the `summary()` output for Model 3:

```
Random effects:
 Groups   Name          Variance Std.Dev. Corr 
 Team_ID  (Intercept)   0.4202   0.6482        
          Shots_on_five 0.1486   0.3855   -1.00
 Residual               4.8233   2.1962        
Number of obs: 300, groups:  Team_ID, 30
```

and for the unconditional means model:

```
Random effects:
 Groups   Name        Variance Std.Dev.
 Team_ID  (Intercept) 14.96    3.868   
 Residual             14.61    3.822   
Number of obs: 300, groups:  Team_ID, 30
```

We can compute the reduction in Level-1 residual variance as:

$$
R^2_{\epsilon} = \frac{14.61 - 4.8233}{14.61} = 0.670
$$

The reduction in level-1 variance (from Model 0) was 67.0\%. Similarly for the level-2 intercept residuals:

$$
R^2_{\eta_{0}} = \frac{14.96 - 0.4202}{14.96} = 0.972
$$

The reduction in level-2 variance (from Model 0) was 97.2\%.


What about the reduction in level-2 residual slope varianbce? The unconditional means model does not include level-2 residuals for slope so we cannot compare a model under study to that model's slope residuals. Instead we compare them to the simplest model that includes slope residuals; namely Model 2.

```
Random effects:
 Groups   Name          Variance Std.Dev. Corr
 Team_ID  (Intercept)   0.09279  0.3046       
          Shots_on_five 0.09913  0.3148   1.00
 Residual               5.10616  2.2597       
Number of obs: 300, groups:  Team_ID, 30
```

$$
R^2_{\eta_{1}} = \frac{0.09913 - 0.1486}{0.09913} = -.499
$$

The reduction in level-2 slope variance (from Model 2) was $-49.9$\%.


In mixed effects models, these different computations for the pseudo-$R^2$ give give differing results. 

$$
\begin{split}
R^2_{Y,\hat{Y}} &= 0.842\\
R^2_{\epsilon} &= 0.670\\
R^2_{\eta_{0}} &= 0.972\\
R^2_{\eta_{1}} &= -.499
\end{split}
$$

Which of these you report depends on what you are interested in, level-1 variation or level-2 variation. Also note that these computations can produce negative results; an increase in variance.

# Plotting Multilevel Models

There are two plots that you might want to consider: (1) a plot of the fixed effects model; and (2) a plot of each team's model (fixed effects + random effects). Let's consider a model that includes RE for both intercept and slopes, and a level-2 predictor, coaching experience, for the intercept: 

$$
\begin{split}
\mathrm{Life~Satisfaction}_i &= \beta^{*}_0 + \beta^{*}_1(\mathrm{SO5}) + \epsilon_i \\
\beta^{*}_0 &= \beta_{00} + \beta_{01}(\mathrm{Coach}) + \eta_{0j}\\
\beta^{*}_1 &= \beta_{10} + \eta_{1j}
\end{split}
$$

```{r tidy=FALSE}
lmer.4 = lmer(Life_Satisfaction ~ 1  + Shots_on_five + Coach_Experience + 
                (1 + Shots_on_five | Team_ID), data = nba)
summary(lmer.4)
```



The fixed effects part of that model is:

$$
\hat{\mathrm{Life~Satisfaction}_i} = 3.78 + 3.10(\mathrm{SO5}_i) + 1.66(\mathrm{Coach}_j)
$$

To plot the fixed effects model, we (1) create a dataset with the appropriate predictors; (2) use the model to estimate the Y-hats; and (3) plot.

```{r out.width='4in', out.height='4in'}
# Create data set 
plotData = expand.grid(
	Shots_on_five = seq(from = 0, to = 5, by = 0.1),
	Coach_Experience = 1.967 # set to mean
	)

# Compute y-hat values for the fixed-effects (average) model
# Use predict() with re.form=NA argument
plotData$yhat = predict(lmer.4, newdata = plotData, re.form = NA)


# Examine data
plotData

# Plot; coerce coaching experience into a factor (or do it before plotting)
ggplot(data = plotData, aes(x = Shots_on_five, y = yhat)) +
  geom_line() +
  theme_bw() +
  xlab("Shots on Five") +
  ylab("Predicted Life Satisfaction")
```

## Plotting the Fixed and Random Effects

The process for plotting these is similar to the previous plot. However, when we set up the initial data frame we use `expand.grid()` to include the level-2 grouping variable (`Team_ID`) and the predictors.


```

The frame slot produces a matrix of the outcome, predictor(s), and level-2 ID for each student. This is what we need in Step 1 of the plotting process.

```{r out.width='4in', out.height='4in'}
plotData = expand.grid(
	Shots_on_five = seq(from = 0, to = 5, by = 0.1),
	Coach_Experience = 1.967,
	Team_ID = unique(nba$Team_ID)
	)

# Compute y-hat values for the fixed-effects (average) model
# Use predict() with re.form=NA argument
plotData$yhat = predict(lmer.4, newdata = plotData)

# Plot
ggplot(data = plotData, aes(x = Shots_on_five, y = yhat, group = Team_ID)) +
	geom_line() +
  theme_bw() +
  xlab("Shots on Five") +
  ylab("Predicted Life Satisfaction")
```



