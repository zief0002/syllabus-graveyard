---
title: "Information Criteria for Model Selection"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
bibliography: epsy8252.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3)
library(printr)
library(dplyr)
```



# Preparation


We will use the data in the *mnSchools.csv* file. These data include institutional-level attributes for several Minnesota colleges and universities. The source of these data is: [http://www.collegeresults.org](http://www.collegeresults.org). The attributes include:

- `id`: Institution ID number
- `name`: Institution name
- `gradRate`: Six-year graduation rate. This measure represents the proportion of first-time, full-time, bachelor's or equivalent degree-seeking students who started in Fall 2005 and graduated within 6 years.
- `public`: Dummy variable indicating educational sector (0 = private institution; 1 = public institution)
- `sat`: Estimated median SAT score for incoming freshmen at the institution
- `tuition`: Cost of attendance for full-time, first-time degree/certificate-seeking in-state undergraduate students living on campus for academic year 2013-14. 


```{r}
mn = read.csv(file = "~/Google Drive/Documents/github/EPsy-8252/data/mnSchools.csv")
head(mn)
```


# Comparison of OLS Estimates to ML Estimates

Before we turn to the `mnSchools` data, let's go back to our toy example and consider a regression model using x  to predict y. We will fit the model using the `lm()` function, which utilizes OLS estimation. We will also fit the model using the `mle2()` function from the **bbmle** package. This function obtains estimates using ML estimation. First we fit the `lm()` model.

```{r}
x = c(4, 0, 3, 4, 7, 0, 0, 3, 0, 2)
y = c(53, 56, 37, 55, 50, 36, 22, 75, 37, 42)

# Fit model using OLS
lm.1 = lm(y ~ 1 + x)

# Output the coefficients, SEs, t-values, and p-values
summary(lm.1)$coefficients

# Output the estimate of sigma_e
summary(lm.1)$sigma

# -2*LL
-2 * logLik(lm.1)[1]
```


To use the `mle2()` function, we need to provide a user-written function that returns the negative log-likelihood given a set of parameter inputs. For simple regression, recall that we need to estimate three parameters: $\beta_0$, $\beta_1$, and $\sigma_{\epsilon}$. Below we have a function that inputs the three parameters, uses the and returns the negative of the log-likelihood for a simple regression. (Note that the `dnorm()` function uses the `log=TRUE` argument to compute log densities.)

```{r}
regress.ll = function(b0, b1, sigma) { 
  yhats = b0 + b1 * x
  errors = y - yhats
  neg_log_lik = -sum(dnorm(errors, mean = 0, sd = sigma, log = TRUE))
  return(neg_log_lik)
} 
```

Now we can implement the `mle2()` function. This function requires the argument, `minuslogl=`, which gives the user written function returning the negative log-likelihood, it also requires a list of starting values for the input parameters in the user-written function. (Here we give values close to the OLS estimates as starting values.)

```{r message=FALSE}
# Fit model using ML
library(bbmle)
mle.results = mle2(minuslogl = regress.ll, start = list(b0 = 40, b1 = 2, sigma = 15))

# View results
summary(mle.results)
```

Compare the coefficient (`b0` and `b1`) estimates between the two methods of estimation. They are quite similar. The estimate of $\sigma_{\epsilon}$ is different (although close) between the two methods of estimation. This is because in OLS estimation, the estimate turns out to be,

$$
\hat\sigma_{\epsilon}=\frac{\left(Y_i - \hat{Y}_i\right)^2}{n-2},
$$
and for ML estimation, the estimate is,

$$
\hat\sigma_{\epsilon}=\frac{\left(Y_i - \hat{Y}_i\right)^2}{n},
$$
The smaller denominator in OLS results in a higher estimate of the variation.

This, in turn, affects the size of the SE estimates for the coefficients. Lastly, we note that the value of $-2$(log-likelihood) is the same for both the ML and OLS estimated models. This is a useful result. It allows us to use `lm()` to estimate the coefficients from a model and then use its log-likelihood as if we had fitted the model using ML. To see how this works in practice, let's fit a few models using the `mnSchools` data.

# Fit Candidate Models

```{r}
lm.a = lm(gradRate ~ 1 + public, data = mn)
lm.b = lm(gradRate ~ 1 + sat, data = mn)
lm.c = lm(gradRate ~ 1 + tuition, data = mn)
lm.d = lm(gradRate ~ 1 + public + sat, data = mn)
lm.e = lm(gradRate ~ 1 + tuition + sat, data = mn)
lm.f = lm(gradRate ~ 1 + public + tuition, data = mn)
lm.g = lm(gradRate ~ 1 + public + sat + tuition, data = mn)
```

## Compute Log-Likelihood

Now we can compute the log-likelihood values for each model.

```{r}
logLik(lm.a)
logLik(lm.b)
logLik(lm.c)
logLik(lm.d)
logLik(lm.e)
logLik(lm.f)
logLik(lm.g)
```

\newpage

## Compute Deviance

It is common to multiply these values by $-2$. This is called the *deviance*.

```{r}
-2 * logLik(lm.a)
-2 * logLik(lm.b)
-2 * logLik(lm.c)
-2 * logLik(lm.d)
-2 * logLik(lm.e)
-2 * logLik(lm.f)
-2 * logLik(lm.g)
```

## Compute AIC Values

Remember that the higher values of log-likelihood (lower values of deviance) indicate the parameters are more likely given the data. However, in this case we cannot directly compare the log-likelihoods/deviances since the models included a different number of parameters. To account for that, we will add a penalty term to the deviance,

$$
AIC = -2 \times \mathrm{LL} + 2(k)
$$
where $k$ is the number of parameters being estimated in the model (including the intercept and RMSE). Note that the value for $k$ is given as $df$ in the `logLik()` output. This value is called Akiake's Information Criteria (AIC). These values can be compared directly (so long as the same data is used and the same outcome). Smaller values of the AIC indicate a more likely model.

```{r}
-2 * logLik(lm.a)[1] + 2*3
-2 * logLik(lm.b)[1] + 2*3
-2 * logLik(lm.c)[1] + 2*3
-2 * logLik(lm.d)[1] + 2*4
-2 * logLik(lm.e)[1] + 2*4
-2 * logLik(lm.f)[1] + 2*4
-2 * logLik(lm.g)[1] + 2*5
```

Arranging these, we find that Model G (AIC = 224) is the most likely model given the data and the candidate set of models. This is the model that we should adopt. 


# Compute AICc Values

Although AIC has a penalty correction that should account for some bias, it turns out that when the number of parameters is large relative to the sample size, AIC is still biased in favor of models that have more parameters. This led @Hurvich:1989 to propose a second-order bias corrected AIC measure (AICc) computed as

$$
\mathrm{AIC_c} = \mathrm{Deviance} + 2(k)\left( \frac{n}{n - k - 1} \right)
$$
where $k$ is, again, the number of estimated parameters, and $n$ is the sample size used to fit the model. Note that when $n$ is very large (especially relative to $k$) that the last term is essentially 1 and the AICc value would basically reduce to the AIC value. When $n$ is small relative to $k$ this will add more of a penalty to the deviance. The recommendation is to pretty much always use AICc rather than AIC when selecting models. Below, we will compute the AICc for each of the six previosly fitted models. (Note that we use $n=33$ cases for the computation for all the models in this data.)

```{r}
n = 33

# Compute AICc for Model A, B, and C
k = 3
-2 * logLik(lm.a)[[1]] + 2 * k * n / (n - k - 1) # Model A
-2 * logLik(lm.b)[[1]] + 2 * k * n / (n - k - 1) # Model B
-2 * logLik(lm.c)[[1]] + 2 * k * n / (n - k - 1) # Model C


# Compute AICc for Model D, E, and F
k = 4
-2 * logLik(lm.d)[[1]] + 2 * k * n / (n - k - 1) # Model D
-2 * logLik(lm.e)[[1]] + 2 * k * n / (n - k - 1) # Model E
-2 * logLik(lm.f)[[1]] + 2 * k * n / (n - k - 1) # Model F


# Compute AICc for Model G
k = 5
-2 * logLik(lm.g)[[1]] + 2 * k * n / (n - k - 1)
```

Based on the $\mathrm{AIC_c}$ values, we would adopt Model E. It is the most likely model given the data and the six candidate models.

# Use AICc() Function

In practice, we will use the `AICc()` function from the **AICcmodavg** package to compute the AICc value directly.

```{r message=FALSE}
library(AICcmodavg)

AICc(lm.a)
AICc(lm.b)
AICc(lm.c)
AICc(lm.d)
AICc(lm.e)
AICc(lm.f)
AICc(lm.g)
```

# References


<!-- In summary, here are the four models, and their AICc values. -->

<!-- ```{r echo=FALSE} -->
<!-- modTab = data.frame( -->
<!--   Model = c("Model A", "Model B", "Model C", "Model D"), -->
<!--   AICc = c(a, b, c, d) -->
<!-- ) -->
<!-- arrange(modTab, AICc) -->
<!-- ``` -->


<!-- Model A has the lowest AICc value. Given the data, and the candidate set of models, Model A is estimated to be closest to full reality. -->



<!-- # $\Delta$AICc -->

<!-- Since the minimum AICc value is for Model A, we compute the difference between each model's AICc value and Model A's AICc value, $\Delta_i$. -->

<!-- ```{r} -->
<!-- # Compute dalta values -->
<!-- delta_a = a - a -->
<!-- delta_b = b - a -->
<!-- delta_c = c - a -->
<!-- delta_d = d - a -->
<!-- ``` -->

<!-- ```{r echo=FALSE} -->
<!-- modTab2 = data.frame( -->
<!--   Model = c("Model A", "Model B", "Model C", "Model D"), -->
<!--   Delta = c(delta_a, delta_b, delta_c, delta_d) -->
<!-- ) -->

<!-- arrange(modTab2, Delta) -->
<!-- ``` -->

<!-- Using the criteria in Figure 2 on p. 25 of Burnham, Anderson, &amp; Huyvaert (2011): -->

<!-- - Model A has a lot of empirical support -->
<!-- - Model C and B have less empirical support than Model A, but, nonetheless still a fair amount of empirical support -->
<!-- - Model D has considerably less empirical support than Model A -->




<!-- # Relative Likelihood -->

<!-- The relative likelihood provides the likelihood of each of the candidate models, given the set of candidate models. -->


<!-- ```{r} -->
<!-- rel_a = exp(-1/2 * delta_a)  -->
<!-- rel_b = exp(-1/2 * delta_b) -->
<!-- rel_c = exp(-1/2 * delta_c) -->
<!-- rel_d = exp(-1/2 * delta_d) -->
<!-- ``` -->

<!-- ```{r echo=FALSE} -->
<!-- modTab3 = data.frame( -->
<!--   Model = c("Model A", "Model B", "Model C", "Model D"), -->
<!--   relLik= c(rel_a, rel_b, rel_c, rel_d) -->
<!-- ) -->
<!-- arrange(modTab3, desc(relLik)) -->
<!-- ``` -->

<!-- Relative to the other three candidate models, Model A has the highest relative likelihood, making it the most liekly model. Model C is slightly more likely than Model B, and Model D is not a likely candidate relative to the others in the set. -->

<!-- # Evidence Ratio -->

<!-- By themselves, the weights (relative likelihoods) do no give us really any more information than we already had by examining the $\Delta_i$ values. One way to make use of these values is to compute the evidence ratio for each model. If we use the relative likelihood of the best candidate model in the numerator, the resulting evidence ratio allows for a comparison of each candidate model to the best model. -->

<!-- ```{r} -->
<!-- er_a = rel_a / rel_a  -->
<!-- er_b = rel_a / rel_b   -->
<!-- er_c = rel_a / rel_c -->
<!-- er_d = rel_a / rel_d -->
<!-- ``` -->

<!-- ```{r echo = FALSE} -->
<!-- modTab4 = data.frame( -->
<!--   Model = c("Model A", "Model B", "Model C", "Model D"), -->
<!--   ER = c(er_a, er_b, er_c, er_d) -->
<!-- ) -->
<!-- arrange(modTab4, ER) -->
<!-- ``` -->

<!-- - Given the data, and the set of candidate models, the empirical support for Model A is 4.26 times that for Model B.  -->
<!-- - Given the data, and the set of candidate models, the empirical support for Model A is 3.94 times that for Model C. -->
<!-- - Given the data, and the set of candidate models, the empirical support for Model A is 19.70 times that for Model D. -->



<!-- # Model Probability -->

<!-- Also referred to as Akaike Weights, these provide a numerical measure of the probability of each model given the data and the candidate set. -->

<!-- ```{r} -->
<!-- mp_a = rel_a / (rel_a + rel_b + rel_c + rel_d) -->
<!-- mp_b = rel_b / (rel_a + rel_b + rel_c + rel_d) -->
<!-- mp_c = rel_c / (rel_a + rel_b + rel_c + rel_d) -->
<!-- mp_d = rel_d / (rel_a + rel_b + rel_c + rel_d) -->
<!-- ``` -->

<!-- ```{r echo = FALSE} -->
<!-- modTab5 = data.frame( -->
<!--   Model = c("Model A", "Model B", "Model C", "Model D"), -->
<!--   Prob = c(mp_a, mp_b, mp_c, mp_d) -->
<!-- ) -->
<!-- arrange(modTab5, desc(Prob)) -->
<!-- ``` -->

<!-- - Given the data, and the set of candidate models, the probability of Model A is 0.650. -->
<!-- - Given the data, and the set of candidate models, the probability of Model B is 0.152. -->
<!-- - Given the data, and the set of candidate models, the probability of Model C is 0.165. -->
<!-- - Given the data, and the set of candidate models, the probability of Model D is 0.033. -->





<!-- We will use the `aictab()` function from the **AICcmodavg** package to compute these values directly from the `lm()` fitted models. This function takes a list of models in the candidate set (it actually has to be an R list). The optional argument `modnames=` is a vector of model names associated with the models in the candidate set. -->

<!-- ```{r} -->
<!-- library(AICcmodavg) -->

<!-- # AICc Table for Model Selection -->
<!-- myAIC = aictab( -->
<!--   cand.set = list(lm.a, lm.b, lm.c, lm.d, lm.e, lm.f, lm.g), -->
<!--   modnames = c("Model A", "Model B", "Model C", "Model D", "Model E", "Model F", "Model G") -->
<!--   ) -->

<!-- # View table -->
<!-- myAIC -->
<!-- ``` -->

<!-- Note the output includes the number of parameters ($k$) and AICc value for each candidate model, and prints them in order from most likely to least likely based on the AICc. It also includes several other values which we will discuss in the future.  -->

<!-- , $\Delta$AICc value, model probability (AICcWt), cumulative probability (Cum.Wt) and log-likelihood (LL). We have to compute the evidence ratios separately. -->

<!-- ```{r} -->
<!-- # Evidence Ratios -->
<!-- evidence(myAIC, model.high = "Model A", model.low = "Model B") -->
<!-- evidence(myAIC, model.high = "Model A", model.low = "Model C") -->
<!-- evidence(myAIC, model.high = "Model A", model.low = "Model D") -->
<!-- ``` -->



<!-- ```{r} -->
<!-- # Examine object -->
<!-- str(myAIC) -->
<!-- ``` -->

<!-- Since `myAIC` is a data frame, we can add columns, use indexing, submit results to `ggplot()`, etc. Here we compute the evidence ratios using information from the `myAIC` object, and add them as an additional column. -->

<!-- ```{r} -->
<!-- # Create new column -->
<!-- myAIC$ER = max(myAIC$AICcWt) / myAIC$AICcWt -->
<!-- myAIC -->
<!-- ``` -->

<!-- We can also customize the output to only include information we are interested in. For example, we could remove columns, re-order columns, or change column names. -->

<!-- ```{r} -->
<!-- # Get existing column names -->
<!-- names(myAIC) -->

<!-- # Select and order columns for better printing -->
<!-- myAIC = myAIC[ , c("Modnames", "LL", "K", "AICc", "Delta_AICc", "ER", "AICcWt")] -->

<!-- # Rename columns for better printing -->
<!-- names(myAIC) = c("Model", "Log Lik.", "K", "AICc", "$\\Delta$AICc", "Evidence Ratio", "Model Probability") -->
<!-- ``` -->

<!-- ```{r echo=FALSE} -->
<!-- knitr::kable(myAIC, row.names = FALSE) -->
<!-- ``` -->

<!-- All of this evidence points to Model A as the best candidate model given the data. Model B and C have some tenability, but are not nearly as supported as Model A. Finally, Model D seems almost completely unsupported. -->





