---
title: "Quantifying Sampling Error"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
bibliography: epsy8252.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3, scipen = 99)
library(printr)
library(dplyr)
```

# Preparation


We will eventually use the data in the *mnSchools.csv* file. These data include institutional-level attributes for several Minnesota colleges and universities. The source of these data is: [http://www.collegeresults.org](http://www.collegeresults.org). The attributes include:

- `id`: Institution ID number
- `name`: Institution name
- `gradRate`: Six-year graduation rate. This measure represents the proportion of first-time, full-time, bachelor's or equivalent degree-seeking students who started in Fall 2005 and graduated within 6 years.
- `public`: Dummy variable indicating educational sector (0 = private institution; 1 = public institution)
- `sat`: Estimated median SAT score for incoming freshmen at the institution
- `tuition`: Cost of attendance for full-time, first-time degree/certificate-seeking in-state undergraduate students living on campus for academic year 2013-14. 


```{r}
mn = read.csv(file = "~/Google Drive/Documents/github/EPsy-8252/data/mnSchools.csv")
head(mn)
```


# Sampling Distributions

A *sampling distribution* is the probability distribution of a particular statistic based on drawing ALL possible random samples from a population and computing that statistic for each of them. When the population is infinitely large, the sampling distribution is a pure theoretical construct and can only be derived from theoretical results (e.g., mathematical proof) when they are known, or approximated using other methods (e.g., simulation). The goal for statisticians is to **quantify the variation** in the statistic that is due to random sampling---quantify the sampling error. 

To understand this, let's work with a more concrete example. Let's assume we have FULL information about the population we are sampling from: Normally distributed with a mean of 0 and a standard deviation of 1. Let's also assume we want to draw samples of size 100 from this population, and for each sample we will collect the sample mean. Our goal is to describe/quantify the variation in the mean that is due to sampling error. Visually, this looks like the following:

\begin{center}
\includegraphics[width=5in]{images/thought-experiment-means.pdf}
\end{center}

## Statistical Theory

Because we have full information about the population and it is normally distributed, we can use statistical theory to help with achieving our goal. Theory says that if the population is normally distributed, that the sampling distribution of the sample mean will also be normally distributed. The mean of the sampling distribution of the sample mean (that's a mouthful, so statisticians use the term *expected value*) will be identical to the population mean ($E(\bar{x}) = \mu$). The standard deviation of the sampling distribution of the sample mean (the standard error) will be equal to the standard deviation of the population divided by the square root of the sample size ($\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$). Thus, using theory, for our example, the sampling distribution for the sample mean will be

- Normally distributed
- $E(\bar{x}) = 0$
- $\mathrm{SE}_{\bar{x}} = \frac{1}{\sqrt{100}} = 0.1$

The goal was to quantify the sampling error, which we did when we computed the standard error. This, along with the expected value, tells us that, on average, the sample mean (computed from a random sample of size 100 drawn from our known population) will have a value of 0. However, we should not be surprised if the sample mean deviates from zero by as much as 0.2 (twice the SE). 

## Simulation

We can also approximate the shape, center, and standard error of the sampling distribution of the sample mean using simulation. In R, to draw from a given normal distribution we use the `rnorm()` function. Below, we draw 100 random observations from the normal distribution having a mean of 0 and a standard deviation of 1 and also compute its mean.

```{r}
mean(rnorm(n = 100, mean = 0, sd = 1))
```

If we do this many, many times, each time recording the sample mean, we can then examine the probability distribution of these means (to approximate the shape), compute the mean value of all these means (to approximate the expected value), and compute the standard deviation of all these means (to approximate the standard error). The `do()` function from the **mosaic** package will help us record and collect these means. Below we carry out this sampling process 10,000 times.

```{r message=FALSE, out.width='3.5in'}
library(mosaic)

myMeans = do(10000) * mean(rnorm(n = 100, mean = 0, sd = 1))
#head(myMeans)

# Plot the distribution of sample means
library(sm)
sm.density(myMeans$mean, xlab = "Sample mean")

# Compute the mean of the sample means
mean(myMeans$mean)

# Compute the sd of the sample means
sd(myMeans$mean)
```

Remember that these are approximations of the values that we got using statistical theory. But, you can see that they are reasonable approximations of these values. The shape looks approximately normal, the mean of the simulated means is close to 0, and the approximation of the standard error is close to 0.1.

# Partial Information

To simulate, or to use theory, we needed to have FULL information about the population, namely its shape, the mean, and the standard deviation. What happens when we don't have full information? Well, in those cases, the theory leads only to approximations as well (and only under certain conditions). For example, when the standrad deviation of the population is unknown, then we have to use the standard deviation of each random sample as an approximation of this value. In that case, the resulting sampling distribution of the sample mean is not normally distributed. The additional uncertainty added from having to use the sample standard deviation as an approximation to the population standard deviation makes the sampling distribution $t$-distributed. Or, when the population shape is not normally distributed, we have to rely on a result from theory called the *Central Limit Theorem* which says that if the sample size is large enough, then even though the population wasn't normally distributed, the sampling distribution of the sample mean will still be approximately normal. 

But, the known theory under partial information is quite limited.

- **It mostly requires a normal distribution of the population.** When the population is not normally distributed, the theory we have is much more limited and weaker. There is a reason that the asusmptions for most of the tests/CIs you have learned to this point have ana assumption that the population is normally distributed.
- **We only have theory for some statistics.** We know a lot about the sampling distribution of the sample mean, regression coefficients, and some others. There is little theory about the sampling distribution for other statistics. For example, there is no formula to compute the standard error from the sampling distribution of the sample median. Nor can we compute from a formula the SE for statistics such as $R^2$.

# Bootstrapping: An Alternative Method to Approximate the SE

Bootstrapping is a simulation-based approach to approximating the SE. The advantages of bootstrapping are that we do not need full information about the population (in fact, it has even been used when there is essentially no information about the population). It is also a method that works to approximate the SE across almost all statistics you may want to find the SE for. 

The basic premise of bootstrapping is that any one sample is a smaller reflection of the population. In that sense, we can substitute the sample for the population and use simulation to *resample* from this sample. This is called the "plug-in" principle. (We are plugging-in the sample for the population.) Visually we depict this concept below.

\begin{center}
\includegraphics[width=5in]{images/bootstrap.pdf}
\end{center}

If we were to resample $n$ observations out of a sample that itself has $n$ observations, we would get the same $n$ observations each time. To avoid this problem, when we resample from the observed sample, we resample $n$ observations WITH REPLACEMENT. To illustrate this, let's sample 100 observations from the population we have been working with. We will call this our observed sample. (The `set.seed()` function allows us to replicate this by giving us the same observed sample.)

```{r}
set.seed(567)
observed = rnorm(n = 100, mean = 0, sd = 1)
observed
```

The `resample()` function from the **mosaic** package draws a bootstrap sample (samples with replacement).

```{r}
resample(observed)
```

It randomly sampled the same number of observations as was in the observed sample ($n = 100$). However, it is not sampling form the population, but from the observed sample. It is also sampling with replacement. We will use follow the same simulation procedure we did with sampling from the population, but instead we will resample from our observed data.

```{r out.width='3.5in'}
myBootstrap = do(10000) * mean(resample(observed))
#head(myBootstrap)

# Plot the distribution of bootstrapped means
sm.density(myBootstrap$mean, xlab = "Bootstrapped mean")

# Compute the mean of the bootsrapped means
mean(myBootstrap$mean)

# Compute the sd of the bootstrapped means
sd(myBootstrap$mean)
```

Here the bootstrap distribution of resampled means has qualities which are based on the observed sample. In other words, the expected value of the bootstrapped means is approximately equal to the mean of the observed sample. The shape of the bootstrap distribution of resampled means is relateed to the shape of the observed sample. Those are byproducts of the resampling process. Recall that our goal was to estimate the sampling error (not the shape or mean). That is where bootstrapping shines. Notice that the bootstrapped SE is approximately equal to the SE we computed from theory (and approximated by sampling from the population through simulation), 0.1. This means, all we need is a sample of data from which we can bootstrap to obtain an approximation of the SE; we do not need the population from which that sample was drawn from. 

## Bootstrapping Example

Let's use the Minnesota College data to see how we can use bootstrapping in practice. Say we want to estimate the median graduation rate and we want to give an range (interval) of plausible values for this estimate. In a sense this is akin to finding a confidence interval for the median. First, we compute the sample median:

```{r}
median(mn$gradRate)
```

Now we want to obtain an estimate how much the sample median varies because of sampling error. To do this, we will draw bootstrap resamples from the observed data and compute the median for each of these. Then we can find the standard deviation of these bootstrapped medians. 

```{r}
myBootstrap = do(10000) * median(resample(mn$gradRate))
#head(myBootstrap)

# Compute the sd of the bootstrapped medians
se_median = sd(myBootstrap$median)
se_median
```

Combining this with our observed estimate of 63.5, we can produce an interval estimate. Typically we add/subtract two standard errors to the observed estimate in order to obtain our interval estimate. This is appropriate if the sampling distribution is symmetric. Although we don't have the sampling distribution, the bootstrap distribution should have some of the same qualities, so we will plot that distribution in order to evaluate its symmetry.

```{r out.width='3.5in'}
sm.density(myBootstrap$median, xlab = "Bootstrapped median")
```

## Percentile Interval

The bootstrap distribution seems slightly left-skewed. To compensate for that, we will compute a *percentile interval* rather than adding/subtracting two SEs to the observed median. In a normal distribution, adding and subtracting two SEs is akin to encompassing the middle 95\% of the sampling distribution. The percentile interval mimics this, but finds the middle 95\% of the bootstrap distribution. To do this, we arrange the bootstrapped medians in order from smallest to largest. Then we drop the lowest 2.5\% and the highest 2.5\% (leaving the middle 95\%). The smallest and largest value left indicate the endpoints of our interval. We can quickly access these values by using the `quantile()` function.

```{r}
lower_limit = quantile(myBootstrap$median, probs = 0.025)
lower_limit

upper_limit = quantile(myBootstrap$median, probs = 0.975)
upper_limit
```

The percentile interval estimate is then [`r round(lower_limit, 1)`, `r round(upper_limit, 1)`].

Note if we had used the $\pm 2(SE)$ method our interval would have been [`r round(63.5 - 2*se_median, 1)`, `r round(63.5 + 2*se_median, 1)`]. While this is close to what we got for the percentile interval, it is different because of the asymmetry in the bootstrap distribution. When the sampling/bootstrap distribution is symmetric, the intervals prodeuce using these two methods give very similar endpoints (in the case of the normal distribution, they are exactly the same). The more asymmetric the distributio the more different the results produced by these two methods. 





