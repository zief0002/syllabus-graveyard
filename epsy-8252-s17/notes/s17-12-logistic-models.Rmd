---
title: "Logistic Models"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
bibliography: epsy8252.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3, scipen = 99)
library(printr)
```

# Preparation

We will use the data from the *graduation.csv* file in these notes. These data include student-level attributes for $n=2344$ randomly sampled students who were first-time, full-time freshman from the 2002 cohort at a large, midwestern research university classified by the Carnegie Foundation as having very high research activity. Any students who transferred to another institution were removed from the data. The source of these data is: @Jones-White:2014. We will use these data to explore predictors of college graduation.

The attributes in the *graduation.csv* file include:

- `degree`: Did the student graduate from the institution? (0 = No; 1 = Yes)
- `act`: Studentâ€™s ACT score (If the student reported a SAT score, a concordance table was used to transform the score to a comparable ACT score.)
- `scholarship`: Amount of scholarship offered to student (in thousands of dollars)
- `ap`: Number of Advanced Placement credits at time of enrollment
- `firstgen`: Is the student a first generation college student? (0 = No; 1 = Yes)
- `nontrad`: Is the student a non-traditional student (older than 19 years old at the time of freshman enrollment)? (0 = No; 1 = Yes)


```{r message=FALSE}
# Read in player-level and team-level data
library(readr)
grad = read_csv(file = "~/Google Drive/Documents/EPsy-8252/data/graduation.csv")
head(grad)

# Load other libraries
library(AICcmodavg)
library(broom)
library(dplyr)
library(ggplot2)
library(sm)
```

\newpage

# Data Exploration

To begin the analysis, we will explore the outcome variable `degree`. Since this is a dichotomous variable, we can look at counts/proportions. The analysis suggests that most freshmen who enroll at this institution tend to graduate (73\%). 

```{r}
grad %>% group_by(degree) %>% summarize(Count = n(), Prop = n() / nrow(grad))
```

We will also explore the `act` variable, which we will use as a predictor in the analysis. 

```{r out.width='3.5in'}
sm.density(grad$act, xlab = "ACT score")
grad %>% summarize( M = mean(act), SD = sd(act) )
```

The plot is approximately normally distributed and indicates that freshmen at this institution have a mean ACT score near 25. While there is a great deal of variation in ACT scores (scores range from 10 to 36), most students have a score between 21 and 29. 

\newpage

## Relationship between ACT Score and Graduation

When the outcome variable was continous, we examined relationships graphically via a scatterplot. If we try that when the outcome is dichotomous, we run into problems.

```{r out.width='3in'}
ggplot(data = grad, aes(x = act, y = degree)) +
  geom_point() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Graduated")
```

Since there are only two values for the outcome, many of the observations are over-plotted, and it is impossible to tell anything about the relationship between the variables. One solution to this problem is to add (or subtract) a tiny amount to each student's `degree` value. This is called "jittering" the observations". To do this, use the `jitter()` function. (Note that the amount of jittering can be adjusted by including an additional argument to the `jitter()` function.)

\newpage

```{r out.width='3in'}
ggplot(data = grad, aes(x = act, y = jitter(degree))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Graduated")
```

This spreads out the observations vertically, so we no longer have the problem of overplotting. But, the relationship is still difficult to discern. Adding the regression smoother helps us see that there is a positive relationship between ACT scores and graduation. 

What does this mean? To understand this, we have to think about what the linear regression is modeling. Remember that the regression model is predicting the average $Y$ at each $X$. Since our $Y$ is dichotomous, the average represents the proportion of students with a 1. In other words, the regression predicts the proportion of students who graduate for a particular ACT score. The positive relationship between ACT score and graduation suggests that higher ACT scores are associated with higher proportions of students who graduate. We can also see this relationship by examining the correlation matrix between the two variables.

```{r}
cor(grad[ , c("degree", "act")])
```

\newpage

# Fitting the Linear Probability (Proportion) Model

We can fit the linear model shown in the plot using the `lm()` function as we always have.

```{r}
lm.1 = lm(degree ~ 1 + act, data = grad)
summary(lm.1)
```

Differences in ACT score account for 3.8\% of the variation in graduation. This is statistically significant, $F(1,~2342)=92.8$, $p<.001$. The fitted model is

$$
\hat{\pi}_i = 0.22 + 0.02(\mathrm{ACT~Score}_i)
$$

where $\hat{\pi}_i$ is the predicted proportion of students who graduate. Interpreting the coefficients,

- On average, 0.22 of students having an ACT score of 0 are predicted to graduate. (Extrapolation)
- Each one-point difference in ACT score is associated with an additional 0.02 predicted improvement in the proportion of students graduating, on average.

Let's examine the model assumptions.

```{r out.width='3in'}
out = augment(lm.1)
#head(out)

# Examine normality assumption
sm.density(out$.std.resid, xlab = "Standardized residuals")

# Examine linearity and homoskedasticity
ggplot(data = out, aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  theme_bw() +
  xlab("Fitted values") +
  ylab("Standardized residuals")
```

It is clear that the assumptions associated with linear regression are violated. First off, the residuals are not normally distributed. They are in fact, bimodal. The scatterplot of the residuals versus the fitted values also indicates violation of the linearity assumption.

\newpage

## Understanding the Residuals from the Linear Probability Model

Look closely at the scatterplot of the residuals vs. the fitted values. At each fitted value, there are only two residual values. Why is this? Recall that residuals are computed as $\epsilon_i=Y_i - \hat{Y}_i$. Now, remember that $Y_i$ can only be one of two values, 0 or 1. Also remember that in the linear probability model $\hat{Y}_i=\hat{\pi}_i$. Thus, for $Y=0$,

$$
\begin{split}
\epsilon_i &= 0 - \hat{Y}_i \\
&= - \hat{\pi}_i
\end{split}
$$

And, if $Y=1$,

$$
\begin{split}
\epsilon_i &= 1 - \hat{Y}_i \\
&= 1 - \hat{\pi}_i
\end{split}
$$


This means that the residual computed using a particular fitted value (or from a particular ACT value given that the fitted values is a function of $X$) can only take on one of two values: $- \hat{\pi}_i$ or $1 - \hat{\pi}_i$. (Plotting the residuals for all fitted values, the marginal distribution of the residuals, results in a bimodal distribution for this same reason.)

# Alternative Models to the Linaer Probability Model

Using the linear probability model leads to direct violations of the linear model's assumptions. If that isn't problematic enough, it is possible to run into severe issues when we make predictions. For example, given the constant effect of $X$ in these models it is possible to have an $X$ value that results in a predicted proportion that is either greater than 1 or less than 0. This is a problem since proportions are constrained to the range of $\left[0,~1\right]$.

Before we consider any alternative models, let's actually examine the empirical proportions of students who graduate at different ACT scores.

```{r out.width='3in'}
graduates = grad %>% 
  group_by(act, degree) %>% 
  summarize( N = n() ) %>% 
  mutate( Prop = N / sum (N) ) %>%
  filter(degree == 1)

# View data
graduates

# Plot proportions
ggplot(data = graduates, aes(x = act, y = Prop)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Proportion of graduates")
```

The loess smoother suggests that the proportion of students who graduate is a non-linear function of ACT scores. 

\newpage

Many of the non-linear models that are typically used to model dichotomous outcome data are "S"-shaped models. Below is a plot of one-such "S"-shaped model.

```{r echo=FALSE, out.width='3in'}
x = seq(from = -4, to = 4, by = .01)
y = 1 / (1 + exp(-(0 + 4*x)))

plot(x, y, type = "l", xlab = "X", ylab = "P(X = 1)")
```

The non-linear "S"-shaped model has many attractive features. First, the predicted $Y$ values are bounded between 0 and 1. Furthermore, as $X$ gets smaller, the proportion of $Y=1$, approaches 0 at a slower rate. Similarly, as $X$ gets larger, the proportion of $Y=1$, approaches 1 at a slower rate. Lastly, this model curve is monotonic; smaller values of $X$ are associated with smaller proportions of $Y=1$ (or if the "S" were backwards, larger values of $X$ would be associated with smaller proportions of $Y=1$). The key is that there are no bends in the curve; it is always growing or always decreasing. 

In our graduation example, the empirical data maps well to this curve. Higher ACT scores are associated with a higher proportion of students who graduate (monotonic). The effect of ACT, however, is not constant, and seems to diminish at higher ACT scores. Lastly, we want to bound the proportion at every ACT score to lie between 0 and 1.

How do we fit such an "S"-shaped curve? We apply a transformation function, call it $\Phi$, to the predicted values. Mathematically,

$$
\Phi(\pi_i) = \Phi\bigg[\beta_0 + \beta_1(X)\bigg]
$$

The specific transformation function used is any mathematical function that can fit the criteria we had before (monotonic, nonlinear, maps to $[0,~1]$ space). There are several mathematical functions that do this. One common function that meets these specifications is the *logistic function*. Mathematically, the logistic function is

$$
\Phi(w) = \frac{1}{1 + e^{-w}}
$$

where $w$ is the value fed into the logistic function. For example, to logistically transform $w=3$, we use

$$
\begin{split}
\Phi(3) &= \frac{1}{1 + e^{-3}} \\
&= 0.953
\end{split}
$$

\newpage

Below we show how to transform many such values using R.

```{r out.width='3.5in'}
# Set up values
w = seq(from = -4, to = 4, by = 0.01)

# Transform using logistic function
phi = 1 / (1 + exp(-w))

# Plot the results
plot(x = w, y = phi, type = "l")
```

You can see that by using this transformation we get a monotonic "S"-shaped curve. Now try substituting a really large value of $w$ into the function. This gives an asymptote at 1. Also substitute a really "large"" negative value in for $w$. This gives an asymptote at 0. So this function also bounds the output between 0 and 1.

How does this work in a regression? There, we are transforming the predicted values, the $\pi_i$ values.

$$
\begin{split}
\Phi(\hat\pi_i) &= \Phi\bigg[\beta_0 + \beta_1(X_i)\bigg] = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
\end{split}
$$

Since we took a linear model ($\beta_0 + \beta_1(X_i)$) and applied a logistic transformation, the resulting model is the *linear logistic model* or more simply, the *logistic model*. 

\newpage

## Re-Expressing a Logistic Transformation

The logistic model expresses the proportion of 1s $\pi_i$ as a function of the predictor $X$. It can be mathematically expressed as

$$
\pi_i = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
$$

We can re-express this using algebra and rules of logarithms.

$$
\begin{split}
\pi_i &= \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}} \\
\pi_i \times (1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]} ) &= 1 \\
\pi_i + \pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 \\
\pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 - \pi_i \\
e^{-\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{1 - \pi_i}{\pi_i} \\
e^{\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{\pi_i}{1 - \pi_i} \\
\ln \bigg(e^{\big[\beta_0 + \beta_1(X_i)\big]}\bigg) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i} \bigg) \\
\beta_0 + \beta_1(X_i) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg)
\end{split}
$$

Or,

$$
\ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg) = \beta_0 + \beta_1(X_i)
$$

The logistic model expresses the log of $\frac{\pi_i}{1 - \pi_i}$ as a linear function of $X$. 

### Log-Odds or Logits

The ratio that we are taking the logarithm of, $\frac{\pi_i}{1 - \pi_i}$, is referred to as *odds*. Odds are the ratio of two probabilities. Namely the chance an event occurs ($\pi_i$) versus the chance that same event does not occur ($1 - \pi_i$). As such, it gives the *relative chance* of that event occurring. To understand this better, we will look at a couple examples.

Let's assume that the probability of getting an "A" in a course is 0.7. Then we know the probability of NOT getting an "A" in that course is 0.3. The odds of getting an "A" are then

$$
\mathrm{Odds} = \frac{0.7}{0.3} = 2.33
$$

That the probability of getting an "A" in the class is 2.33 times as likely as NOT getting an "A" in the class. This is the relative probability of getting an "A". 

\newpage

As another example, Fivethirtyeight.com computed the [probability that a Canadian hockey team would win the Stanley Cup as 0.17](https://fivethirtyeight.com/features/will-canada-end-its-stanley-cup-drought-well-its-not-impossible/). The odds of a Canadian team winning the Stanley Cup is then

$$
\mathrm{Odds} = \frac{0.17}{0.83} = 0.21
$$

The probability that a Canadian team wins the Stanley Cup is 0.21 times as likely as a Canadian team NOT winning the Stanley Cup. (Odds less than 1 indicate that is is more likely for an event NOT to occur than to occur. Invert the fraction to compute how much more like the event is not to occur.)

In the logistic model, we are predicting the log-odds (also referred to as the *logit*. When we get these values, we typically transform the logits to odds by inverting the log-transformation (take $e$ to that power.) 

# Fitting the Logistic Model in R

To fit a logistic model using R, we use the `glm()` function. GLM stands for Generalized Linear Model. It is an extension of the Linear Model (that we have been working with for two semesters) that allows for a transformation of the predicted values to allow for modeling residuals that follow other probability distributions. The transformation used is different depending on which probability distribution needs to be modeled. For example,

\begin{table}[H]
\centering
\label{my-label}
\begin{tabular}{p{6cm}p{2cm}p{3cm}}
\hline
\bf{Typical Use}  & \bf{Distribution} & \bf{Transformation} \\
\hline
Modeling counts/proportions of 1s in $k=2$ categories & Binomial & Logistic function \\
\hline
Modeling average Y in continuous data & Normal & Identity function \\
\hline
Modeling count/proportion of occurrences in fixed amount of time/space & Poisson & Log function \\
\hline
Modeling counts/proportions of 1s in $k>2$ categories & Multinomial & Logistic function \newline (multiple equations)\\
\hline
\end{tabular}
\end{table}

The transformation function is often referred to as the *link function*; it is the function that links the probability distribution to the linear model. The syntax to fit the logistic model using `glm()` is

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{1~+~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula depicting the model and the `data=` arguments are specified in the same manner as in the `lm()` function. Since the model is a generalized model, we need to specify the residual distribution (binomial) and the link function (logit) via the `family=` argument. For our example,

```{r}
glm.1 = glm(degree ~ 1 + act, data = grad, family = binomial(link = "logit"))
```

\newpage

The output of the model can be printed using `summary()`.

```{r}
summary(glm.1)
```

The fitted equation for the model is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.61 + 0.11(\mathrm{ACT~Score}_i)
$$

We interpret the coefficients in the same manner as we interpret coefficients from a linear model, with the caveat that the outcome is now in log-odds (or logits):

- For students with an ACT score of 0, their predicted log-odd of graduating are $-1.61$.
- Each one-point difference in ACT score is associated with a difference of 0.11in the predicted log-odds of graduating, on average.

For better interpretations, we can back-transform log-odds to odds. This is typically a better metric for interpretation of the coefficients. To back-transform to odds, we exponentiate both sides of the fitted equation and use the rules of exponents to simplify:

$$
\begin{split}
e^{\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg)} &= e^{-1.61 + 0.11(\mathrm{ACT~Score}_i)} \\
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(\mathrm{ACT~Score}_i)}
\end{split}
$$

When ACT score = 0, the predicted odds are


$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(0)} \\
&= e^{-1.61} \times 1 \\
&= e^{-1.61} \\
&= 0.2
\end{split}
$$

For students with an ACT score of 0, their odds of graduating is 0.2. That is, for these students, the probability of graduating is 0.2 times that of not graduating. (It is far more likey these students will not graduate.)

To interpret the effect of ACT on the odds of graduating, we will compare the odds of graduating for students that have ACT score that differ by one point. Say ACT = 0 and ACT = 1.

We already know the predicted odds for students with ACT = 0, namely $e^{-1.61}$. For students with an ACT of 1, their predicted odds of graduating are

$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(1)} \\
&= e^{-1.61} \times e^{0.11} \\
\end{split}
$$

These students odds of graduating are $e^{0.11}$ times greater than students with an ACT score of 0. Moreover, this increase in the odds, on average, is the case for every one-point difference in ACT score. In general,

- The predicted odds for $X=0$ are $e^{\hat\beta_0}$.
- Each one-unit difference in $X$ is associated with a $e^{\hat\beta_1}$ times increase (decrease) in the odds.

We can obtain these values in R by using the `coef()` function to obtain the fitted model's coefficients and then exponentiating them using the `exp()` function. 

```{r}
exp(coef(glm.1))
```

From these values, we interpret the coefficients in the odds metric as

- The predicted odds of graduating for students with an ACT score of 0 are 0.20.
- Each one-unit difference in ACT score is associated with 1.11 times greater odds of graduating.

## Plotting the Results from the Fitted Model

To even further understand and interpret the fitted model, we can plot the fitted values for a range of ACT scores. Typically the proportion of students graduating ($\hat\pi_i$) is what should be plotted, as that is what we were initially modeling. This process is exactly the same as the process of plotting fitted model results for any of the other models we have worked with. The only difference is that when we use the `predict()` function to get the fitted values, we have to specify that we want the $\hat\pi_i$ values (the default is logits). We do this by including the argument `type="response"`. Below we plot the results from our fitted logistic model.

```{r out.width='3in'}
plotData = expand.grid(
  act = seq(from = 10, to = 36, by = 1)
)

plotData$pi_hat = predict(glm.1, newdata = plotData, type = "response")

ggplot(data = plotData, aes(x = act, y = pi_hat)) +
  geom_line() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1)
```

The monotonic increase in the curve indicates the positive effect of ACT score on the probability of graduating. (Note that we typically interpret $\pi_i$ as probability rather than proportion when interpreting logistic models.) The magnitude of this effect depends on ACT score. For lower ACT scores there is a larger effect of ACT score on the probability of graduating than for higher ACT scores.

# Model-Level Summaries

The `summary()` output for the GLM model also included model-level information. For the model we fitted, the model-level information was

```
    Null deviance: 2722.5  on 2343  degrees of freedom
Residual deviance: 2633.2  on 2342  degrees of freedom
AIC: 2637

Number of Fisher Scoring iterations: 4
```

The metric of measuring residual fit is the deviance (remember the deviance was $-2 \times$ log-likelihood). The *null deviance* is the residual deviance from fitting the intercept-only model.

```{r}
glm.0 = glm(degree ~ 1, data = grad, family = binomial(link = "logit"))
deviance((glm.0))
```


The *residual deviance* is the residual deviance from fitting whichever model was fitted, in our case the model that used ACT score as a predictor.

```{r}
deviance(glm.1)
```

Recall that deviance is akin to the sum of squared residuals (SSE), smaller values indicate less error. In our case, the model that includes ACT score as a predictor has less error than the intercept only model; its deviance is 90 less than the intercept-only model. 

There are two ways to determine whether this decrease in deviance is statistically significant. The first is to examine the significance of the ACT predictor in the fitted model. Since that is the only predictor included above-and-beyond the intercept, the $p$-value associated with it indicates whether the ACT predictor is statistically relevant.

The second method to test the improvement in deviance is a test of nested models. Since the intercept-only model is nested in the model that includes ACT as a predictor, we can use a *Likelihood Ratio Test* to examine this. To do so, we use the `anova()` function with the added argument `test="LRT"`.  

```{r}
anova(glm.0, glm.1, test = "LRT")
```

The null hypothesis of this test is that there is NO improvement in the deviance. The results of this test, $\chi^2(1)=89.3$, $p<.001$, indicate that we should reject the null hypothesis. The more complex model has significantly less error than the intercept-only model and should be adopted.

We can also use information criteria for making decisions about effects.

```{r}
myAIC = aictab(
  cand.set = list(glm.0, glm.1),
  modnames = c("Intercept-only", "ACT score")
)

myAIC
```

Here, given the data and the candidate set of models, there is overwhelming evidence to support the model that includes ACT score.

\newpage

# Adding Covariates

Including covariates in the logistic model is done the same way as for `lm()` models. For example, say we wanted to examine the effect of ACT score on probability of graduating, after controlling for whether or not a student was first generation college student. We fit that model as

```{r}
glm.2 = glm(degree ~ 1 + act + firstgen, data = grad, family = binomial(link = "logit"))
summary(glm.2)
```

The fitted equation is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.48 + 0.09(\mathrm{ACT~Score}_i) + 0.52(\mathrm{First~Generation}_i)
$$

Using the logit/log-odds metric, we interpret the coefficients as:

- For students who are not first generation college students with an ACT score of 0, the predicted log-odds of graduating are $-1.48$, on average.
- Each one-point difference in ACT score is associated with a difference of 0.09 in the predicted log-odds of graduating, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, have a predicted log-odds of graduating that is 0.52 higher than students who are not first generation students, after controlling for differences in ACT scores.

\newpage

If we back-transform to odds,

```{r}
exp(coef(glm.2))
```

The interpretations are:

- For students with an ACT score of 0 who are not first generation college students, the predicted odds of graduating are $0.23$, on average.
- Each one-point difference in ACT score is associated with improving the odds of graduating 1.09 times, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, predicted odds of graduating are 1.67 times that of students who are not first generation students, after controlling for differences in ACT scores.

We can also plot the predicted probability of graduating to aid interpretation.


```{r fig.width=8, fig.height=6, out.width='4in'}
plotData = expand.grid(
  act = seq(from = 10, to = 36, by = 1),
  firstgen = c(0, 1)
)

plotData$pi_hat = predict(glm.2, newdata = plotData, type = "response")

plotData$firstgen = factor(plotData$firstgen, levels = c(0, 1), labels = c("Non First Generation Students", "First Geeration Students"))

ggplot(data = plotData, aes(x = act, y = pi_hat, color = firstgen)) +
  geom_line() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1) +
  scale_color_brewer(name = "", palette = "Set1")
```

# References
