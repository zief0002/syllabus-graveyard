---
title: "Information Criteria for Model Selection"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
bibliography: epsy8252.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
options(digits = 3)
library(printr)
```



We are going to use the log-likelihood for model selection. To do this, we fit a set of candidate models we are choosing between using ML estimation and compute the log-likelihood value for each model. Models with higher log-likelihood values will be favored; they are more likely given the data. 

Remember that the value of log-likelihood is the same for both the ML and OLS estimated models. Thus, it does not matter how we fit the model; we can use `lm()` to estimate the coefficients from a model since it is easier to use. 

## Preparation

In this set of notes, we will use the data in the *edschools-2010.csv* file. These data include institutional-level attributes for several graduate education schools/programs rated in 2010 by *U.S. News and World Report*. The attributes include:

- `school`: Institution name
- `rank`: U.S. News and World Report rank
- `peer_rate`: Peer assessment score (5.0 = highest).
- `gre`: Average GRE score for 2009 incoming students
- `sf_ratio`: 2009 student-to-faculty ratio
- `phd_fac_ratio`: 2009 doctoral student-to-faculty ratio
- `avg_res`: 2009 funded research per faculty member (in thousands of dollars)
- `doc_accept`: 2009 doctoral acceptance rate


```{r message=FALSE}
# Load libraries
library(AICcmodavg)
library(broom)
library(dplyr)
library(readr)

# Import the data
ed = read_csv(file = "~/Dropbox/epsy-8252/data/ed-schools-2010.csv")
head(ed)
```

## Scientific Hypotheses

We are interested in understanding how our peers in education rate other programs. Based on the substantive literature we have three hypotheses about how programs are rated:

- **H1:** The rating of a program is attributable to the quality of its students.
- **H2:** Peer ratings are attributable to the level of interaction faculty members have with students.
- **H3:** Peer ratings are attributable to the research prestige of the programs.

### Translating Hypotheses to Models

We need to translate these hypotheses into statistical models that we can then fit to a set of data. The models are only proxies for these hypotheses. However, that being said, the validity of using the models as proxies is dependent on whether we have measured well, whether the translation makes substantive sense given the literature base, etc. Here is how we are measuring the different attributes:

- We will use average GRE score as a measure of student-body quality.
- We will use student-to faculty ratio and doctoral student-to-faculty ratio as measures of faculty interaction.
- We will use doctoral student acceptance rate and average faculty research funding as measures of research prestige.

Once this has been identified, we can write out the models associated with the scientific hypotheses. These models using regression notation are:

- **M1:** $\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{GRE}_i) + \epsilon_i$
- **M2:** $\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{student\mbox{-}to\mbox{-}faculty~ratio}_i) + \beta_2(\mathrm{doctoral~student\mbox{-}to\mbox{-}faculty~ratio}_i) + \epsilon_i$
- **M3:** $\mathrm{Peer~Rating}_i = \beta_0 + \beta_1(\mathrm{doctoral~acceptance~rate}_i) + \beta_2(\mathrm{average~research~funding}_i) + \epsilon_i$

These are referred to as the *candidate models*. Now we fit these three candidate models in R.

```{r}
lm.1 = lm(peer_rate ~ 1 + gre, data = ed)
lm.2 = lm(peer_rate ~ 1 + sf_ratio + phd_fac_ratio, data = ed)
lm.3 = lm(peer_rate ~ 1 + doc_accept + avg_res, data = ed)
```

## Compute Log-Likelihood

Now we can compute the log-likelihood values for each model.

```{r}
logLik(lm.1)
logLik(lm.2)
logLik(lm.3)
```

The log-likelihood values are also available from the `glance()` function's output.

```{r}
glance(lm.1)
```

These values suggest that the best fitting model is Model 2; it has the highest log-likelihood value.

\newpage

## Deviance: An Alternative Fit Value

It is common to multiply the log-likelihood values by $-2$. This is called the *deviance*. This alleviates the deviance. Deviance is a measure of error, so when evaluating deviance values, lower is better. (The square brackets in the syntax grab the log-likelihood value from the `logLik()` output.)

```{r}
-2 * logLik(lm.1)[1] #Model 1
-2 * logLik(lm.2)[1] #Model 2
-2 * logLik(lm.3)[1] #Model 3
```

Again, the best fitting model is Model 2; it has the lowest deviance value. Whether you evaluate using the log-likelihood, or the deviance, you will end up with the same model. Using deviance, however, has the advantages of (1) having a direct relationship to model error, so it is more interpretable, and (2) not being negative.

## Compute AIC Values

Remember that lower values of deviance indicate the model (as defined via the set of parameters) is more likely given the data. However, in practice we cannot directly compare the deviances since the models include a different number of parameters. To account for this, we will add a penalty term to the deviance,

$$
AIC = \mathrm{Deviance} + 2(k)
$$

where $k$ is the number of parameters being estimated in the model (including the intercept and RMSE). Note that the value for $k$ is given as *df* in the `logLik()` output. For our three models, the *df* values are:

- **M1:** 3 *df* ($\hat\beta_0$, $\hat\beta_1$, RMSE)
- **M2:** 4 *df* ($\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, RMSE)
- **M3:** 4 *df* ($\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, RMSE)

This penalty-adjusted value is called Akiake's Information Criteria (AIC). These values can be compared directly, so long as:

- The exact same data is used to fit the models, and 
- The exact same outcome is used to fit the models. 

Smaller values of the AIC indicate a more likely model.

```{r}
-2 * logLik(lm.1)[1] + 2*3 #Model 1
-2 * logLik(lm.2)[1] + 2*4 #Model 2
-2 * logLik(lm.3)[1] + 2*4 #Model 3
```

Arranging these, we find that Model 2 (AIC = 49.9) is the most likely model given the data and the candidate set of models. This is the "best" model given the candidate set of models and the data. 


## Corrected AIC (AICc): Adjusting for Bias Based on Sample Size and Model Complexity

Although AIC has a penalty correction that should account for some bias, it turns out that when the number of parameters is large relative to the sample size, AIC is still biased in favor of models that have more parameters. This led @Hurvich:1989 to propose a second-order bias corrected AIC measure (AICc) computed as

$$
\mathrm{AIC_c} = \mathrm{Deviance} + 2(k)\left( \frac{n}{n - k - 1} \right)
$$

where $k$ is, again, the number of estimated parameters, and $n$ is the sample size used to fit the model. Note that when $n$ is very large (especially relative to $k$) that the last term is essentially 1 and the AICc value would basically reduce to the AIC value. When $n$ is small relative to $k$ this will add more of a penalty to the deviance. *The recommendation is to pretty much always use AICc rather than AIC when selecting models.* 

Below, we will compute the AICc for each of the three candidate models. (Note that we use $n=52$ cases for the computation for all the models in this data.)

```{r}
n = 52

# Compute AICc for Model 1, B, and C
-2 * logLik(lm.1)[[1]] + 2 * 3 * n / (n - 3 - 1) #Model 1
-2 * logLik(lm.2)[[1]] + 2 * 4 * n / (n - 4 - 1) #Model 2
-2 * logLik(lm.3)[[1]] + 2 * 4 * n / (n - 4 - 1) #Model 3
```

Based on the $\mathrm{AIC_c}$ values, the best model is again Model 2. It is the most likely model given the data and the six candidate models.

### Use AICc() Function

In practice, we will use the `AICc()` function from the **AICcmodavg** package to compute the AICc value directly.

```{r message=FALSE}
AICc(lm.1)
AICc(lm.2)
AICc(lm.3)
```

\newpage

In summary, here are the three candidate models, and their AICc values.

```{r echo=FALSE}
myAIC = aictab(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c("Model 1", "Model 2", "Model 3")
)

tab1 = myAIC[ , c("Modnames", "LL", "K", "AICc")] %>%
  mutate(Model = Modnames) %>%
  select(Model, LL, K, AICc)

kable(tab1)
```


Because the models are proxies for the scientific hypotheses, we can rank order the scientific hypotheses based on the empirical support for each. Of the three scientific hypotheses, H2 (Peer ratings are attributable to the level of interaction faculty members have with students.) has the most empirical support. The second most empirically supported hypothesis is H1. (The rating of a program is attributable to the quality of its students.) Finally, H3 has the least amount of empirical support of the three. (Peer ratings are attributable to the research prestige of the programs.)

# $\Delta$AICc

How much more empirical support does H2 have than H1 or H3? We can quantify this by computing the difference in AICc values between the best fitting model and all other models.

Since the minimum AICc value in our candidate models was associated with Model 2, we compute the difference between each model's AICc value and Model 2's AICc value. This is referred to as # $\Delta$AICc.

```{r}
# Compute dalta values
AICc(lm.2) - AICc(lm.1) #Model 1
AICc(lm.2) - AICc(lm.2) #Model 2
AICc(lm.2) - AICc(lm.3) #Model 3
```

```{r echo=FALSE}
tab2 = myAIC[ , c("Modnames", "LL", "K", "AICc", "Delta_AICc")] %>%
  mutate(Model = Modnames, "$\\Delta$AICc" = Delta_AICc) %>%
  select(Model, LL, K, AICc, "$\\Delta$AICc")

kable(tab2)
```

@Burnham:2011 [p. 25] give rough guidelines for interpreting ∆AICc values. They suggest that models with $\Delta$AICc values less than 2 are plausible, those in the range of 4--7 have some empirical support, those in the range of 9--11 have relatively little support, and those greater than 13 have essentially no empirical support. Using these criteria:

- Model 2 has a lot of empirical support
- Model 1 and Model 3 both have less empirical support than Model 2, but, nonetheless still both have a fair amount of empirical support

The empirical support is not unequivicably in favor of H2. There is empirical support for all three models. This might mean that we cannot ultimately reject H1 and H3. Or at least that we need to collect additional data to examine the hypotheses.


# Relative Likelihood and Evidence Ratios

Onw way we mathematically formalize the strength of evidence for each model is to compute the relative likelihood. To compute the relative likelihood,

$$
\mathrm{Relative~Likelihood} = e ^ {−\frac{1}{2} (\Delta AICc)}
$$

The relative likelihood provides the likelihood of each of the candidate models, given the set of candidate models and the data.


```{r}
exp(-1/2 * 2.96) #Model 1
exp(-1/2 * 0.00) #Model 2
exp(-1/2 * 6.95) #Model 3
```

```{r echo=FALSE}
tab3 = myAIC[ , c("Modnames", "LL", "K", "AICc", "Delta_AICc")] %>%
  mutate(Model = Modnames, "$\\Delta$AICc" = Delta_AICc) %>%
  select(Model, LL, K, AICc, "$\\Delta$AICc")
tab3$'Rel. Lik.' = exp(-1/2 * tab3$"$\\Delta$AICc")

kable(tab3)
```


These quantities allow evidentiary statements for comparing the scientific hypothese. These are referred to as *evidence ratios*. To compute an evidence ratio, we divide the relative likelihood for any two hypotheses. This will quantify how much more likely one hypothesis is than another given the data. For example, *given the data*,

- The empirical support for Hypothesis H2 is 4.4 (1/.228) times that of the empirical support for Hypothesis H1. 
- The empirical support for Hypothesis H2 is 32 (1/.031) times that of the empirical support for Hypothesis H1.

In general, software that computes evidence ratios do so for each model relative to the candidate model with the highest relative likelihood. The resulting evidence ratios allow for a comparison of each hypothesis to the most empirically supported hypothesis. Of course, given the relative likelihood for any two hypotheses, you can always compute the evidence ratio between the associated hypotheses.

- The empirical support for Hypothesis H1 is 7.4 (.228/.031) times that of the empirical support for Hypothesis H3.

```{r echo = FALSE}
tab4 = myAIC[ , c("Modnames", "LL", "K", "AICc", "Delta_AICc")] %>%
  mutate(Model = Modnames, "$\\Delta$AICc" = Delta_AICc) %>%
  select(Model, LL, K, AICc, "$\\Delta$AICc")
tab4$'Rel. Lik.' = exp(-1/2 * tab3$"$\\Delta$AICc")
tab4$ER = 1 / tab4$'Rel. Lik.'

kable(tab4)
```

\newpage

# Model Probability

Also referred to as Akaike Weights ($w_i$), model probabilities provide a numerical measure of the probability of each model given the data and the candidate set. It can be computed as

$$
w_i = \frac{\mathrm{Relative~Likelihood~for~Model~J}}{\sum_j \mathrm{Relative~Likelihood}}
$$

```{r}
sum_rel = 1 + .228 + .031 

.228 / sum_rel #Model 1
1 / sum_rel #Model 2
.031 / sum_rel #Model 3
```

These values can be interpreted as the probability of the hypothesis given the data and the candidate set of models. For example, given the data and the candidate set of models:

- The probability of Hypothesis H1 is 0.181.
- The probability of Hypothesis H2 is 0.794.
- The probability of Hypothesis H3 is 0.025.


```{r echo = FALSE}
tab5 = myAIC[ , c("Modnames", "LL", "K", "AICc", "Delta_AICc")] %>%
  mutate(Model = Modnames, "$\\Delta$AICc" = Delta_AICc) %>%
  select(Model, LL, K, AICc, "$\\Delta$AICc")
tab5$'Rel. Lik.' = exp(-1/2 * tab3$"$\\Delta$AICc")
tab5$ER = 1 / tab4$'Rel. Lik.'

sum_rel = sum(tab5$'Rel. Lik.')
tab5$'$w_i$' = tab5$'Rel. Lik.' / sum_rel

kable(tab5)


```

 
## Using the `aictab()` Function

We will use the `aictab()` function from the **AICcmodavg** package to compute many of the model evidence values directly from the `lm()` fitted models. This function takes a list of models in the candidate set (it actually has to be an R list). The optional argument `modnames=` is a vector of model names associated with the models in the candidate set.

```{r}
myAIC = aictab(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c("Model 1", "Model 2", "Model 3")
  )

# View table
myAIC
```

Note the output includes the number of parameters (`K`) and AICc value (`AICc`) for each candidate model, and prints them in order from most likely to least likely based on the AICc. It also includes the $\Delta$AICc values and the model probabilities (`AICcWt`) and log-likelihood (`LL`) values. The `Cum.Wt` column gives the cumulative model probabilies. (For example the probability of H2 or H1 is 0.98.)

We have to compute the evidence ratios separately. We do this using the `evidence()` function. This function takes the output from the `aictab()` function as well as the names from that table (given in the `modnames=` argument) for the two models you want to compute the evidence ratio for.

```{r}
# Evidence Ratios
evidence(myAIC, model.high = "Model 2", model.low = "Model 1")
evidence(myAIC, model.high = "Model 2", model.low = "Model 2")
evidence(myAIC, model.high = "Model 1", model.low = "Model 3")
```

### Pretty-Printing Model Evidence Tables in Markdown

We can use the `data.frame()` function to coerce the output from the `aictab()` function into a data frame. Then we can use **dplyr** functions to re-order, re-name and add columns to the evidence table. Lastly, we can use `kable()` to format the table for pretty-printing in Markdown.

```{r}
x = data.frame(myAIC) %>%
  select(
    Model = Modnames, 
    LL, K, AICc, Delta_AICc,
    w_i = AICcWt
  ) %>%
  mutate(
    ER = max(w_i) / w_i
  )

# Here we employ indexing to change the fifth column name
# We use LaTeX math notation in the names to use the Greek letter Delta
names(x)[5] = '$\\Delta$AICc'

# Here we employ indexing to change the sixth column name
# We use LaTeX math notation to write a subscript
names(x)[6] = '$w_i$'

kable(x, caption = "Table of Model Evidence for Three Candidate Models. (LL = Log-Likelihood; K = Model df; $w_i$ = Model Probability; ER = Evidence Ratio)")
```


## Some Final Thoughts

Based on the different quantifications:

- Hypothesis H2 has the most empirical support.
- There is some empirical support for Hypothesis H1 relative to the other two hypotheses.
- There is very little empirical support for H3 relative to the other two hypotheses.

This might mean that in practice we focus on the the first and second hypotheses, reporting and discussing results from both M1 and M2. We can get a summary of the model rankings along with qualitative descriptors of the empirical support (weight) using the `confset()` function. The `method="ordinal"` argument rank orders the models for us.

```{r}
confset(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c("Model 1", "Model 2", "Model 3"), 
  method = "ordinal"
  )
```

\newpage

It is important to note that it is ultimately the set of scientific hypotheses that we are evaluating, using the fit from the associated statistical models to a set of data. If we use a different set of data, we may have a whole new ranking of models, and thus the empirical support is linked to the data.

It is also important to note that we are not evaluating individual predictors in the models, just the model as a whole. Because of this, it is not appropriate to remove preidctors after adopting a model(s). 

Lastly, the use of $p$-values is not compatible with the use of model-level selection methods such as information criteria. See @Anderson:2008 for more detail. Because of this, it is typical to not even report $p$-values when carrying out this type of analysis.


# References
