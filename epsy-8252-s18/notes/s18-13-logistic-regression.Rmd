---
title: "Logistic Regression"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Bembo Std"
sansfont: "Helvetica Neue UltraLight"
monofont: Inconsolata
urlcolor: "umn2"
bibliography: epsy8252.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3, scipen = 99)
library(printr)
```

# Preparation

We will agian use the data from the *graduation.csv* file in these notes. The source of these data is: @Jones-White:2014. We will use these data to explore predictors of college graduation.

```{r message=FALSE}
# Load librarires
library(AICcmodavg)
library(broom)
library(dplyr)
library(ggplot2)
library(readr)
library(sm)

# Read in graduation data
grad = read_csv(file = "~/Dropbox/epsy-8252/data/graduation.csv")
```


In the last set of notes, we saw that using the linear probability model leads to direct violations of the linear model's assumptions. If that isn't problematic enough, it is possible to run into severe issues when we make predictions. For example, given the constant effect of $X$ in these models it is possible to have an $X$ value that results in a predicted proportion that is either greater than 1 or less than 0. This is a problem since proportions are constrained to the range of $\left[0,~1\right]$.

Before we consider any alternative models, let's actually examine the empirical proportions of students who graduate at different ACT scores.

```{r out.width='3in', message=FALSE, fig.cap='The loess smoother suggests that the proportion of students who graduate is a non-linear function of ACT scores.'}
graduates = grad %>% 
  group_by(act, degree) %>% 
  summarize( N = n() ) %>% 
  mutate( Prop = N / sum (N) ) %>%
  filter(degree == 1) %>%
  ungroup() #Makes the resulting tibble regular

# View data
head(graduates, 10)

# Plot proportions
ggplot(data = graduates, aes(x = act, y = Prop)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_bw() +
  xlab("ACT score") +
  ylab("Proportion of graduates")
```

\newpage

# Alternative Models to the Linear Probability Model

Many of the non-linear models that are typically used to model dichotomous outcome data are "S"-shaped models. Below is a plot of one-such "S"-shaped model.

```{r echo=FALSE, out.width='3in'}
x = seq(from = -4, to = 4, by = .01)
y = 1 / (1 + exp(-(0 + 4*x)))

plot(x, y, type = "l", xlab = "X", ylab = "P(X = 1)")
```

The non-linear "S"-shaped model has many attractive features. First, the predicted $Y$ values are bounded between 0 and 1. Furthermore, as $X$ gets smaller, the proportion of $Y=1$, approaches 0 at a slower rate. Similarly, as $X$ gets larger, the proportion of $Y=1$, approaches 1 at a slower rate. Lastly, this model curve is monotonic; smaller values of $X$ are associated with smaller proportions of $Y=1$ (or if the "S" were backwards, larger values of $X$ would be associated with smaller proportions of $Y=1$). The key is that there are no bends in the curve; it is always growing or always decreasing. 

In our graduation example, the empirical data maps well to this curve. Higher ACT scores are associated with a higher proportion of students who graduate (monotonic). The effect of ACT, however, is not constant, and seems to diminish at higher ACT scores. Lastly, we want to bound the proportion at every ACT score to lie between 0 and 1.

How do we fit such an "S"-shaped curve? We apply a transformation function, call it $\Phi$, to the predicted values. Mathematically,

$$
\Phi(\pi_i) = \Phi\bigg[\beta_0 + \beta_1(X)\bigg]
$$

The specific transformation function used is any mathematical function that can fit the criteria we had before (monotonic, nonlinear, maps to $[0,~1]$ space). There are several mathematical functions that do this. One common function that meets these specifications is the *logistic function*. Mathematically, the logistic function is

$$
\Phi(w) = \frac{1}{1 + e^{-w}}
$$

where $w$ is the value fed into the logistic function. For example, to logistically transform $w=3$, we use

$$
\begin{split}
\Phi(3) &= \frac{1}{1 + e^{-3}} \\
&= 0.953
\end{split}
$$

\newpage

Below we show how to transform many such values using R.

```{r fig.width=6, fig.height=6, out.width='3in', fig.cap='Plot of the transformed phi(w) values for a sequence of w values.', fig.pos='H'}
example = tibble(
  w = seq(from = -4, to = 4, by = 0.01)  # Set up values
  ) %>%
  mutate(
    phi = 1 / (1 + exp(-w))  # Transform using logistic function
  )

# View data
head(example)

# Plot the results
ggplot(data = example, aes(x = w, y = phi)) +
  geom_line() +
  theme_bw()
```

You can see that by using this transformation we get a monotonic "S"-shaped curve. Now try substituting a really large value of $w$ into the function. This gives an asymptote at 1. Also substitute a really "large"" negative value in for $w$. This gives an asymptote at 0. So this function also bounds the output between 0 and 1.

\newpage

How does this work in a regression? There, we are transforming the *predicted values*, the $\pi_i$ values, which we express as a function of the predictors:

$$
\begin{split}
\Phi(\hat\pi_i) &= \Phi\bigg[\beta_0 + \beta_1(X_i)\bigg] \\
& = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
\end{split}
$$

Since we took a linear model ($\beta_0 + \beta_1(X_i)$) and applied a logistic transformation, the resulting model is the *linear logistic model* or more simply, the *logistic model*. 

## Re-Expressing a Logistic Transformation

The logistic model expresses the proportion of 1s $\pi_i$ as a function of the predictor $X$. It can be mathematically expressed as

$$
\pi_i = \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}}
$$

We can re-express this using algebra and rules of logarithms.

$$
\begin{split}
\pi_i &= \frac{1}{1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]}} \\
\pi_i \times (1 + e^{-\big[\beta_0 + \beta_1(X_i)\big]} ) &= 1 \\
\pi_i + \pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 \\
\pi_i(e^{-\big[\beta_0 + \beta_1(X_i)\big]}) &= 1 - \pi_i \\
e^{-\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{1 - \pi_i}{\pi_i} \\
e^{\big[\beta_0 + \beta_1(X_i)\big]} &= \frac{\pi_i}{1 - \pi_i} \\
\ln \bigg(e^{\big[\beta_0 + \beta_1(X_i)\big]}\bigg) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i} \bigg) \\
\beta_0 + \beta_1(X_i) &= \ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg)
\end{split}
$$

Or,

$$
\ln \bigg( \frac{\pi_i}{1 - \pi_i}\bigg) = \beta_0 + \beta_1(X_i)
$$

The logistic model expresses the log of $\frac{\pi_i}{1 - \pi_i}$ as a linear function of $X$. 

### Log-Odds or Logits

The ratio that we are taking the logarithm of, $\frac{\pi_i}{1 - \pi_i}$, is referred to as *odds*. Odds are the ratio of two probabilities. Namely the chance an event occurs ($\pi_i$) versus the chance that same event does not occur ($1 - \pi_i$). As such, it gives the *relative chance* of that event occurring. To understand this better, we will look at a couple examples.

Let's assume that the probability of getting an "A" in a course is 0.7. Then we know the probability of NOT getting an "A" in that course is 0.3. The odds of getting an "A" are then

$$
\mathrm{Odds} = \frac{0.7}{0.3} = 2.33
$$

That the probability of getting an "A" in the class is 2.33 times as likely as NOT getting an "A" in the class. This is the relative probability of getting an "A". 

\newpage

As another example, Fivethirtyeight.com computed the [probability that a Canadian hockey team would win the Stanley Cup as 0.17](https://fivethirtyeight.com/features/will-canada-end-its-stanley-cup-drought-well-its-not-impossible/). The odds of a Canadian team winning the Stanley Cup is then

$$
\mathrm{Odds} = \frac{0.17}{0.83} = 0.21
$$

The probability that a Canadian team wins the Stanley Cup is 0.21 times as likely as a Canadian team NOT winning the Stanley Cup. (Odds less than 1 indicate that is is more likely for an event NOT to occur than to occur. Invert the fraction to compute how much more like the event is not to occur.)

In the logistic model, we are predicting the log-odds (also referred to as the *logit*. When we get these values, we typically transform the logits to odds by inverting the log-transformation (take $e$ to that power.) 

# Fitting the Logistic Model in R

To fit a logistic model using R, we use the `glm()` function. GLM stands for Generalized Linear Model. It is an extension of the Linear Model (that we have been working with for two semesters) that allows for a transformation of the predicted values to allow for modeling residuals that follow other probability distributions. The transformation used is different depending on which probability distribution needs to be modeled. For example,

\begin{table}[H]
\centering
\label{my-label}
\begin{tabular}{p{6cm}p{2cm}p{3cm}}
\hline
\bf{Typical Use}  & \bf{Distribution} & \bf{Transformation} \\
\hline
Modeling counts/proportions of 1s in $k=2$ categories & Binomial & Logistic function \\
\hline
Modeling average Y in continuous data & Normal & Identity function \\
\hline
Modeling count/proportion of occurrences in fixed amount of time/space & Poisson & Log function \\
\hline
Modeling counts/proportions of 1s in $k>2$ categories & Multinomial & Logistic function \newline (multiple equations)\\
\hline
\end{tabular}
\end{table}

The transformation function is often referred to as the *link function*; it is the function that links the probability distribution to the linear model. The syntax to fit the logistic model using `glm()` is

$$
\mathtt{glm(} \mathrm{y} \sim \mathrm{1~+~x,~}\mathtt{data=}~\mathrm{dataframe,~}\mathtt{family~=~binomial(link~=~"logit")}
$$

The formula depicting the model and the `data=` arguments are specified in the same manner as in the `lm()` function. Since the model is a generalized model, we need to specify the residual distribution (binomial) and the link function (logit) via the `family=` argument. For our example,

```{r}
glm.1 = glm(degree ~ 1 + act, data = grad, family = binomial(link = "logit"))
```

\newpage

The output of the model can be printed using `summary()`.

```{r}
summary(glm.1)
```

The fitted equation for the model is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.61 + 0.11(\mathrm{ACT~Score}_i)
$$

We interpret the coefficients in the same manner as we interpret coefficients from a linear model, with the caveat that the outcome is now in log-odds (or logits):

- For students with an ACT score of 0, their predicted log-odd of graduating are $-1.61$.
- Each one-point difference in ACT score is associated with a difference of 0.11in the predicted log-odds of graduating, on average.

For better interpretations, we can back-transform log-odds to odds. This is typically a better metric for interpretation of the coefficients. To back-transform to odds, we exponentiate both sides of the fitted equation and use the rules of exponents to simplify:

$$
\begin{split}
e^{\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg)} &= e^{-1.61 + 0.11(\mathrm{ACT~Score}_i)} \\
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(\mathrm{ACT~Score}_i)}
\end{split}
$$

When ACT score = 0, the predicted odds are


$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(0)} \\
&= e^{-1.61} \times 1 \\
&= e^{-1.61} \\
&= 0.2
\end{split}
$$

For students with an ACT score of 0, their odds of graduating is 0.2. That is, for these students, the probability of graduating is 0.2 times that of not graduating. (It is far more likey these students will not graduate.)

To interpret the effect of ACT on the odds of graduating, we will compare the odds of graduating for students that have ACT score that differ by one point. Say ACT = 0 and ACT = 1.

We already know the predicted odds for students with ACT = 0, namely $e^{-1.61}$. For students with an ACT of 1, their predicted odds of graduating are

$$
\begin{split}
\frac{\hat\pi_i}{1 - \hat\pi_i} &= e^{-1.61} \times e^{0.11(1)} \\
&= e^{-1.61} \times e^{0.11} \\
\end{split}
$$

These students odds of graduating are $e^{0.11}$ times greater than students with an ACT score of 0. Moreover, this increase in the odds, on average, is the case for every one-point difference in ACT score. In general,

- The predicted odds for $X=0$ are $e^{\hat\beta_0}$.
- Each one-unit difference in $X$ is associated with a $e^{\hat\beta_1}$ times increase (decrease) in the odds.

We can obtain these values in R by using the `coef()` function to obtain the fitted model's coefficients and then exponentiating them using the `exp()` function. 

```{r}
exp(coef(glm.1))
```

From these values, we interpret the coefficients in the odds metric as

- The predicted odds of graduating for students with an ACT score of 0 are 0.20.
- Each one-unit difference in ACT score is associated with 1.11 times greater odds of graduating.

## Plotting the Results from the Fitted Model

To even further understand and interpret the fitted model, we can plot the fitted values for a range of ACT scores. Typically the proportion of students graduating ($\hat\pi_i$) is what should be plotted, as that is what we were initially modeling. This process is exactly the same as the process of plotting fitted model results for any of the other models we have worked with. The only difference is that when we use the `predict()` function to get the fitted values, we have to specify that we want the $\hat\pi_i$ values (the default is logits). We do this by including the argument `type="response"`. Below we plot the results from our fitted logistic model.

```{r out.width='3in', fig.cap='Predicted probability of graduating college as a function of ACT score.'}
# Create the data to plot
plotData = expand.grid(
  act = seq(from = 10, to = 36, by = 1)
  ) %>%
  mutate(
    pi_hat = predict(glm.1, newdata = ., type = "response")
  )

# Plot the data
ggplot(data = plotData, aes(x = act, y = pi_hat)) +
  geom_line() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1)
```

The monotonic increase in the curve indicates the positive effect of ACT score on the probability of graduating. (Note that we typically interpret $\pi_i$ as probability rather than proportion when interpreting logistic models.) The magnitude of this effect depends on ACT score. For lower ACT scores there is a larger effect of ACT score on the probability of graduating than for higher ACT scores.

# Model-Level Summaries

The `summary()` output for the GLM model also included model-level information. For the model we fitted, the model-level information was

```
    Null deviance: 2722.5  on 2343  degrees of freedom
Residual deviance: 2633.2  on 2342  degrees of freedom
AIC: 2637

Number of Fisher Scoring iterations: 4
```

The metric of measuring residual fit is the deviance (remember the deviance was $-2 \times$ log-likelihood). The *null deviance* is the residual deviance from fitting the intercept-only model.

```{r}
glm.0 = glm(degree ~ 1, data = grad, family = binomial(link = "logit"))

# Compute deviance
-2 * logLik(glm.0)[[1]]
```


The *residual deviance* is the residual deviance from fitting whichever model was fitted, in our case the model that used ACT score as a predictor.

```{r}
-2 * logLik(glm.1)[[1]]
```

Recall that deviance is akin to the sum of squared residuals (SSE), smaller values indicate less error. In our case, the model that includes ACT score as a predictor has less error than the intercept only model; its deviance is 90 less than the intercept-only model. 

There are two ways to determine whether this decrease in deviance is statistically significant. The first is to examine the significance of the ACT predictor in the fitted model. Since that is the only predictor included above-and-beyond the intercept, the $p$-value associated with it indicates whether the ACT predictor is statistically relevant.

The second method to test the improvement in deviance is a test of nested models. Since the intercept-only model is nested in the model that includes ACT as a predictor, we can use a *Likelihood Ratio Test* to examine this. To do so, we use the `anova()` function with the added argument `test="LRT"`.  

```{r}
anova(glm.0, glm.1, test = "LRT")
```

The null hypothesis of this test is that there is NO improvement in the deviance. The results of this test, $\chi^2(1)=89.3$, $p<.001$, indicate that we should reject the null hypothesis. The more complex model has significantly less error than the intercept-only model and should be adopted.

\newpage

We could have also used information criteria for making decisions about effects.

```{r}
myAIC = aictab(
  cand.set = list(glm.0, glm.1),
  modnames = c("Intercept-only", "ACT score")
)

myAIC
```

Here, given the data and the candidate set of models, there is overwhelming evidence to support the model that includes ACT score.



# Including Covariates

Including covariates in the logistic model is done the same way as for `lm()` models. For example, say we wanted to examine the effect of ACT score on probability of graduating, after controlling for whether or not a student was first generation college student. We fit that model as

```{r}
glm.2 = glm(degree ~ 1 + act + firstgen, data = grad, family = binomial(link = "logit"))
summary(glm.2)
```

The fitted equation is

$$
\ln \bigg( \frac{\hat\pi_i}{1 - \hat\pi_i}\bigg) = -1.48 + 0.09(\mathrm{ACT~Score}_i) + 0.52(\mathrm{First~Generation}_i)
$$

Using the logit/log-odds metric, we interpret the coefficients as:

- For students who are not first generation college students with an ACT score of 0, the predicted log-odds of graduating are $-1.48$, on average.
- Each one-point difference in ACT score is associated with a difference of 0.09 in the predicted log-odds of graduating, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, have a predicted log-odds of graduating that is 0.52 higher than students who are not first generation students, after controlling for differences in ACT scores.


If we back-transform to odds,

```{r}
exp(coef(glm.2))
```

The interpretations are:

- For students with an ACT score of 0 who are not first generation college students, the predicted odds of graduating are $0.23$, on average.
- Each one-point difference in ACT score is associated with improving the odds of graduating 1.09 times, on average, after controlling for whether o not the students are first generation college students.
- First generation college students, on average, predicted odds of graduating are 1.67 times that of students who are not first generation students, after controlling for differences in ACT scores.

We can also plot the predicted probability of graduating to aid interpretation.


```{r fig.width=8, fig.height=6, fig.width=8, out.width='4.5in', fig.cap='Predicted probability of graduating college as a function of ACT score for first and non-firt generation students.'}
# Creat data to plot
plotData = expand.grid(
  act = seq(from = 10, to = 36, by = 1),
  firstgen = c(0, 1)
  ) %>%
  mutate(
    pi_hat = predict(glm.2, newdata = ., type = "response"),
    firstgen = factor(firstgen, 
                      levels = c(0, 1), 
                      labels = c("Non First Generation Students", "First Generation Students")
                      )
  )


# PLot the data
ggplot(data = plotData, aes(x = act, y = pi_hat, color = firstgen)) +
  geom_line() +
  theme_bw() +
  xlab("ACT score") +
  ylab("Predicted probability of graduating") +
  ylim(0, 1) +
  scale_color_brewer(name = "", palette = "Set1")
```

# References
