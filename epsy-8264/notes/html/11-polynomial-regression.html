<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="EPsy 8264" />


<title>Polynomial Regression</title>

<link href="11-polynomial-regression_files/tufte-css/tufte.css" rel="stylesheet" />
<link href="11-polynomial-regression_files/tufte-css/envisioned.css" rel="stylesheet" />
<link href="11-polynomial-regression_files/highlightjs/default.css" rel="stylesheet" />
<script src="11-polynomial-regression_files/highlightjs/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<link rel="stylesheet" href="my_style.css" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Polynomial Regression</h1>
<h4 class="author"><em>EPsy 8264</em></h4>
<h4 class="date"><em>October 2018</em></h4>



<p>There may be relationships that you want to model that are not linear. Polynomial regression is one methodology that we can use to model nonlinearity. To fit a polynomial regression we include additional predictors to the model that are obtained by raising each of the original predictors to a power. For example, to fit a cubic regression model we include the three predictors: <span class="math inline">\(X\)</span>, <span class="math inline">\(X^2\)</span>, and <span class="math inline">\(X^3\)</span>.</p>
<p>Adding polynomial terms to the model, like any other predictor correlated with the outcome will improve the fit of the model to the data. Figure 1 shows the results of fitting three different polynomial regression models to the <em>polynomial-example.csv</em> data and the resulting model-level <span class="math inline">\(R^2\)</span> values.</p>
<div class="figure fullwidth">
<img src="11-polynomial-regression_files/figure-html/unnamed-chunk-1-1.png" alt=" " width="1152"  />
<p class="caption marginnote shownote">
</p>
</div>
<p>By <em>saturating</em> the model with polynomial terms we can obtain perfect fit to the data. Unfortunately, this 8th-degree polynomial model is unlikely to generalize to new data sets. We have overfitted the model to the data. Also, interpolated predictions (e.g., at <span class="math inline">\(x = 50\)</span>) are suspect, at best. Even models with lower-order polynomials tend to be overfitted to the data. Fortunately, there are several methods we can use to avoid overfitting.</p>
<div id="bluegill-example" class="section level2">
<h2>Bluegill Example</h2>
<p>The data in <em>bluegills.csv</em> include measurements taken on 78 bluegills from <a href="https://www.dnr.state.mn.us/lakefind/search.html?name=Lake+Mary&amp;county=21">Lake Mary</a> in Minnesota. On each fish, a key scale was removed. The age of a fish is determined by <a href="https://fishbio.com/field-notes/inside-fishbio/reading-scales">counting the number of annular rings on the scale</a>. The goal of the analysis is to relate length of the fish at capture to the age of the fish. These data were collected by Richard Frie, and discussed in <span class="citation">(Weisberg, 1986, 2005)</span>.</p>
<pre class="r"><code># Load libraries
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(readr)

# Import data
bluegills = read_csv(&quot;~/Dropbox/epsy-8264/data/bluegills.csv&quot;)
head(bluegills)</code></pre>
<pre><code>## # A tibble: 6 x 2
##     age length
##   &lt;int&gt;  &lt;int&gt;
## 1     1     67
## 2     1     62
## 3     2    109
## 4     2     83
## 5     2     91
## 6     2     88</code></pre>
<p>The scatterplot of age versus length (left) suggests that there may be a nonlinear relationship between the age and length of a fish. The plot of the residuals versus the fitted values from the model where we regress length on age (right) also suggests that the relationship is nonlinear.</p>
<p><img src="11-polynomial-regression_files/figure-html/unnamed-chunk-3-1.png" width="768"  /></p>
<p>One method of dealing with this nonlinearity is to include higher degree polynomial effects of age. For example, we could include a quadratic (2nd-degree polynomial) term in addition to the linear term in the model. There are six distinct age (<span class="math inline">\(x\)</span>) values in the data, so a a 5th-degree polynomial model would saturate the data.</p>
<p><label for="tufte-mn-1" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;">If the data includes <span class="math inline"><span class="math inline">\(k\)</span></span> distinct <span class="math inline"><span class="math inline">\(x\)</span></span>-values, a <span class="math inline"><span class="math inline">\(k-1\)</span></span>-degree polynomial model will saturate the data.</span></span></p>
<p>To account for the nonlinearity, we will fit and evaluate a suite of potential models: (1) a first-degree polynomial (linear); (2) a second-degree polynomial (quadratic); (3) a third-degree polynomial (cubic); (4) a fourth-degree polynomial (quartic); and (5) a fifth-degree polynomial (quintic).</p>
<p>There are many methods for fitting polynomial models.</p>
<p><label for="tufte-mn-2" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;">All three methods produce the same coefficient-level and model-level output. There are benefits and costs to each of these methods, and each may be useful depending on the situation.</span></span></p>
<ul>
<li>Create the polynomial terms in the data and use them in the <code>lm()</code> function.</li>
<li>Create the polynomial terms directly in the <code>lm()</code> function without creating them in the data.</li>
<li>Use the <code>poly()</code> function to create the polynomial terms in the <code>lm()</code> function.</li>
</ul>
<p>To illustrate each of these methods, we will fit a third-degree polynomial.</p>
<div id="method-1-create-polynomial-terms-in-the-data" class="section level4">
<h4>Method 1: Create Polynomial Terms in the Data</h4>
<p>The first method for fitting a polynomial regression model is to create the polyomial terms in the data and then use these terms in the <code>lm()</code> function. To do this, we will first create the <span class="math inline">\(age^2\)</span> and <span class="math inline">\(age^3\)</span> terms in the data set.</p>
<pre class="r"><code># Create the quadratic and cubic terms
bluegills = bluegills %&gt;%
  mutate(
    age_quad = age ^ 2,
    age_cubic = age ^ 3
  )

head(bluegills)</code></pre>
<pre><code>## # A tibble: 6 x 4
##     age length age_quad age_cubic
##   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1     1     67        1         1
## 2     1     62        1         1
## 3     2    109        4         8
## 4     2     83        4         8
## 5     2     91        4         8
## 6     2     88        4         8</code></pre>
<p>Then we can include them in the models. For example, to fit the cubic (third-degree) polynomial model:</p>
<pre class="r"><code>lm.3 = lm(length ~ 1 + age + age_quad + age_cubic, data = bluegills)
tidy(lm.3)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)    9.81     21.8       0.451 0.654  
## 2 age           58.2      21.4       2.72  0.00811
## 3 age_quad      -6.04      6.54     -0.923 0.359  
## 4 age_cubic      0.128     0.628     0.203 0.839</code></pre>
<pre class="r"><code>glance(lm.3)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.801         0.793  11.0      99.4 6.79e-26     4  -295.  601.  613.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
</div>
<div id="method-2-create-polynomial-terms-in-the-lm-function" class="section level4">
<h4>Method 2: Create Polynomial Terms in the lm() Function</h4>
<p>The second method for fitting a polynomial regression model creates the polynomial terms directly in the <code>lm()</code> function. If you use this method you will not have to <code>mutate()</code> new terms into the data. To create the terms directly in the <code>lm()</code> we use the <code>I()</code> function:</p>
<pre class="r"><code>lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3), data = bluegills)
tidy(lm.3)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)    9.81     21.8       0.451 0.654  
## 2 age           58.2      21.4       2.72  0.00811
## 3 I(age^2)      -6.04      6.54     -0.923 0.359  
## 4 I(age^3)       0.128     0.628     0.203 0.839</code></pre>
<pre class="r"><code>glance(lm.3)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.801         0.793  11.0      99.4 6.79e-26     4  -295.  601.  613.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
</div>
<div id="method-3-use-poly-function-in-the-lm-function" class="section level4">
<h4>Method 3: Use poly() Function in the lm() Function</h4>
<p>The third method is to use the <code>poly()</code> function. This function will create a set of polynomial terms for a provided variable and degree. We also include the argument <code>raw=TRUE</code>. For example, the cubic polynomial can be fitted as:</p>
<p><label for="tufte-mn-3" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle"><span class="marginnote"><span style="display: block;">Using the <code>poly()</code> function is syntactically efficient, but there are functions that will not work with this method (e.g., <code>augment()</code> to create the regression diagnostics).</span></span></p>
<pre class="r"><code>lm.3 = lm(length ~ 1 + poly(age, 3, raw = TRUE), data = bluegills)
tidy(lm.3)</code></pre>
<pre><code>## # A tibble: 4 x 5
##   term                      estimate std.error statistic p.value
##   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)                  9.81     21.8       0.451 0.654  
## 2 poly(age, 3, raw = TRUE)1   58.2      21.4       2.72  0.00811
## 3 poly(age, 3, raw = TRUE)2   -6.04      6.54     -0.923 0.359  
## 4 poly(age, 3, raw = TRUE)3    0.128     0.628     0.203 0.839</code></pre>
<pre class="r"><code>glance(lm.3)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.801         0.793  11.0      99.4 6.79e-26     4  -295.  601.  613.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>The <code>poly()</code> function is useful in exploring relationships in ggplot. We can include polynomials in the <code>geom_smooth()</code> layer by including the argument <code>formula=</code>. For example to add a 3rd-degree polynomial regression smoother to the plot we include <code>formula=y~poly(x, 3, raw = TRUE)</code> in the <code>geom_smooth()</code> layer. The syntax for this would be:</p>
<pre class="r"><code>ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y~poly(x, 3, raw = TRUE), se = FALSE) +
  theme_bw() </code></pre>
<p>Below are all the polynomial models (up to the saturated model) that can be fitted to the bluegill data.</p>
<div class="figure fullwidth">
<img src="11-polynomial-regression_files/figure-html/unnamed-chunk-12-1.png" alt=" " width="1152"  />
<p class="caption marginnote shownote">
</p>
</div>
<p>The plots suggest that within the 2nd-, 3rd, 4th, and 5th-degree polynomials are all but indistinguishable from one another. This might suggest that the quadratic polynomial might be sufficient to account for the nonlinearity.</p>
</div>
<div id="matrix-algebra" class="section level4">
<h4>Matrix Algebra</h4>
<p>Including polynomial terms in the regression model is no different than including any other predictors in the model; the design matrix just reflects these polynomial effects. For example the design matrix for the cubic polynomial would be,</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
1 &amp; \mathrm{Age}_1 &amp; \mathrm{Age}_1^2 \\
1 &amp; \mathrm{Age}_2 &amp; \mathrm{Age}_2^2 \\
1 &amp; \mathrm{Age}_3 &amp; \mathrm{Age}_3^2 \\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; \mathrm{Age}_n &amp; \mathrm{Age}_n^2 \\
\end{bmatrix}
\]</span></p>
<p>All of the matrix algebra (to obtain coefficients, variance–covariance matrices, etc.) is all the same as it was for any other linear model.</p>
</div>
</div>
<div id="evaluating-the-polynomial-terms-statistical-inference" class="section level2">
<h2>Evaluating the Polynomial Terms: Statistical Inference</h2>
<p>We will begin our analysis by fitting five polynomial models; 1st-degree through 5th-degree polynomials.</p>
<pre class="r"><code>lm.1 = lm(length ~ 1 + age,                                             data = bluegills)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = bluegills)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = bluegills)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = bluegills)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = bluegills)</code></pre>
<p>One method of evaluating the polynomial models, since the models are nested, is to use an <span class="math inline">\(F\)</span>-test to test the change in <span class="math inline">\(R^2\)</span>. The hypothesis for this is that:</p>
<p><span class="math display">\[
H_0: R^2_{\mathrm{Simple~Model}} = R^2_{\mathrm{Complex~Model}}
\]</span></p>
<p>For example to compare the linear to the quadratic polynomial model,</p>
<p><span class="math display">\[
H_0: R^2_{\mathrm{Linear~Model}} = R^2_{\mathrm{Quadratic~Model}}
\]</span></p>
<p>We can evaluate this by submitting both models to the <code>anova()</code> function.</p>
<pre class="r"><code>anova(lm.1, lm.2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: length ~ 1 + age
## Model 2: length ~ 1 + age + I(age^2)
##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     76 11892.8                                  
## 2     75  8920.7  1    2972.1 24.988 3.675e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Here we would reject the null hypothesis; <span class="math inline">\(F(1, 8921) = 25\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This suggests it is likely that the quadratic age term explains additional variation in fish length above and beyond the linear term.</p>
<p>To evaluate the necessity of the cubic term, we compare the quadratic model to the cubic model, etc. We can include a sequence of nested models in the <code>anova()</code> function to carry out all of these tests simultaneously.</p>
<pre class="r"><code>anova(lm.1, lm.2, lm.3, lm.4, lm.5)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: length ~ 1 + age
## Model 2: length ~ 1 + age + I(age^2)
## Model 3: length ~ 1 + age + I(age^2) + I(age^3)
## Model 4: length ~ 1 + age + I(age^2) + I(age^3) + I(age^4)
## Model 5: length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5)
##   Res.Df     RSS Df Sum of Sq       F    Pr(&gt;F)    
## 1     76 11892.8                                   
## 2     75  8920.7  1   2972.14 24.2825 5.143e-06 ***
## 3     74  8915.7  1      4.99  0.0408    0.8406    
## 4     73  8883.8  1     31.86  0.2603    0.6115    
## 5     72  8812.7  1     71.17  0.5814    0.4482    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Based on these results, we would adopt the quadratic polynomial model. Here we fail to reject the null hypothesis that the cubic term explains additional variation over and above the quadratic model; <span class="math inline">\(F(1, 8916) = 0.04\)</span>, <span class="math inline">\(p = 0.84\)</span>. It is unlikely that the cubic age term explains additional variation in fish length above and beyond the linear and quadratic terms. This would lead us to adopt the quadratic model.</p>
</div>
<div id="examining-the-residuals-and-influence" class="section level2">
<h2>Examining the Residuals and Influence</h2>
<p>Like any other model we adopt, it is germaine that we examine the tenability of the assumptions. Evaluation of the residuals suggests that including the quadratic effect mitigated the nonlinearity seen earlier. The assumptions of normality and homoskedasticity also seem tenable. Cursory examination of leverage and influence diagnostics show four potential problematic observations. However, removing these four observations (analyses not shown) did not noticably improve the fit to the assumptions.</p>
<pre class="r"><code>residualPlot(lm.2)
qqPlot(lm.2, id = FALSE)
influencePlot(lm.2)</code></pre>
<div class="figure fullwidth">
<img src="11-polynomial-regression_files/figure-html/unnamed-chunk-17-1.png" alt=" " width="1152"  />
<p class="caption marginnote shownote">
</p>
</div>
<pre><code>##      StudRes        Hat      CookD
## 1  0.4349770 0.28010234 0.02480721
## 16 1.9402355 0.05945046 0.07649640
## 42 2.1444952 0.06446316 0.10079167
## 74 0.2243668 0.37060953 0.01000752</code></pre>
</div>
<div id="reporting-the-results" class="section level2">
<h2>Reporting the Results</h2>
<p>When we report the results of a polynomial regression model evaluated using statistical inference, we typically report the model-level (<span class="math inline">\(R^2\)</span>, <span class="math inline">\(F\)</span>, <span class="math inline">\(p\)</span>), and coefficent-level (<span class="math inline">\(B\)</span>, <span class="math inline">\(SE\)</span>, <span class="math inline">\(p\)</span>) summaries for the adopted model. Interpretations of the model are also provided. However, since a polynomial model is essentially an interaction model (interaction of a predictor with itself) the constituent main-effects are not typically interpreted. Providing a plot of the fitted model is generally the easiest way to help facilitate these interpretations.</p>
<p>At the model level:</p>
<pre class="r"><code>glance(lm.2)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.801         0.796  10.9      151. 4.96e-27     3  -296.  599.  608.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<p>The quadratic polynomial model using age to predict variation in fish length is statsitically significant; <span class="math inline">\(F(2, 75)=151\)</span>, <span class="math inline">\(p&lt;.001\)</span>. The model explains 79.6% of the variation in fish length.</p>
<p>At the coefficient-level:</p>
<pre class="r"><code>tidy(lm.2)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    13.6     11.0        1.24 2.20e- 1
## 2 age            54.0      6.49       8.33 2.81e-12
## 3 I(age^2)       -4.72     0.944     -5.00 3.67e- 6</code></pre>
<p>The fitted equation would be:</p>
<p><span class="math display">\[
\hat{\mathrm{Fish~Length}}_i = 13.62 + 54.05(\mathrm{Age}_i) - 4.72(\mathrm{Age}^2_i)
\]</span></p>
<p>Since this model is an interaction model, we would only interpret the coefficient associated with the highest order interaction term (quadratic term) and not the constituent lower-order effects (linear term). Here, the significant quadratic term implies that the effect of age on length depends on age. For a better interpretation, we can plot the fitted model and interpret the findings from the plot.</p>
<pre class="r"><code>data.frame(
  age = seq(from = 1, to = 6, by = 0.1) #Set up sequence of x-values
  ) %&gt;%
  mutate(
    yhat = predict(lm.2, newdata = .) #Get y-hat values based on model
  ) %&gt;%
  ggplot(aes(x = age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab(&quot;Age&quot;) +
    ylab(&quot;Predicted length&quot;)</code></pre>
<p><img src="11-polynomial-regression_files/figure-html/unnamed-chunk-20-1.png" width="672"  /></p>
<p>This shows the negative quadratic relationship (upside-down U-shape) between age and length. For younger fish, there is a large positive relationship between age and length. This relationship diminishes as fish age and may even be negative for older fish (although in our data that is extrapolation).</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Weisberg:1986">
<p>Weisberg, S. (1986). A linear model approach to the backcalculation of fish length. <em>Journal of the American Statistical Association</em>, <em>81</em>, 922–929.</p>
</div>
<div id="ref-Weisberg:2005">
<p>Weisberg, S. (2005). <em>Applied linear regression</em> (3rd ed.). New York: Wiley.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
