<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="EPsy 8264" />


<title>Cross-Validation</title>

<link href="12-cross-validation_files/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="12-cross-validation_files/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="12-cross-validation_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="12-cross-validation_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<link rel="stylesheet" href="my_style.css" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Cross-Validation</h1>
<h4 class="author"><em>EPsy 8264</em></h4>
<h4 class="date"><em>October 2018</em></h4>



<p>In the previous set of notes, we modeled a nonlinear relationship between bluegill age and length, using data on <em>bluegills.csv</em>. Based on evaluating a series of nested <span class="math inline">\(F\)</span>-tests, we adopted a quadratic model. While hypothesis testing and <span class="math inline">\(p\)</span>-values are an important tool, they can lead to several problems:</p>
<ul>
<li>Increased Type I error rates: We carried out two tests to adopt the quadratic model. We probably should have adjusted the <span class="math inline">\(p\)</span>-values obtained from the <span class="math inline">\(F\)</span>-tests to accommodate this.</li>
<li>Overfit/Lack of generalization: The adopted model still has the potential to not perform well on future data.</li>
</ul>
<pre class="r"><code># Load libraries
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(modelr)
library(purrr)
library(readr)

# Import data
bluegills = read_csv(&quot;~/Dropbox/epsy-8264/data/bluegills.csv&quot;)
head(bluegills)</code></pre>
<pre><code>## # A tibble: 6 x 2
##     age length
##   &lt;int&gt;  &lt;int&gt;
## 1     1     67
## 2     1     62
## 3     2    109
## 4     2     83
## 5     2     91
## 6     2     88</code></pre>
<div id="evaluating-the-polynomial-terms-cross-validation" class="section level2">
<h2>Evaluating the Polynomial Terms: Cross-Validation</h2>
<p>A better method for model selection, that does not use testing, is <em>cross-validation</em>. To perform cross-validation, you randomly split the data set into two parts: a <em>training</em> set, and a <em>validation</em> set. Any candidate models are then fitted on the training data and evaluated using the validation data. Because we use a different data set to evaluate the models, the resulting evaluation is not biased toward the data we used to fit the model (i.e., less overfit). Because of this, we get better indications about the generalizability of the models to new/future data.</p>
<p>In our polynomial regression example, the algorithm for performing the cross-validation are:</p>
<ol style="list-style-type: decimal">
<li>Randomly divide the bluegill data into two sets of observations; training and validation data sets.</li>
<li>Fit all five candidate models (1st–5th degree polynomials) to the training observations.</li>
<li>Use the estimated coefficients from those fits to estimate fitted values and residuals for the observations in the validation data.</li>
<li>Based on the residuals from the validation observations, compute measures of model performance (e.g., MSE).</li>
</ol>
<p>Next, we will explore and carry out each of the steps in this algorithm to adopt a model for the bluegill data.</p>
<div id="divide-the-data-into-a-training-and-validation-set" class="section level3">
<h3>Divide the Data into a Training and Validation Set</h3>
<p>There are many ways to do this, but I will use the <code>sample()</code> function to randomly sample 39 case numbers, from 1 to 78 (the number of rows in the bluegills data), to make up the training data. Then I use the <code>filter()</code> function to select those cases. The remaining cases (those 39 observations not sampled) are put into a validation set.</p>
<pre class="r"><code># Make the random sampling replicable
set.seed(42)

# Select the cases to be in the training set
training_cases = sample(1:nrow(bluegills), size = 39, replace = FALSE)

# Create training data
train = bluegills %&gt;%
  filter(row_number() %in% training_cases)

# Create validation data
validate = bluegills %&gt;%
  filter(!row_number() %in% training_cases)</code></pre>
</div>
<div id="fit-the-candidate-models-to-the-training-data" class="section level3">
<h3>Fit the Candidate Models to the Training Data</h3>
<p>We now fit the five candidate models to the observations in the training data. Remember that the highest-order polynomial model we could fit was a 5th-degree polynomial model, which saturated the data.</p>
<pre class="r"><code>lm.1 = lm(length ~ 1 + age,                                             data = train)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)</code></pre>
</div>
<div id="fit-the-models-obtained-to-the-validation-observations" class="section level3">
<h3>Fit the Models Obtained to the Validation Observations</h3>
<p>Now we can obtain the predicted lengths for the observations in the validation data based on the coefficients from the models fitted to the training data.</p>
<pre class="r"><code># Get the predicted values for the validation data
yhat_1 = predict(lm.1, newdata = validate)
yhat_2 = predict(lm.2, newdata = validate)
yhat_3 = predict(lm.3, newdata = validate)
yhat_4 = predict(lm.4, newdata = validate)
yhat_5 = predict(lm.5, newdata = validate)</code></pre>
<p>These vectors of fitted values can be used to compute the residuals for the validation observations, which in turn can be used to compute the <em>Cross-Validated Mean Square Error</em> (CV-MSE). The CV-MSE is computed as:</p>
<p><span class="math display">\[
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n e_i^2
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations in the validation set. We can compute the CV-MSE for each of the candidate models.</p>
<pre class="r"><code>sum((validate$length - yhat_1) ^ 2) / nrow(validate)</code></pre>
<pre><code>## [1] 166.332</code></pre>
<pre class="r"><code>sum((validate$length - yhat_2) ^ 2) / nrow(validate)</code></pre>
<pre><code>## [1] 134.7842</code></pre>
<pre class="r"><code>sum((validate$length - yhat_3) ^ 2) / nrow(validate)</code></pre>
<pre><code>## [1] 134.8915</code></pre>
<pre class="r"><code>sum((validate$length - yhat_4) ^ 2) / nrow(validate)</code></pre>
<pre><code>## [1] 143.4878</code></pre>
<pre class="r"><code>sum((validate$length - yhat_5) ^ 2) / nrow(validate)</code></pre>
<pre><code>## [1] 144.2924</code></pre>
<p>The candidate model having the smallest CV-MSE is the model that should be adopted. In this case, the values for the CV-MSE suggest adopting the quadratic or the cubic polynomial models.</p>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>Leave-One-Out Cross-Validation</h2>
<p>There are two major problems with the cross-validation we just performed.</p>
<ol style="list-style-type: decimal">
<li>The estimate of the CV-MSE is highly dependent on the observations chosen to be in the training and validation sets. This is illustrated in the figure below which shows the CV-MSE values for the three models for 10 different random splits of the data. Some of these splits support adopting the quadratic polynomial model, other splits support adopting the cubic polynomial model, and one supports the linear model!</li>
</ol>
<p><img src="12-cross-validation_files/figure-html/unnamed-chunk-6-1.png" width="768"  /></p>
<ol start="2" style="list-style-type: decimal">
<li>Only a subset of the observations (those in the training set) are used to initially fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the CV-MSE may tend to overestimate the error rate (model accuracy measure) for the model fit.</li>
</ol>
<p>Leave-one-out cross-validation (LOOCV) is one method that can be used to overcome these issues. The algorithm for performing LOOCV is:</p>
<ol style="list-style-type: decimal">
<li>Hold out the <span class="math inline">\(i\)</span>th observatin as your validation data (a single observation) and use the remaining <span class="math inline">\(n-1\)</span> observations as your training data.</li>
<li>Fit all candidate models to the training data.</li>
<li>Use the estimated coefficients from those fits to compute <span class="math inline">\(\mathrm{MSE}_i\)</span> using the validation data.</li>
<li>Repeat Steps 2–4 for each observation.</li>
</ol>
<p>We then have <span class="math inline">\(n\)</span> estimates of the MSE that can be averaged to get the overall CV-MSE.</p>
<p><span class="math display">\[
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \mathrm{MSE}_i
\]</span> <label for="tufte-mn-1" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;">Since the validation dataset is composed of a single observation (<span class="math inline"><span class="math inline">\(n=1\)</span></span>), <span class="math inline"><span class="math inline">\(\mathrm{MSE}_i\)</span></span> is simply the squared residual value for the validation observation.</span> <span style="display: block;"><span class="math display"><span class="math display">\[
\begin{split}
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n e_i^2 \\
= \frac{1}{1} \sum_{i=1}^n e_i^2 \\
= \sum_{i=1}^n e_i^2
\end{split}
\]</span></span></span></span></p>
<p>We can carry out LOOCV by using a <code>for()</code> loop to iterate through the algorithm for each of the <span class="math inline">\(n\)</span> observations. Here is some example syntax:</p>
<pre class="r"><code># Set up empty vector to store results
mse_1 = rep(NA, 78)
mse_2 = rep(NA, 78)
mse_3 = rep(NA, 78)
mse_4 = rep(NA, 78)
mse_5 = rep(NA, 78)

# Loop through the cross-validation
for(i in 1:78){
  train = bluegills %&gt;% filter(row_number() != i)
  validate = bluegills %&gt;% filter(row_number() == i)
  
  lm.1 = lm(length ~ 1 + age,                                             data = train)
  lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
  lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
  lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
  lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)
  
  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)
  yhat_5 = predict(lm.5, newdata = validate)
  
  mse_1[i] = (validate$length - yhat_1) ^ 2
  mse_2[i] = (validate$length - yhat_2) ^ 2
  mse_3[i] = (validate$length - yhat_3) ^ 2
  mse_4[i] = (validate$length - yhat_4) ^ 2
  mse_5[i] = (validate$length - yhat_5) ^ 2
}</code></pre>
<pre><code>## Warning in predict.lm(lm.5, newdata = validate): prediction from a rank-
## deficient fit may be misleading</code></pre>
<pre class="r"><code># Compute CV-MSE
mean(mse_1)</code></pre>
<pre><code>## [1] 164.9605</code></pre>
<pre class="r"><code>mean(mse_2)</code></pre>
<pre><code>## [1] 122.3664</code></pre>
<pre class="r"><code>mean(mse_3)</code></pre>
<pre><code>## [1] 124.1223</code></pre>
<pre class="r"><code>mean(mse_4)</code></pre>
<pre><code>## [1] 141.4185</code></pre>
<pre class="r"><code>mean(mse_5)</code></pre>
<pre><code>## [1] 144.6784</code></pre>
<p>The LOOCV results point toward the quadratic model (smallest average CV-MSE), although the CV-MSE for the cubic model is not that much larger. Since this method is less biased; it trains the models on a much larger set of observations, its results are more believable than the simple cross-validation. Another advantage of LOOCV is that it will always produce the same results as opposed to simple cross-validation) since there is no randomness in producing the training and validation datasets.</p>
<p>LOOCV can be computationally expensive; we have to fit the candidate models <span class="math inline">\(n\)</span> times. It turns out, however, that models fit with least squares linear or polynomial regression, can give us the LOOCV results using the following formula:</p>
<p><span class="math display">\[
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \bigg(\frac{e_i}{1-h_{ii}}\bigg)^2
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value from the original least squares fit, and <span class="math inline">\(h_{ii}\)</span> is the leverage value. This is similar to the model (biased) MSE, except the <span class="math inline">\(i\)</span>th residual is divided by <span class="math inline">\(1 − h_{ii}\)</span>.</p>
<p>LOOCV is a very general method, and can be used with any kind of modeling. For example we could use it with logistic regression, or mixed-effects analysis, or any of the methods you have encountered in your statistics courses to date. That being said, the formula does not hold for all these methods, so outside of linear and polynomial regression, the candidate models will actually need to be fitted <span class="math inline">\(n\)</span> times.</p>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>k-Fold Cross-Validation</h2>
<p>One alternative to LOOCV is <span class="math inline">\(k\)</span>-fold cross-validation. The algorithm for <span class="math inline">\(k\)</span>-fold cross-validation is:</p>
<ol style="list-style-type: decimal">
<li>Randomly divide the bluegill data into <span class="math inline">\(k\)</span> groups or folds.</li>
<li>Hold out the <span class="math inline">\(i\)</span>th fold as your validation data and use the remaining <span class="math inline">\(k-1\)</span> folds as your training data.</li>
<li>Fit all candidate models to the training data.</li>
<li>Use the estimated coefficients from those fits to compute <span class="math inline">\(\mathrm{MSE}_i\)</span> using the validation data.</li>
<li>Repeat Steps 2–4 for each fold.</li>
</ol>
<p>We then have <span class="math inline">\(k\)</span> estimates of the MSE that can be averaged to get the overall CV-MSE.</p>
<p><span class="math display">\[
\mathrm{CV\mbox{-}MSE} = \frac{1}{k} \sum_{i=1}^k \mathrm{MSE}_i
\]</span></p>
<p>From the ideas presented in the algorithm, it is clear that LOOCV is a special case of <span class="math inline">\(k\)</span>-fold cross-validation in which <span class="math inline">\(k\)</span> is set to equal <span class="math inline">\(n\)</span>. In practice we typically use <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span>.</p>
<p>We can carry out <span class="math inline">\(k\)</span>-fold cross-validation by using the <code>crossv_kfold()</code> function from the <strong>modelr</strong> package. This function takes the argument <code>k=</code> to indicate the number of folds. Here is some example syntax to carry out a 10-fold cross-validation:</p>
<pre class="r"><code># Divide data into 10 folds
set.seed(100)
my_cv = bluegills %&gt;%
  crossv_kfold(k = 10)</code></pre>
<p>Then we will use the <code>map()</code> and <code>map2_dbl()</code> functions from the <strong>purrr</strong> package to fit a model to the training (<code>train</code>) data and find the MSE on the validation (<code>test</code>) data created from the <code>crossv_kfold()</code> function. We will have to carry this out for each of the candidate models. <label for="tufte-mn-2" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;">For more detailed information about using the purrr functions, see Jenny Bryan’s fantastic <a href="https://jennybc.github.io/purrr-tutorial/index.html">purrr tutorial</a>.</span></span></p>
<pre class="r"><code># Linear model
cv_1 = my_cv %&gt;%
  mutate(
    model = map(train, ~lm(length ~ 1 + age, data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    polynomial = 1
    )

# Quadratic model
cv_2 = my_cv %&gt;%
  mutate(
    model = map(train, ~lm(length ~ 1 + age + I(age^2), data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    polynomial = 2
    )

# Cubic model
cv_3 = my_cv %&gt;%
  mutate(
    model = map(train, ~lm(length ~ 1 + age + I(age^2) + I(age^3), data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    polynomial = 3
    )

# Quartic model
cv_4 = my_cv %&gt;%
  mutate(
    model = map(train, ~lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4), data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    polynomial = 4
    )

# Quartic model
cv_5 = my_cv %&gt;%
  mutate(
    model = map(train, ~lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = .)),
    MSE = map2_dbl(model, test, modelr::mse),
    polynomial = 5
    )</code></pre>
<pre><code>## Warning in predict.lm(model, data): prediction from a rank-deficient fit
## may be misleading</code></pre>
<p>Once we have the results, we can stack these into single data frame and then use <code>group_by()</code> and <code>summarize()</code> to obtain the CV-MSE estimates.</p>
<pre class="r"><code>rbind(cv_1, cv_2, cv_3, cv_4, cv_5) %&gt;%
  group_by(polynomial) %&gt;%
  summarize(
    cv_mse = mean(MSE)
  )</code></pre>
<pre><code>## # A tibble: 5 x 2
##   polynomial cv_mse
##        &lt;dbl&gt;  &lt;dbl&gt;
## 1          1   168.
## 2          2   121.
## 3          3   122.
## 4          4   162.
## 5          5   165.</code></pre>
<p>The results of carrying out the 10-fold cross-validation suggest that we adopt the quadratic or cubic model.</p>
<p>Using <span class="math inline">\(k\)</span>-fold cross-validation is computationally less expensive so long as <span class="math inline">\(k&lt;n\)</span>. But this computational gain has a cost in that the results are again dependent on the <span class="math inline">\(k\)</span> random splits. This variation, however, is less than that in the single split simple cross-validation. We can also alleviate some of this by fitting the <span class="math inline">\(k\)</span>-fold cross-validation several times and averaging across the results to get the CV-MSE estimate. <label for="tufte-mn-3" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle"><span class="marginnote"><span style="display: block;">There are several R packages that will fit a <span class="math inline"><span class="math inline">\(k\)</span></span>-fold cross-validation (e.g., <strong>DAAG</strong>).</span></span></p>
</div>
<div id="model-selection" class="section level2">
<h2>Model Selection</h2>
<p>The different methods of selecting variables suggest we adopt the following models:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Method
</th>
<th style="text-align:left;">
Model
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Nested F-Tests
</td>
<td style="text-align:left;">
Quadratic
</td>
</tr>
<tr>
<td style="text-align:left;">
Simple CV
</td>
<td style="text-align:left;">
Quadratic/Cubic
</td>
</tr>
<tr>
<td style="text-align:left;">
LOOCV
</td>
<td style="text-align:left;">
Quadratic
</td>
</tr>
<tr>
<td style="text-align:left;">
10-Fold CV
</td>
<td style="text-align:left;">
Quadratic/Cubic
</td>
</tr>
</tbody>
</table>
<p><label for="tufte-mn-4" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle"><span class="marginnote"><span style="display: block;">While the different CV methods can suggest different models, this is usually less problematic when the sample size is larger; we only have <span class="math inline"><span class="math inline">\(n=78\)</span></span> in the bluegill data.</span></span></p>
<p>The LOOCV and <span class="math inline">\(k\)</span>-fold CV are better suited for considering how the model will perform on future data sets, so I would adopt the quadratic model at this point (although there is some evidence for the cubic model as well). To get the coefficient estimates, we fit whichever model we adopt to the FULL data set (with all the observations) as this will give us the “best” estimates.</p>
<pre class="r"><code>lm.2 = lm(length ~ 1 + age + I(age^2), data = bluegills)
glance(lm.2)</code></pre>
<pre><code>## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
## *     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.801         0.796  10.9      151. 4.96e-27     3  -296.  599.  608.
## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;</code></pre>
<pre class="r"><code>tidy(lm.2)</code></pre>
<pre><code>## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    13.6     11.0        1.24 2.20e- 1
## 2 age            54.0      6.49       8.33 2.81e-12
## 3 I(age^2)       -4.72     0.944     -5.00 3.67e- 6</code></pre>
</div>
<div id="reporting-results-from-a-cross-validation" class="section level2">
<h2>Reporting Results from a Cross-Validation</h2>
<p>When we report the results of a polynomial regression model evaluated using cross-validation, there are some subtle differences in what is reported. At the model-level, we report the <span class="math inline">\(R^2\)</span> from the adopted model fitted to the full-data. We also report the CV-MSE as a measure of the model error for future observations (generalized error).</p>
<p>At the coefficent-level we typically report the coefficient estimates and standard errors based on fitting he adopted model to the full data. However, we DO NOT REPORT NOR INTERPRET P-VALUES. The <span class="math inline">\(p\)</span>-values do not take into account that the model was selected using cross-validation. Because of this they are incredibly misleading. As with any model, interpretations should also be offered, again typically by way of providing a plot of the fitted model to help facilitate these interpretations.</p>
<p>The quadratic polynomial model using age to predict variation in fish length is statsitically significant; <span class="math inline">\(F(2, 75)=151\)</span>, <span class="math inline">\(p&lt;.001\)</span>. The model explains 79.6% of the variation in fish length. The fitted equation is:</p>
<p><span class="math display">\[
\hat{\mathrm{Fish~Length}}_i = 13.62 + 54.05(\mathrm{Age}_i) - 4.72(\mathrm{Age}^2_i)
\]</span></p>
<p>This model suggests a negative quadratic relationship (upside-down U-shape) between age and fish length. For younger fish, there is a large positive relationship between age and length. This relationship diminishes as fish age and may even be negative for older fish (although in our data that is extrapolation).</p>
</div>
<div id="information-criteria" class="section level2">
<h2>Information Criteria</h2>
<p>In EPsy 8252, we learned about using information criteria for model selection. In particular, we used AIC, BIC, and AICc for model selection. It turns out the AIC-based model selection and cross-validation are asymptotically equivalent <span class="citation">Stone (1977)</span>. For linear models, using BIC for model selection is symptotically equivalent to leave-<span class="math inline">\(v\)</span>-out cross-validation when <span class="math inline">\(v = n\bigg(1 - \frac{1}{\ln (n) - 1}\bigg)\)</span> <span class="citation">(Shao, 1997)</span>.</p>
<p>As a practical note, computer simulations have suggested that the results from using AICc for model selection will, on average, be quite similar to those from cross-validation techniques. As such, AICc can be a useful alternative to the computationally expensive cross-validation methods. However, using AICc does not provide a measure of the model performance (MSE) in new datasets like cross-validation does.</p>
<pre class="r"><code># Fit models to all data
lm.1 = lm(length ~ 1 + age,                                             data = bluegills)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = bluegills)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = bluegills)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = bluegills)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = bluegills)


# Load library
library(AICcmodavg)

# Get AICc for all models
aictab(
  cand.set = list(lm.1, lm.2, lm.3, lm.4, lm.5),
  modnames = c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Cubic&quot;, &quot;Quartic&quot;, &quot;Quintic&quot;)
)</code></pre>
<pre><code>## 
## Model selection based on AICc:
## 
##           K   AICc Delta_AICc AICcWt Cum.Wt      LL
## Quadratic 4 599.58       0.00   0.67   0.67 -295.51
## Cubic     5 601.82       2.24   0.22   0.89 -295.49
## Quartic   6 603.89       4.31   0.08   0.97 -295.35
## Quintic   7 605.68       6.10   0.03   1.00 -295.04
## Linear    3 619.78      20.21   0.00   1.00 -306.73</code></pre>
</div>
<div id="structural-collinearity" class="section level2">
<h2>Structural Collinearity</h2>
<p>Let’s imagine that we adopted the cubic polynomial model. This model includes two predictors: the linear and quadratic effects of age. The correlation matrix of these predictors suggests that the two predictors are highly correlated. (This is not surprising given that we created the quadratic and cubic terms from the linear term.)</p>
<p><label for="tufte-mn-5" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle"><span class="marginnote"><span style="display: block;">Note that we need to create the polynomial terms in the data to examine the correlation matrix.</span></span></p>
<pre class="r"><code>bluegills %&gt;%
  mutate(
    age_quad  = age ^ 2,
    age_cubic = age ^ 3
  ) %&gt;%
  select(length, age, age_quad, age_cubic) %&gt;%
  correlate()</code></pre>
<pre><code>## 
## Correlation method: &#39;pearson&#39;
## Missing treated using: &#39;pairwise.complete.obs&#39;</code></pre>
<pre><code>## # A tibble: 4 x 5
##   rowname   length    age age_quad age_cubic
##   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 length    NA      0.857    0.786     0.701
## 2 age        0.857 NA        0.978     0.928
## 3 age_quad   0.786  0.978   NA         0.984
## 4 age_cubic  0.701  0.928    0.984    NA</code></pre>
<p>If we look at the VIF indices associated with the coefficients from the quadratic model, we find that the sampling variances for the coefficients are grossly inflated. When we have collinearity that is a result of the structural form of the model (e.g., polynomials, interactions), we refer to it as <em>structural multicollinearity</em>.</p>
<pre class="r"><code>lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3), data = bluegills)
vif(lm.3)</code></pre>
<pre><code>##       age  I(age^2)  I(age^3) 
##  251.3875 1111.3596  342.3745</code></pre>
<pre class="r"><code># Effect on SEs
sqrt(vif(lm.3))</code></pre>
<pre><code>##      age I(age^2) I(age^3) 
## 15.85520 33.33706 18.50336</code></pre>
<p>To alleviate this, we can center the age predictor and use the centered age to create the polynomial terms.</p>
<pre class="r"><code>bluegills = bluegills %&gt;%
  mutate(
    c_age = as.numeric(scale(age, center = TRUE, scale = FALSE)),
    c_age2 = c_age ^ 2,
    c_age3 = c_age ^ 3
  )</code></pre>
<p>The new correlation matrix using the predictors based on centered age show much smaller correlations between the predictors.</p>
<pre class="r"><code>bluegills %&gt;%
  select(length, c_age, c_age2, c_age3) %&gt;%
  correlate()</code></pre>
<pre><code>## 
## Correlation method: &#39;pearson&#39;
## Missing treated using: &#39;pairwise.complete.obs&#39;</code></pre>
<pre><code>## # A tibble: 4 x 5
##   rowname  length   c_age  c_age2  c_age3
##   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 length   NA       0.857  -0.541   0.753
## 2 c_age     0.857  NA      -0.350   0.807
## 3 c_age2   -0.541  -0.350  NA      -0.482
## 4 c_age3    0.753   0.807  -0.482  NA</code></pre>
<p>The model based on the polynomials created from the centered age shows much better VIF indices.</p>
<pre class="r"><code>lm.3_c = lm(length ~ 1 + c_age + I(c_age^2) + I(c_age^3), data = bluegills)
vif(lm.3_c)</code></pre>
<pre><code>##      c_age I(c_age^2) I(c_age^3) 
##   2.884870   1.310728   3.298860</code></pre>
<pre class="r"><code># Effect on SEs
sqrt(vif(lm.3_c))</code></pre>
<pre><code>##      c_age I(c_age^2) I(c_age^3) 
##   1.698490   1.144870   1.816277</code></pre>
<p>While the coefficients for the polynomials based on the centered age are different (they should be), the fitted model produces the same plot (just shifted to the left; centered).</p>
<pre class="r"><code>coef(lm.3_c)</code></pre>
<pre><code>## (Intercept)       c_age  I(c_age^2)  I(c_age^3) 
## 147.6024757  19.4461177  -4.6437569   0.1278874</code></pre>
<pre class="r"><code>data.frame(
  c_age = seq(from = -2.63, to = 2.37, by = 0.01) #Set up sequence of x-values
  ) %&gt;%
  mutate(
    yhat = predict(lm.3_c, newdata = .) #Get y-hat values based on model
  ) %&gt;%
  ggplot(aes(x = c_age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab(&quot;Centered Age&quot;) +
    ylab(&quot;Predicted length&quot;)</code></pre>
<p><img src="12-cross-validation_files/figure-html/unnamed-chunk-26-1.png" width="768"  /></p>
<div id="orthogonal-polynomials" class="section level3">
<h3>Orthogonal Polynomials</h3>
<p>Another way to deal with the structural multicollinearity is to use <em>orthogonal polynomial contrasts</em>. In the design matrix for the polynomials based on the raw age values, each row is composed of <span class="math inline">\([1,x,x^2]\)</span>, where <span class="math inline">\(x\)</span> is age. Othogonal contrasts are a particular way of generating the design matrix so that the columns are linearly independent. To create these we use the <code>poly()</code> function with the argument <code>raw=FALSE</code> (or omit the <code>raw=</code> argument all together). Below are the first several rows of the design matrix for the: (1) polynomials based on raw age; (2) polynomials based on centered age; and (3) polynomials based on orthogonal contrasts.</p>
<pre class="r"><code># Polynomials based on raw age
lm.2 = lm(length ~ 1 + poly(age, 2, raw = TRUE), data = bluegills)
head(model.matrix(lm.2))</code></pre>
<pre><code>##   (Intercept) poly(age, 2, raw = TRUE)1 poly(age, 2, raw = TRUE)2
## 1           1                         1                         1
## 2           1                         1                         1
## 3           1                         2                         4
## 4           1                         2                         4
## 5           1                         2                         4
## 6           1                         2                         4</code></pre>
<pre class="r"><code># Polynomials based on centered age
lm.2_center = lm(length ~ 1 + poly(c_age, 2, raw = TRUE), data = bluegills)
head(model.matrix(lm.2_center))</code></pre>
<pre><code>##   (Intercept) poly(c_age, 2, raw = TRUE)1 poly(c_age, 2, raw = TRUE)2
## 1           1                   -2.628205                    6.907462
## 2           1                   -2.628205                    6.907462
## 3           1                   -1.628205                    2.651052
## 4           1                   -1.628205                    2.651052
## 5           1                   -1.628205                    2.651052
## 6           1                   -1.628205                    2.651052</code></pre>
<pre class="r"><code># Polynomials based on orthogonal contrasts
lm.2_ortho = lm(length ~ 1 + poly(age, 2), data = bluegills)
head(model.matrix(lm.2_ortho))</code></pre>
<pre><code>##   (Intercept) poly(age, 2)1 poly(age, 2)2
## 1           1    -0.3229769    0.40369268
## 2           1    -0.3229769    0.40369268
## 3           1    -0.2000881    0.08120775
## 4           1    -0.2000881    0.08120775
## 5           1    -0.2000881    0.08120775
## 6           1    -0.2000881    0.08120775</code></pre>
<p>Similar to the centered values, the orthogonal polynomials produce different estimates of the coefficients and SEs, but the <span class="math inline">\(p\)</span>-values for the highest order polynomial term and the interpretations from the plot of the resulting fitted model will be identical to that of the model created from the raw ages.</p>
<p><label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle"><span class="marginnote"><span style="display: block;">Because of the unchanged inferences for the highest order term and the equivalent model interpretations, while dealing with structural multicollinearity can be convenient in some cases, it is not essential.</span></span></p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-Shao:1997">
<p>Shao, J. (1997). An asymptotic theory for linear model selection. <em>Statistica Sinica</em>, <em>7</em>, 221–264.</p>
</div>
<div id="ref-Stone:1977">
<p>Stone, M. (1977). An asymptotic equivalence of choice of model by cross-validation and akaike’s criterion. <em>Journal of the Royal Statistical Society, Series B</em>, <em>39</em>, 44–47.</p>
</div>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
