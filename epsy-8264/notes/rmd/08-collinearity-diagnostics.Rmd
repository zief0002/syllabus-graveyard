---
title: "Collinearity Diagnostics"
description: |
  A brief introduction to empirical diagnostics to detect collinearity. Example taken from @Chatterjee:2012.
author:
  - name: Andrew Zieffler 
    url: http://www.datadreaming.org/
date: "`r Sys.Date()`"
output: radix::radix_article
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
```

In 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in @Coleman:1966 and @Mosteller:1972. The data in *equal-educational-opportunity.csv* consist of data taken from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:

- `achievement`: Measurement indicating the student achievement level
- `faculty`: Measurement indicating the faculty's credentials
- `peer`: Measurement indicating the influence of peer groups in the school
- `school`: Measurement indicating the school facilities (e.g., building, teaching materials)

We will use these data to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credientials and peer influence.

```{r}
# Load libraries
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(readr)
library(sm)

# Read in data
eeo = read_csv("~/Dropbox/epsy-8264/data/equal-education-opportunity.csv")
head(eeo)
```

### Regression Analysis

To examine the RQ, the following model was posited:

$$
\mathrm{Achievement}_i = \beta_0 + \beta_1(\mathrm{Faculty}_i) + \beta_2(\mathrm{Peer}_i) + \beta_3(\mathrm{School}_i) + \epsilon_i
$$

```{r eval=FALSE}
# Fit the regression model
lm.1 = lm(achievement ~ faculty + peer + school, data = eeo)

# Examine assumptions
qqPlot(lm.1)
residualPlot(lm.1)
```



```{r echo=FALSE, fig.width=8, fig.height=4}
# Fit the regression model
lm.1 = lm(achievement ~ faculty + peer + school, data = eeo)

# Examine assumptions; look for problematic observations
par(mfrow = c(1, 2))
qqPlot(lm.1, id = FALSE)
residualPlot(lm.1)
par(mfrow = c(1, 1))
```

```{r fig.width=6, fig.height=6}
# Index plots of several regression diagnostics
influenceIndexPlot(lm.1)
```

School 28 may be problematic, but after removing this observation (not shown) it made little improvement in the residual plots. As such, School 28 was retained in the data. As the assumptions seem reasonably met, we next look to the model-level and coefficient-level output:

```{r}
# Model-level information
glance(lm.1)

# Coefficient-level information
tidy(lm.1)
```

Examining this information we find:

- 20\% of the variation in student achievement is explained by the model; which is statistically significant $F(3, 66)=5.72$; $p=0.002$.
- However, none of the individual coefficients are statistically significant!

These results are typical when there is *collinearity* in the model. 

## Collinearity

Collinearity is defined mathematically as when any of the predictors (*X*) in a regression model is a perfect linear combination of the other predictors:

$$
X_{k+1} = c_0(1) + c_1X_1 + c_2X_2 + c_3X_3 + \ldots + c_kX_k
$$

When this happens, the matrix $\mathbf{X}^{\intercal}\mathbf{X}$ is singular, and the OLS normal equations do not have a unique solution. Recall that the sampling variance for a predictor, $B_j$ is

$$
\mathrm{Var}(B_j) = \frac{1}{1 - R^2_j} \times \frac{\sigma^2_\epsilon}{(n-1)S^2_j}
$$

The first term in this product is referred to as the *variance inflation factor* (VIF). Recall that $R^2_j$ in this term is the squared multiple correlation between $X_j$ on the other $X$s. 

- When the correlation between $X_j$ and the other $X$s is 0 (they are independent; *orthogonal*) then the VIF becomes 1.
- When the correlation between $X_j$ and the other $X$s is high then the VIF becomes larger than 1; it becomes a multiplier of the variance.
- When there is perfect correlation between $X_j$ and the other $X$s then the VIF approaches $\infty$; the sampling variances (and SEs) are infinitely large.

In practice it is rare to have perfect collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept).

It is, however, common to have strong less-than-perfect collinearity in practice. In thes cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; making them quite large.

## Signs of Potential Collinearity

In our example we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:

- Large changes in the size of the estimated coefficients when variables are added to the model;
- Large changes in the size of the estimated coefficients when an observation is added or deleted;
- The signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;
- Large SEs on variables that are expected to be important predictors.


## Collinearity Diagnostics

We can empirically diagnose problematic collinearity in the data [@Belsley:1991; @Belsley:1980]. Before we do, however, it is important that the functional form of the model has been correctly specified. Collinearity produces unstable estimates of the coefficients and sampling variances which result from strong linear relationships between the predictors. In applied work, the model needs to be specified before we can estimate coefficients or their sampling variances; hence, collinearity should only be investigated after the model has been satisfactorily specified. 

### High Correlations among Predictors

Collinearity can sometimes be anticipated by examining the pairwise correlations between the predictors.

```{r}
eeo %>%
  select(faculty, peer, school) %>%
  correlate()
```

All three of the predictors are highly correlated with one another.

Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step). There are three common methods statisticians use to detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix.


### Variance Inflation Factor

The variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:

$$
\mathrm{VIF} = \frac{1}{1 - R^2_j}
$$

The VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. 

The table below shows the and range of correlation values for $R_j$, and their resulting effects on the size of the confidence interval for the coefficient.

```{r echo=FALSE, layout="l-body-outset"}
tab_01 = data.frame(
  R = seq(from = 0.5, to = 0.9, by = 0.1)
) %>%
  mutate(
    R2 = R^2,
    VIF = 1 / (1 - R2),
    sqrt_vif = sqrt(VIF)
  )

kable(tab_01, 
      digits = 2, 
      col.names = c("$R_j$", "$R^2_j$", "$\\mathrm{VIF}$", "$\\sqrt{\\mathrm{VIF}}$"), 
      format = "html"
      ) %>%
  kable_styling(position = "center") %>%
  row_spec(c(1, 3, 5), background = "#dfdfdf")
```

For example, if the correlation between $X_j$ and the other $X$s is 0.9, then the CIs for the coefficients would increase by a factor of of 2.29. The uncertainty in the estimates would more than double!

We can use the `vif()` function from the **car** package to compute the variance inflation factors for each coefficient.

```{r}
# VIF
vif(lm.1)

# Square root of VIF
sqrt(vif(lm.1))
```

All three coefficients are impacted by VIF. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.


### Eigenvalues of the Correlation Matrix

Each $k \times k$ matrix has a set of $k$ scalars, called eigenvalues (denoted $\lambda$) associated with it. These eigenvalues can be arranged in descending order such that,

$$
\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \ldots \geq \lambda_k
$$

Because the correlation matrix of the predictors is a square matrix, we can find a corresponding set of eigenvalues for this correlation matrix. It turns out that if any of these eigenvalues is exactly equal to zero, there would be a linear dependence among the predictors. In practice, if one of the eigenvalues is quite a bit smaller than the others (and near zero), there is collinearity.

Empirically, we compute the sum of the reciprocals of the eigenvalues 

$$
\sum_{i=1}^k \frac{1}{\lambda_i}
$$

If the sum is greater than five times the number of predictors, it is a sign of collinearity.

#### Using R to Compute the Eigenvalues of the Correlation Matrix

We can use the `eigen()` function to compute the eigenvalues of a square matrix. We provide this function the correlation matrix for the model's predictors. To date, I have been using the `correlate()` function to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truely a matrix object. Instead, we will use the `cor()` function, which produces a matrix object, to produce the correlation matrix.

```{r}
# Correlation matrix of predictors
x = cor(eeo[c("faculty", "peer", "school")])
x
```

Once we have the correlation matrix, we can use the `eigen()` function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix. 

```{r}
# Compute eigenvalues and eigenvectors
eigen(x)

# Sum of reciprocal of eigenvalues
sum(1 / eigen(x)$values)
```

We compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; $5 \times 3 =15$. Since this sum is greater than 15, we would conclude that there is a collinearity problem. 


### Condition Indices

One related diagnostic measure of collinearity are the *condition indices* of the correlation matrix. The *j*th condition index is defined as

$$
\kappa_j = \sqrt{\frac{\lambda_1}{\lambda_j}}
$$

for $j=1,2,3,\ldots,k$, where $\lambda_1$ is the first (largest) eigenvalue and $\lambda_j$ is the *j*th eigenvalue. 

The first condition index, $\kappa_1$, will always be equal to 1, and the other condition indices will be larger than one. The largest condition index, which will be,

$$
\kappa_k = \sqrt{\frac{\lambda_1}{\lambda_k}}
$$

where $\lambda_k$ is the smallest eigenvalue, is known as the *condition number* of the correlation matrix. If the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity. 

From empirical work, condition numbers that exceed 15 are typically problematic (this indicates that the maximum eigenvalue is more than 225 times greater than the maximum eigenvalue). When the condition number exceeds 30, corrective action will almost surely need to be taken.

```{r}
# Compute condition indices
sqrt(max(eigen(x)$values) / eigen(x)$values)
```

The condition number of 19.26, suggests strong collinearity among the predictors.


## Fixing Collinearity in Practice

Although there are several solutions in practice, none are a magic bullet. Here are three potential fixes:

- Re-specify the model
    - Combine collinear predictors, 
    - Drop one (or more) of the collinear predictors---This changes what you are controlling for.
- Biased estimation
    - Trade small amount of bias for a reduction in coefficient variability
- Introduce prior information about the coefficients
    - This can be done formally in the analysis (e.g., Bayesian analysis)
    - It can be used to give a different model specification

Note that although collinearity is a data problem, the most common fixes in practice are to change the model. For example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. 

This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question. The highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.



