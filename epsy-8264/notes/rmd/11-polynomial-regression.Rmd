---
title: "Polynomial Regression"
author: "EPsy 8264"
date: "October 2018"
output:
  tufte::tufte_html:
     tufte_variant: "envisioned"
     self_contained: no
     css: my_style.css
highlight: tango
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
link-citations: no
---


```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

There may be relationships that you want to model that are not linear. Polynomial regression is one methodology that we can use to model nonlinearity. To fit a polynomial regression we include additional predictors to the model that are obtained by raising each of the original predictors to a power. For example, to fit a cubic regression model we include the three predictors: $X$, $X^2$, and $X^3$. 

Adding polynomial terms to the model, like any other predictor correlated with the outcome will improve the fit of the model to the data. Figure 1 shows the results of fitting three different polynomial regression models to the *polynomial-example.csv* data and the resulting model-level $R^2$ values.

```{r echo=FALSE, message=FALSE, fig.height = 4, fig.width = 12, fig.fullwidth=TRUE}
# Load libraries
library(broom)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(readr)

# Read in data
example = read_csv("~/Dropbox/epsy-8264/data/polynomial-example.csv")

p1 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = expression(paste(R^2, "= 0.913")),
    subtitle = "y ~ 1 + x"
  )

p2 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 5)) +
  labs(
    title = expression(paste(R^2, "= 0.943")),
    subtitle = expression(paste("y ~ 1 + x + ", x^2, " + ", x^3, " + ", x^4, " + ", x^5))
  )

p3 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 9)) +
  labs(
    title = expression(paste(R^2, "= 1.000")),
    subtitle = expression(paste("y ~ 1 + x + ", x^2, " + ", x^3, " + ", x^4, " + ", x^5, " + ", x^6, " + ", x^7, " + ", x^8, " + ", x^9))
  )

grid.arrange(p1, p2, p3, nrow = 1)
```

By *saturating* the model with polynomial terms we can obtain perfect fit to the data. Unfortunately, this 8th-degree polynomial model is unlikely to generalize to new data sets. We have overfitted the model to the data. Also, interpolated predictions (e.g., at $x = 50$) are suspect, at best. Even models with lower-order polynomials tend to be overfitted to the data. Fortunately, there are several methods we can use to avoid overfitting.

## Bluegill Example

The data in *bluegills.csv* include measurements taken on 78 bluegills from [Lake Mary](https://www.dnr.state.mn.us/lakefind/search.html?name=Lake+Mary&county=21) in Minnesota. On each fish, a key scale was removed. The age of a fish is determined by [counting the number of annular rings on the scale](https://fishbio.com/field-notes/inside-fishbio/reading-scales). The goal of the analysis is to relate length of the fish at capture to the age of the fish. These data were collected by Richard Frie, and discussed in [@Weisberg:1986; @Weisberg:2005].

```{r message=FALSE}
# Load libraries
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(readr)

# Import data
bluegills = read_csv("~/Dropbox/epsy-8264/data/bluegills.csv")
head(bluegills)
```


The scatterplot of age versus length (left) suggests that there may be a nonlinear relationship between the age and length of a fish. The plot of the residuals versus the fitted values from the model where we regress length on age (right) also suggests that the relationship is nonlinear. 

```{r echo=FALSE, fig.width=8, fig.height=4}
# Plot of length versus age
p1 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  xlab("Age") +
  ylab("Length") +
  theme_bw()
  

# Residual plot
lm.1 = lm(length ~ 1 + age, data = bluegills)
p2 = augment(lm.1) %>% 
  ggplot(aes(x = .fitted, y = .std.resid)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    #geom_smooth(se = FALSE) +
    xlab("Fitted values") +
    ylab("Standardized residuals") +
    theme_bw()

grid.arrange(p1, p2, nrow = 1)
```

One method of dealing with this nonlinearity is to include higher degree polynomial effects of age. For example, we could include a quadratic (2nd-degree polynomial) term in addition to the linear term in the model. There are six distinct age ($x$) values in the data, so a a 5th-degree polynomial model would saturate the data.

```{marginfigure}
If the data includes $k$ distinct $x$-values, a $k-1$-degree polynomial model will saturate the data.
```

To account for the nonlinearity, we will fit and evaluate a suite of potential models: (1) a first-degree polynomial (linear); (2) a second-degree polynomial (quadratic); (3) a third-degree polynomial (cubic); (4) a fourth-degree polynomial (quartic); and (5) a fifth-degree polynomial (quintic). 

There are many methods for fitting polynomial models.

```{marginfigure}
All three methods produce the same coefficient-level and model-level output. There are benefits and costs to each of these methods, and each may be useful depending on the situation. 
```

- Create the polynomial terms in the data and use them in the `lm()` function.
- Create the polynomial terms directly in the `lm()` function without creating them in the data.
- Use the `poly()` function to create the polynomial terms in the `lm()` function.

To illustrate each of these methods, we will fit a third-degree polynomial.



#### Method 1: Create Polynomial Terms in the Data

The first method for fitting a polynomial regression model is to create the polyomial terms in the data and then use these terms in the `lm()` function. To do this, we will first create the $age^2$ and $age^3$ terms in the data set.

```{r}
# Create the quadratic and cubic terms
bluegills = bluegills %>%
  mutate(
    age_quad = age ^ 2,
    age_cubic = age ^ 3
  )

head(bluegills)
```

Then we can include them in the models. For example, to fit the cubic (third-degree) polynomial model:

```{r}
lm.3 = lm(length ~ 1 + age + age_quad + age_cubic, data = bluegills)
tidy(lm.3)
glance(lm.3)
```

#### Method 2: Create Polynomial Terms in the lm() Function

The second method for fitting a polynomial regression model creates the polynomial terms directly in the `lm()` function. If you use this method you will not have to `mutate()` new terms into the data. To create the terms directly in the `lm()` we use the `I()` function:

```{r}
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3), data = bluegills)
tidy(lm.3)
glance(lm.3)
```

#### Method 3: Use poly() Function in the lm() Function

The third method is to use the `poly()` function. This function will create a set of polynomial terms for a provided variable and degree. We also include the argument `raw=TRUE`. For example, the cubic polynomial can be fitted as:

```{marginfigure}
Using the `poly()` function is syntactically efficient, but there are functions that will not work with this method (e.g., `augment()` to create the regression diagnostics).  
```

```{r}
lm.3 = lm(length ~ 1 + poly(age, 3, raw = TRUE), data = bluegills)
tidy(lm.3)
glance(lm.3)
```

The `poly()` function is useful in exploring relationships in ggplot. We can include polynomials in the `geom_smooth()` layer by including the argument `formula=`. For example to add a 3rd-degree polynomial regression smoother to the plot we include `formula=y~poly(x, 3, raw = TRUE)` in the `geom_smooth()` layer. The syntax for this would be:

```{r eval=FALSE}
ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~poly(x, 3, raw = TRUE), se = FALSE) +
  theme_bw() 
```

Below are all the polynomial models (up to the saturated model) that can be fitted to the bluegill data.

```{r echo=FALSE, message=FALSE, fig.height = 8, fig.width = 12, fig.fullwidth=TRUE}
p1 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("Age") +
  ylab("Length") +
  ggtitle("1st-Degree Polynomial")

p2 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~poly(x, 2, raw = TRUE), se = FALSE) +
  theme_bw() +
  xlab("Age") +
  ylab("Length") +
  ggtitle("2nd-Degree Polynomial")

p3 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~poly(x, 3, raw = TRUE), se = FALSE) +
  theme_bw() +
  xlab("Age") +
  ylab("Length") +
  ggtitle("3rd-Degree Polynomial")

p4 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~poly(x, 4, raw = TRUE), se = FALSE) +
  theme_bw() +
  xlab("Age") +
  ylab("Length") +
  ggtitle("4th-Degree Polynomial")

p5 = ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~poly(x, 5, raw = TRUE), se = FALSE) +
  theme_bw() +
  xlab("Age") +
  ylab("Length") +
  ggtitle("5th-Degree Polynomial")

grid.arrange(p1, p2, p3, p4, p5, nrow = 2)
```

The plots suggest that within the 2nd-, 3rd, 4th, and 5th-degree polynomials are all but indistinguishable from one another. This might suggest that the quadratic polynomial might be sufficient to account for the nonlinearity.

#### Matrix Algebra

Including polynomial terms in the regression model is no different than including any other predictors in the model; the design matrix just reflects these polynomial effects. For example the design matrix for the cubic polynomial would be,

$$
\mathbf{X} = \begin{bmatrix}
1 & \mathrm{Age}_1 & \mathrm{Age}_1^2 \\
1 & \mathrm{Age}_2 & \mathrm{Age}_2^2 \\
1 & \mathrm{Age}_3 & \mathrm{Age}_3^2 \\
\vdots & \vdots & \vdots \\
1 & \mathrm{Age}_n & \mathrm{Age}_n^2 \\
\end{bmatrix}
$$

All of the matrix algebra (to obtain coefficients, variance--covariance matrices, etc.) is all the same as it was for any other linear model.

## Evaluating the Polynomial Terms: Statistical Inference

We will begin our analysis by fitting five polynomial models; 1st-degree through 5th-degree polynomials.

```{r}
lm.1 = lm(length ~ 1 + age,                                             data = bluegills)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = bluegills)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = bluegills)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = bluegills)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = bluegills)
```

One method of evaluating the polynomial models, since the models are nested, is to use an $F$-test to test the change in $R^2$. The hypothesis for this is that:

$$
H_0: R^2_{\mathrm{Simple~Model}} = R^2_{\mathrm{Complex~Model}}
$$

For example to compare the linear to the quadratic polynomial model,

$$
H_0: R^2_{\mathrm{Linear~Model}} = R^2_{\mathrm{Quadratic~Model}}
$$

We can evaluate this by submitting both models to the `anova()` function.

```{r}
anova(lm.1, lm.2)
```

Here we would reject the null hypothesis; $F(1, 8921) = 25$, $p < .001$. This suggests it is likely that the quadratic age term explains additional variation in fish length above and beyond the linear term.

To evaluate the necessity of the cubic term, we compare the quadratic model to the cubic model, etc. We can include a sequence of nested models in the `anova()` function to carry out all of these tests simultaneously.

```{r}
anova(lm.1, lm.2, lm.3, lm.4, lm.5)
```

Based on these results, we would adopt the quadratic polynomial model. Here we fail to reject the null hypothesis that the cubic term explains additional variation over and above the quadratic model; $F(1, 8916) = 0.04$, $p = 0.84$. It is unlikely that the cubic age term explains additional variation in fish length above and beyond the linear and quadratic terms. This would lead us to adopt the quadratic model.

## Examining the Residuals and Influence

Like any other model we adopt, it is germaine that we examine the tenability of the assumptions. Evaluation of the residuals suggests that including the quadratic effect mitigated the nonlinearity seen earlier. The assumptions of normality and homoskedasticity also seem tenable. Cursory examination of leverage and influence diagnostics show four potential problematic observations. However, removing these four observations (analyses not shown) did not noticably improve the fit to the assumptions.


```{r eval=FALSE}
residualPlot(lm.2)
qqPlot(lm.2, id = FALSE)
influencePlot(lm.2)
```

```{r fig.width=12, fig.height=4, echo=FALSE, fig.fullwidth=TRUE}
par(mfrow = c(1, 3))
residualPlot(lm.2)
qqPlot(lm.2, id = FALSE)
influencePlot(lm.2)
par(mfrow = c(1, 1))
```


## Reporting the Results

When we report the results of a polynomial regression model evaluated using statistical inference, we typically report the model-level ($R^2$, $F$, $p$), and coefficent-level ($B$, $SE$, $p$) summaries for the adopted model. Interpretations of the model are also provided. However, since a polynomial model is essentially an interaction model (interaction of a predictor with itself) the constituent main-effects are not typically interpreted. Providing a plot of the fitted model is generally the easiest way to help facilitate these interpretations.

At the model level:

```{r}
glance(lm.2)
```

The quadratic polynomial model using age to predict variation in fish length is statsitically significant; $F(2, 75)=151$, $p<.001$. The model explains 79.6\% of the variation in fish length.

At the coefficient-level:

```{r}
tidy(lm.2)
```


The fitted equation would be:

$$
\hat{\mathrm{Fish~Length}}_i = 13.62 + 54.05(\mathrm{Age}_i) - 4.72(\mathrm{Age}^2_i)
$$

Since this model is an interaction model, we would only interpret the coefficient associated with the highest order interaction term (quadratic term) and not the constituent lower-order effects (linear term). Here, the significant quadratic term implies that the effect of age on length depends on age. For a better interpretation, we can plot the fitted model and interpret the findings from the plot.

```{r}
data.frame(
  age = seq(from = 1, to = 6, by = 0.1) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.2, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab("Age") +
    ylab("Predicted length")
```

This shows the negative quadratic relationship (upside-down U-shape) between age and length. For younger fish, there is a large positive relationship between age and length. This relationship diminishes as fish age and may even be negative for older fish (although in our data that is extrapolation).


  





## References
