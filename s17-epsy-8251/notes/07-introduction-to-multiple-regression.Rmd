---
title: "Introduction to Multiple Regression"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{xcolor}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
urlcolor: "umn2"
bibliography: epsy8251.bib
csl: apa-single-spaced.csl
always_allow_html: yes
---

```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(prompt=FALSE, comment=NA, message=FALSE, warning=FALSE, tidy=FALSE)
opts_knit$set(width=85)
options(scipen=5)
```

<!-- LaTeX definitions -->

\mdfdefinestyle{mystyle}{userdefinedwidth=5in, align=center, backgroundcolor=yellow, roundcorner=10pt, skipabove=2em}

\mdfdefinestyle{mystyle2}{userdefinedwidth=5.5in, align=center, skipabove=10pt, topline=false, bottomline=false, 
linecolor=myorange, linewidth=5pt}

\mdfdefinestyle{work}{userdefinedwidth=5in, linecolor=blue, align=center, roundcorner=10pt, skipabove=2em}



# Introduction and Research Question

In this set of notes, you will continue your foray into regression analysis. To do so, we will again examine the question of whether education level is related to income using the *riverside.csv* data from @Lewis-Beck:2016. Specifically we will ask, *Do differences in education level explain variation in incomes?* and *Do differences in education level explain variation in incomes even after accounting for differences in seniority?*

# Preparation

```{r preparation, warning=FALSE, message=FALSE}
# Read in data
city = read.csv(file = "~/Google Drive/andy/epsy-8251/data/riverside.csv")
head(city)

# Load libraries
library(dplyr)
library(ggplot2)
library(sm)
```

\pagebreak

# Answering the First Research Question

In previous notes, we fitted a model regressing employees' incomes on education level. 

```{r}
# Fit regression model
lm.1 = lm(income ~ 1 + education, data = city)
summary(lm.1)
```


The fitted equation,
$$
\hat{\mathrm{Income}} = 11,321 + 2,651(\mathrm{Education~Level}),
$$
suggests that the estimated mean income for employees with education levels that differ by one year varies by \$2,651. We also found that differences in education level explained 63.2\% of the variation in income, and that this was statistically significant, $p<.001$. All this suggests that education level is related to income.

## Examining the Seniority Predictor

Let's do some analysis on the seniority predictor.

```{r fig.width=6, fig.height=6, out.width='3.5in', fig.align='center'}
# Examine the marginal distribution
sm.density(city$seniority, xlab = "Seniority (in years)")
```

\begin{center} 
\textit{Figure 1.} Density plot of the marginal distribution of seniority.
\end{center}

\vspace{2em}

```{r}
# Compute mean and standard deviation
city %>% summarize(M = mean(seniority), SD = sd(seniority))
```


Seniority is symmetric with a typical employee having roughly 15 years of seniority. There is quite a lot of variation in seniority, however, with most employees having between 8 and 22 years of seniority. After we examine the mariginal distribution, we should examine the relationships among all of three variables we are considering in the analysis. Typically researchers will examine the scatterplots between each predictor and the outcome (to evaluate the functional forms of the relationships with the outcome) and also examine the correlation matrix.

```{r fig.width=6, fig.height=6, out.width='3.5in', fig.align='center'}
# Relationship between income and seniority
ggplot(data = city, aes(x = seniority, y = income)) +
  geom_point() +
  theme_bw() +
  xlab("Seniority (in years)") +
  ylab("Income (in dollars)")
```

\begin{center}
\textit{Figure 2.} Scatterplot showing the relationship between seniority level and income.
\end{center}

\vspace{2em}

```{r}
# Correlation matrix
cor(city[ , c("income", "education", "seniority")])
```



The relationship between seniority and income seems linear and positive ($r=0.58$). This suggests that employees with more seniority also tend to have higher incomes. Education level and seniority are also modestly correlated ($r=0.33$), indicating that employees with higher education levels tend to also have more seniority. 

Because the correlation between the two predictors is not 0, this calls into question our previous findings about whether there actually is a relationship between education level and income. It might be that this relationship is spurious. That really it is the fact that the reason we saw that employees with higher education levels tended to have higher incomes is that they also tend to have more seniority. What we need to know is whether **after we account for differences in seniority** is there is still a relationship between education level and income. To answer this question, we will need to fit a model that includes both predictors.

\pagebreak

# Simple Regression Model: Seniority as a Predictor of Income

Before we fit the model with both predictors, we will first fit the simple regression model using seniority as a predictor of variation in income.

```{r}
lm.2 = lm(income ~ 1 + seniority, data = city)
summary(lm.2)
```

The fitted equation,
$$
\hat{\mathrm{Income}} = 35,690 + 1,219(\mathrm{Seniority~Level}),
$$
suggests that the estimated mean income for employees with seniority levels that differ by one year varies by \$1,219. We also find that differences in seniority level explain 33.9\% of the variation in income, and that this is statistically significant, $p<.001$. All this suggests that seniority level is related to income.

\pagebreak

# Multiple Regression Model: Education Level and Seniority as a Predictors of Income

To fit the multiple regression model, we will just add (literally) additional predictors to the right-hand side of the `lm()` formula.

```{r}
lm.3 = lm(income ~ 1 + education + seniority, data = city)
summary(lm.3)
```

## Model-Level Inference

To interpret multiple regression results, begin with the model-level information. Together, differences in education level AND seniority explain 74.2\% of the variation in income. This is statistically significant ($p<.001$). 

It is important to note that the $p$-value at the model level is different from any of the coefficeint-level $p$-values. This is because when we include more than one predictor in a model, the hypotheses being tested at these two levels are different. The formal model-level hypothesis that is being tested can be written mathematically as,

$$
H_0:\rho^2 = 0.
$$

This is a test of whether *all the predictors together* explain variation in the outcome variable. Equivallently, we can also write the hypothesis as a function of the predictor effects, namely,

$$
H_0:\beta_{\mathrm{Education~Level}} = \beta_{\mathrm{Seniority}} = 0.
$$

In plain English, this is akin to asking whether the effect of every predictor included in the model is 0. Although these two expressions of the model-level hypothesis look quite different, they are answering the same question, namely whether the model is worthwhile in predicting variation in income. 

## Interpreting the Coefficients 

Now we turn to the coefficient-level information produced in the `summary()` output. First we will write the fitted multiple regression equation,

$$
\hat{\mathrm{Income}} = 6,769 + 2,252(\mathrm{Education~Level}) + 739(\mathrm{Seniority~Level}).
$$

The slopes (of which there are now more than one) are referred to as *partial regression slopes* or *partial effects*. They represent the effect of the predictor *AFTER* accounting for the effects of the other predictors included in the model. For example,

- The partial effect of education level is 2,252. This indicates that a one year difference in education level is associated with a \$2,252 difference in income (on average), after accounting for differences in seniority.
- The partial effect of seniority is 739. This indicates that a one year difference in seniority is associated with a \$739 difference in income (on average), after accounting for differences in education level.

The language "after accounting for" is not ubiquitous in interpreting partial regression coefficients. Some researchers instead use "controlling for", "holding constant", or "partialling out the effects of". For example, the education effect could also be interpreted these ways:

- A one year difference in education level is associated with a \$2,252 difference in income (on average), after controlling for differences in seniority.
- A one year difference in education level is associated with a \$2,252 difference in income (on average), after holding the effect of seniority constant.
- A one year difference in education level is associated with a \$2,252 difference in income (on average), after partialling out the effects of seniority.

Lastly, we can also interpret the intercept:

- The average income for all employees with 0 years of education AND 0 years of seniority is estimated to be \$6,769.

This is the predicted avergage $Y$ value when ALL the predictors have a value of 0. As such, it is often an extrapolated prediction and is not of interest to most applied researchers. For example, in our data, education level ranges from 8 to 24 years and seniority ranges from 1 to 27 years. We have no data that has a zero value for either predictor, let alone for both. This makes prediction tenuous.

## Coefficient Level Inference

At the coefficient level, the hypotheses being tested are about each individual predictor. In plain English, the statistical question being asked is: After accounting for ALL the other predictors included in the model, is there an effect of $X$ on $Y$? The mathematical expression of the hypothesis is

$$
H_0: \beta_k = 0.
$$

These hypotheses are evaluated using a $t$-test. For example, consider the test associated with the education level coefficient.

$$
H_0: \beta_{\mathrm{Education~Level}} = 0
$$

This is akin to asking is there an effect of education level on income after accounting for differences in seniority? The null hypothesis would be rejected, $t(29)=6.73$, $p<.001$, suggesting that there is indeed an effect of education on income after controlling for differences in seniority. (Note that the $df$ for the $t$-test for all of the coefficient tests is equivalent to the error, or denominator, $df$ for the model-level $F$-test.)

# Multiple Regression: Statistical Model

The multiple regression model says that each case's outcome ($Y$) is a function of two or more predictors ($X_1$, $X_2$, \ldots, $X_k$) and some amount of error. Mathematically it can be written as

$$
Y_i = \beta_0 + \beta_1(X1_{i}) + \beta_2(21_{i}) + \ldots + \beta_k(Xk_{i}) + \epsilon_i
$$

As with simple regression we are interested in estimating the values for each of the regression coefficients, namely, $\beta_0$, $\beta_1$, $\beta_2$, \ldots, $\beta_k$. To do this, we again employ least squares estimation to minimize the sum of the squared error terms.

Since we have more than one $X$ term in the fitted equation, the structural part of the model no longer mathematically defines a line. For example, the fitted equation from earlier,

$$
\hat{Y} = 6,769 + 2,252(X1) + 739(X2),
$$

mathematically defines a regression plane. (Note we have three dimensions, $Y$, $X1$, and $X2$. If we add predictors, we have four or more dimensions and we describe a hyperplane.) 

The data and regression plane defined by the education level, seniority level, and income for the City of Riverside employees is shown below. The regression plane is tilted up in both the education level direction (corresponding to a positive partial slope of education) and in the seniority level direction (corresponding to a positive partial slope of seniority). The blue points are above the plane (employees with a positive residual) and the yellow points are below the plane (employees with a negative residual). 

```{r message=FALSE, echo=FALSE, fig.width=6, fig.height=6, out.width='4in', fig.align='center'}
library("scatterplot3d")
city$resid = ifelse(residuals(lm.3) < 0, "#F39D4177", "steelblue")
s3d = scatterplot3d(x = city$education, y = city$seniority, z = city$income, type = "p", color = city$resid, 
  angle = 60, pch = 16, box = FALSE, xlab = "Education level", ylab = "Seniority level", zlab = "Income", 
  cex.symbols = 2, label.tick.marks = FALSE)
# Add regression plane
my.lm = lm(city$income ~ city$education + city$seniority)
s3d$plane3d(my.lm, lty = 1, col = "red")
```

\begin{center} 
\textit{Figure 3.} Three-dimensional scatterplot showing the relationship between education level, seniority, and income. The fitted regression plane is also shown. Blue observations have a positive residual and yellow observations have a negative residual.
\end{center}

The residual sum of squares can be obtained using the `anova()` function to give the ANOVA decomposition of the model.

```{r}
anova(lm.3)
```

Here the $\mathrm{SS}_{\mathrm{Residuals}} = 1,695,313,285$. Any other plane (i.e., different coefficient values for the intercept and predictors) would produce a higher sum of squared residuals value. Note that the $df$ value in the `Residuals` row of the ANOVA output is another way to find the $df$ associated with the $t$-tests for the coefficient tests we presented earlier.

# Presenting Results

It is quite common for researchers to present the results of their regression analyses in table form. Different models are typically presented in different columns and predictors are presented in rows. (Because it is generally of less substantive value, the intercept is often presented in the last row.)

```{r, message=FALSE, results='asis', eval=FALSE, echo=FALSE}
library(stargazer)
stargazer(lm.1, lm.2, lm.3, column.labels = c("Model 1", "Model 2", "Model 3"), covariate.labels = c("Education level", "Seniority"), dep.var.caption = NULL, dep.var.labels = NULL, dep.var.labels.include = FALSE)
```

Table 1. 
*Regression Models Fitted to City Employee Data ($n=32$) Using Education Level and Seniority to Predict Income*

\begin{table}[!htbp] \centering 
\begin{tabular}{@{\extracolsep{5pt}}lccc} 
\hline 
 & Model 1 & Model 2 & Model 3 \\ 
\hline \\[-1.8ex] 
 Education level & 2,651$^{***}$ &  & 2,252$^{***}$ \\ 
  & (370) &  & (335) \\ 
  & & & \\ 
 Seniority &  & 1,219$^{***}$ & 739$^{***}$ \\ 
  &  & (311) & (210) \\ 
  & & & \\ 
 Intercept & 11,321.380$^{*}$ & 35,690.300$^{***}$ & 6,769.172 \\ 
  & (6,123) & (5,073) & (5,373) \\ 
  & & & \\ 
\hline \\[-1.8ex] 
R$^{2}$ & 0.632 & 0.339 & 0.742 \\ 
RMSE & 8,978 & 12,031 & 7,646 \\ 
\hline 
\multicolumn{4}{l}{\textit{Note:} $^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

Based on these fitted models, we can now go back and answer our research questions. Do differences in education level explain variation in incomes? Based on Model 1 the answer is yes. Is this true even after accounting for differences in seniority? Model 3 suggests that, again, the answer is yes. (Since it is not germaine to answerig the RQs, Model 2 could just as easily be omitted from the table.)


# References


